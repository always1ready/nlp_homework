{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371970ff",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3. Исправление опечаток"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cf8bd",
   "metadata": {},
   "source": [
    "## 1. Учет грамматики при оценке исправлений (3 балла)\n",
    "\n",
    "В последнюю итерацию алгоритма для генерации исправлений добавьте еще один компонент - учет грамматической информации. Частично она уже учитывается за счет языковой модели (вероятность предсказывается для словоформы), но такой подход ограничен из-за того, что модель не может ничего предсказать для словоформ, которых не было в обучающей выборке. Чтобы это исправить постройте еще одну \"языковую модель\" на грамматических тэгах:\n",
    "1) Используя mystem или pymorphy, разметьте какой-нибудь корпус (например, кусок wiki из семинара) или воспользуйтесь уже размеченным корпусом (например, opencorpora)\n",
    "2) соберите униграмные и биграмные статистики на уровне грамматических тэгов (например, вместо `задача важна` у вас будет биграм `S,жен,неод=им,ед A=ед,кр,жен`). Для простоты можете начать только с частеречных тэгов и добавить остальную информацию позже\n",
    "3) напишите функцию, которая будет оценивать вероятность данного предложения на основе грамматической языковой модели (статистик из предыдущего шага). Функция должна сначала преобразовать текст в грамматические тэги, используя точно такой же подход, что использовался на шаге 1. \n",
    "4) в функции correct_text_with_lm замените compute_sentence_proba на вашу новую функцию и прогоните получившийся алгоритм на данных\n",
    "5) сравните предсказания с предсказанием изначального correct_text_with_lm, проверьте метрики и посмотрите на различие в ошибках и исправлениях, найдите несколько примеров отличий в предсказаниях этих подходов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67f8d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f307010",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('wiki_data.txt', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd67163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "substrings = tokenize(corpus)\n",
    "tokens = [token.text for token in substrings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0287d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('correct_sents.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1b8be29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: \"\\w\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\w\"? A raw string is also an option.\n",
      "<>:1: SyntaxWarning: \"\\w\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\w\"? A raw string is also an option.\n",
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_9772\\2788637254.py:1: SyntaxWarning: \"\\w\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\w\"? A raw string is also an option.\n",
      "  vocab = Counter(re.findall('\\w+', corpus.lower()))\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter(re.findall('\\w+', corpus.lower()))\n",
    "N = sum(vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f56d9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(word, N=N):\n",
    "    return vocab[word] / N\n",
    "\n",
    "def known(words):\n",
    "    return set(w for w in words if w in vocab)\n",
    "\n",
    "def edits1(word):\n",
    "    letters = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    \n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def candidates(word):\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def correction(word):\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def calculate_metrics(true, actual, predicted):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    total_mistaken = 0\n",
    "    mistaken_fixed = 0\n",
    "\n",
    "    total_correct = 0\n",
    "    correct_broken = 0\n",
    "\n",
    "    for i in range(len(true)):\n",
    "        t, a, p = true[i], actual[i], predicted[i]\n",
    "    \n",
    "        if t == p:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if t == a:\n",
    "            total_correct += 1\n",
    "            if t != p:\n",
    "                correct_broken += 1\n",
    "\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if t == p:\n",
    "                mistaken_fixed += 1\n",
    "\n",
    "    return {\n",
    "        \"total_accuracy\": correct/total,\n",
    "        \"fixed_mistakes\": mistaken_fixed/total_mistaken if total_mistaken > 0 else 0,\n",
    "        \"broken_correct_words\": correct_broken/total_correct if total_correct > 0 else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b7e2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(sentenize(corpus))\n",
    "train_size = int(len(sentences) * 0.9)\n",
    "train_sentences = sentences[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "567bf471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acd1d040f244aada6383682bd12a817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размечено предложений: 174488\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "def get_word_tags(word):\n",
    "    parsed = morph.parse(word)[0]\n",
    "    return str(parsed.tag)\n",
    "\n",
    "tagged_sentences = []\n",
    "for sent in tqdm(train_sentences):\n",
    "    \n",
    "    tokens = [t.text for t in tokenize(sent.text)]\n",
    "\n",
    "    tokens = [t.lower() for t in tokens if t.lower() not in punct and t.strip()]\n",
    "    \n",
    "    if not tokens:\n",
    "        continue\n",
    "    \n",
    "    sent_tags = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            tag = get_word_tags(token)\n",
    "            sent_tags.append((token, tag))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if sent_tags:\n",
    "        tagged_sentences.append(sent_tags)\n",
    "\n",
    "print(f\"Размечено предложений: {len(tagged_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acfa11b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312aa095c3314c8e983a7a32143da905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174488 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram_tags = Counter()\n",
    "bigram_tags = Counter()\n",
    "start_tags = Counter()\n",
    "\n",
    "for sent_tags in tqdm(tagged_sentences):\n",
    "    if not sent_tags:\n",
    "        continue\n",
    "    \n",
    "    tags = [tag for _, tag in sent_tags]\n",
    "    \n",
    "    for tag in tags:\n",
    "        unigram_tags[tag] += 1\n",
    "    \n",
    "    for i in range(1, len(tags)):\n",
    "        bigram = (tags[i-1], tags[i])\n",
    "        bigram_tags[bigram] += 1\n",
    "    \n",
    "    start_tag = tags[0]\n",
    "    start_tags[start_tag] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b15b66d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d292dda8ee5941bb89108a115b8f9443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/174519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class GrammarLanguageModel:\n",
    "    \n",
    "    def __init__(self, smoothing=0.1):\n",
    "\n",
    "        self.smoothing = smoothing\n",
    "        self.tag_counts = Counter()\n",
    "        self.bigram_counts = Counter()\n",
    "        self.start_counts = Counter()\n",
    "        \n",
    "        self.tag_probs = {}\n",
    "        self.bigram_probs = {}\n",
    "        self.start_probs = {}\n",
    "        \n",
    "        self.total_tags = 0\n",
    "        self.total_starts = 0\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "        self.morph = pymorphy3.MorphAnalyzer()\n",
    "        \n",
    "    def get_grammar_tag(self, word):\n",
    "        \n",
    "        try:\n",
    "            parsed = self.morph.parse(word)[0]\n",
    "            tag_str = str(parsed.tag)\n",
    "            \n",
    "            return tag_str\n",
    "        except Exception as e:\n",
    "            return 'UNK'\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        self.tag_counts.clear()\n",
    "        self.bigram_counts.clear()\n",
    "        self.start_counts.clear()\n",
    "        \n",
    "        self.total_tags = 0\n",
    "        self.total_starts = 0\n",
    "        \n",
    "        for i, sentence in enumerate(tqdm(sentences)):\n",
    "            tokens = [t.text.lower() for t in tokenize(sentence)]\n",
    "            \n",
    "            tokens = [t for t in tokens if t not in punct and t.strip()]\n",
    "            \n",
    "            if not tokens:\n",
    "                continue\n",
    "            \n",
    "            tags = []\n",
    "            for token in tokens:\n",
    "                tag = self.get_grammar_tag(token)\n",
    "                tags.append(tag)\n",
    "            \n",
    "            first_tag = tags[0]\n",
    "            self.start_counts[first_tag] += 1\n",
    "            self.total_starts += 1\n",
    "            \n",
    "            for tag in tags:\n",
    "                self.tag_counts[tag] += 1\n",
    "                self.total_tags += 1\n",
    "            \n",
    "            for j in range(1, len(tags)):\n",
    "                prev_tag = tags[j-1]\n",
    "                curr_tag = tags[j]\n",
    "                self.bigram_counts[(prev_tag, curr_tag)] += 1\n",
    "        \n",
    "        self._compute_probabilities()\n",
    "    \n",
    "    def _compute_probabilities(self):\n",
    "        V = len(self.tag_counts)\n",
    "        \n",
    "        self.tag_probs = {}\n",
    "        \n",
    "        for tag, count in self.tag_counts.items():\n",
    "            numerator = count + self.smoothing\n",
    "            \n",
    "            denominator = self.total_tags + self.smoothing * V\n",
    "            \n",
    "            prob = numerator / denominator\n",
    "            self.tag_probs[tag] = prob\n",
    "        \n",
    "        self.unk_prob = self.smoothing / (self.total_tags + self.smoothing * V)\n",
    "        \n",
    "        self.bigram_probs = {}\n",
    "        \n",
    "        for (tag1, tag2), count in self.bigram_counts.items():\n",
    "            tag1_count = self.tag_counts.get(tag1, 0)\n",
    "            \n",
    "            numerator = count + self.smoothing\n",
    "            \n",
    "            denominator = tag1_count + self.smoothing * V\n",
    "            \n",
    "            if denominator > 0:\n",
    "                prob = numerator / denominator\n",
    "                self.bigram_probs[(tag1, tag2)] = prob\n",
    "        \n",
    "        self.start_probs = {}\n",
    "        if self.total_starts > 0:\n",
    "            for tag, count in self.start_counts.items():\n",
    "                self.start_probs[tag] = count / self.total_starts\n",
    "        \n",
    "        self.vocab_size = V\n",
    "    \n",
    "    def get_tag_probability(self, tag):\n",
    "        return self.tag_probs.get(tag, self.unk_prob)\n",
    "    \n",
    "    def get_transition_probability(self, tag1, tag2):\n",
    "        prob = self.bigram_probs.get((tag1, tag2))\n",
    "        \n",
    "        if prob is not None:\n",
    "            return prob\n",
    "        \n",
    "        tag1_count = self.tag_counts.get(tag1, 0)\n",
    "        \n",
    "        numerator = self.smoothing\n",
    "        denominator = tag1_count + self.smoothing * self.vocab_size\n",
    "        \n",
    "        if denominator > 0:\n",
    "            return numerator / denominator\n",
    "        else:\n",
    "            return self.unk_prob\n",
    "    \n",
    "    def get_start_probability(self, tag):\n",
    "        return self.start_probs.get(tag, 0.0)\n",
    "    \n",
    "    def compute_sentence_probability(self, sentence_text, use_log=True):\n",
    "        tokens = [t.text.lower() for t in tokenize(sentence_text)]\n",
    "        tokens = [t for t in tokens if t not in punct and t.strip()]\n",
    "        \n",
    "        if not tokens:\n",
    "            return -np.inf if use_log else 0.0\n",
    "        \n",
    "        tags = []\n",
    "        for token in tokens:\n",
    "            tag = self.get_grammar_tag(token)\n",
    "            tags.append(tag)\n",
    "        \n",
    "        if use_log:\n",
    "            log_prob = 0.0\n",
    "            \n",
    "            first_tag = tags[0]\n",
    "            start_prob = self.get_start_probability(first_tag)\n",
    "            \n",
    "            if start_prob > 0:\n",
    "                log_prob += np.log(start_prob)\n",
    "            else:\n",
    "                tag_prob = self.get_tag_probability(first_tag)\n",
    "                log_prob += np.log(tag_prob)\n",
    "            \n",
    "            for i in range(1, len(tags)):\n",
    "                tag1 = tags[i-1]\n",
    "                tag2 = tags[i]\n",
    "                \n",
    "                trans_prob = self.get_transition_probability(tag1, tag2)\n",
    "                log_prob += np.log(trans_prob)\n",
    "            \n",
    "            return log_prob\n",
    "            \n",
    "        else:\n",
    "            prob = 1.0\n",
    "            \n",
    "            first_tag = tags[0]\n",
    "            start_prob = self.get_start_probability(first_tag)\n",
    "            \n",
    "            if start_prob > 0:\n",
    "                prob *= start_prob\n",
    "            else:\n",
    "                prob *= self.get_tag_probability(first_tag)\n",
    "            \n",
    "            for i in range(1, len(tags)):\n",
    "                tag1 = tags[i-1]\n",
    "                tag2 = tags[i]\n",
    "                \n",
    "                trans_prob = self.get_transition_probability(tag1, tag2)\n",
    "                prob *= trans_prob\n",
    "            \n",
    "            return prob\n",
    "    \n",
    "    def compare_sentences(self, sentence1, sentence2):\n",
    "        prob1 = self.compute_sentence_probability(sentence1, use_log=True)\n",
    "        prob2 = self.compute_sentence_probability(sentence2, use_log=True)\n",
    "        \n",
    "        return {\n",
    "            'sentence1': sentence1,\n",
    "            'sentence2': sentence2,\n",
    "            'log_prob1': prob1,\n",
    "            'log_prob2': prob2,\n",
    "            'more_probable': 1 if prob1 > prob2 else 2\n",
    "        }\n",
    "\n",
    "grammar_lm = GrammarLanguageModel(smoothing=0.1)\n",
    "grammar_lm.train(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0fb38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text_with_grammar(text, grammar_model, top_k=5):\n",
    "    tokens = [token.text for token in tokenize(text)]\n",
    "    \n",
    "    corrections = [] \n",
    "    original_structure = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        word = token.lower()\n",
    "        \n",
    "        is_upper = token[0].isupper() if token else False\n",
    "        is_punct = word in punct or not word.strip()\n",
    "        original_structure.append({'is_upper': is_upper, 'is_punct': is_punct, 'orig': token})\n",
    "        \n",
    "        if is_punct:\n",
    "            corrections.append([word])\n",
    "            continue\n",
    "            \n",
    "        if word in vocab:\n",
    "            corrections.append([word])\n",
    "        else:\n",
    "            cands = list(candidates(word))\n",
    "            \n",
    "            cands = sorted(cands, key=P, reverse=True)[:top_k]\n",
    "            \n",
    "            if not cands:\n",
    "                corrections.append([word])\n",
    "            else:\n",
    "                corrections.append(cands)\n",
    "\n",
    "    possible_sentences_tokens = list(itertools.product(*corrections))\n",
    "    \n",
    "    def get_score(tokens_tuple):\n",
    "        sent_str = \" \".join(tokens_tuple)\n",
    "        return grammar_model.compute_sentence_probability(sent_str, use_log=True)\n",
    "\n",
    "    best_tokens = max(possible_sentences_tokens, key=get_score)\n",
    "    \n",
    "    final_tokens = []\n",
    "    for i, word in enumerate(best_tokens):\n",
    "        if original_structure[i]['is_upper']:\n",
    "            final_tokens.append(word.capitalize())\n",
    "        else:\n",
    "            final_tokens.append(word)\n",
    "            \n",
    "    return \" \".join(final_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39b84e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text_simple(text):\n",
    "    tokens = [t.text for t in tokenize(text)]\n",
    "    \n",
    "    corrected_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.lower() in punct or not token.strip():\n",
    "            corrected_tokens.append(token)\n",
    "            continue\n",
    "        \n",
    "        corrected_word = correction(token.lower())\n",
    "        \n",
    "        if token and token[0].isupper():\n",
    "            corrected_word = corrected_word.capitalize()\n",
    "        \n",
    "        corrected_tokens.append(corrected_word)\n",
    "    \n",
    "    result_parts = []\n",
    "    for i, token in enumerate(corrected_tokens):\n",
    "        if i > 0 and token not in punct and corrected_tokens[i-1] not in punct:\n",
    "            result_parts.append(' ')\n",
    "        result_parts.append(token)\n",
    "    \n",
    "    return ''.join(result_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d7e3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [token.strip(punctuation) for token in tokens_1]\n",
    "    tokens_2 = [token.strip(punctuation) for token in tokens_2]\n",
    "    \n",
    "    tokens_1 = [token for token in tokens_1 if token]\n",
    "    tokens_2 = [token for token in tokens_2 if token]\n",
    "    \n",
    "    assert len(tokens_1) == len(tokens_2)\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))\n",
    "\n",
    "def evaluate_grammar_model(grammar_model):\n",
    "    y_true_all = []\n",
    "    y_actual_all = []\n",
    "    y_pred_grammar = []\n",
    "    \n",
    "    for i in tqdm(range(len(true)), desc=\"Оценка модели\"):\n",
    "        bad_sent = bad[i]\n",
    "        true_sent = true[i]\n",
    "        \n",
    "        word_pairs = align_words(true_sent, bad_sent)\n",
    "        \n",
    "        corrected_sent_str = correct_text_with_grammar(bad_sent, grammar_model)\n",
    "        \n",
    "        corrected_tokens = corrected_sent_str.lower().split()\n",
    "        corrected_tokens = [t.strip(punctuation) for t in corrected_tokens]\n",
    "        corrected_tokens = [t for t in corrected_tokens if t]\n",
    "        \n",
    "        if len(corrected_tokens) != len(word_pairs):\n",
    "             corrected_tokens = [pair[1] for pair in word_pairs]\n",
    "\n",
    "        for j, (true_word, actual_word) in enumerate(word_pairs):\n",
    "            y_true_all.append(true_word)\n",
    "            y_actual_all.append(actual_word)\n",
    "            y_pred_grammar.append(corrected_tokens[j])\n",
    "\n",
    "    metrics = calculate_metrics(y_true_all, y_actual_all, y_pred_grammar)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "796ce54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f311d0b5714741009c088354979de359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Оценка модели:   0%|          | 0/915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'broken_correct_words': 0.07315952681750316,\n",
      " 'fixed_mistakes': 0.5,\n",
      " 'total_accuracy': 0.8718359179589795}\n"
     ]
    }
   ],
   "source": [
    "metrics_result = evaluate_grammar_model(grammar_lm)\n",
    "\n",
    "pprint(metrics_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6978080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clean_words(sentence_str):\n",
    "    tokens = sentence_str.lower().split() \n",
    "    \n",
    "    clean_words = [token.strip(punctuation) for token in tokens]\n",
    "    clean_words = [word for word in clean_words if word]\n",
    "    \n",
    "    return clean_words\n",
    "\n",
    "def evaluate_all_data(grammar_model=None, alpha=0.3):\n",
    "    y_true_all = []\n",
    "    y_actual_all = []\n",
    "    y_pred_simple = []\n",
    "    y_pred_grammar = []\n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(len(true)), desc=\"Обработка пар предложений\"):\n",
    "        true_sent = true[i]\n",
    "        bad_sent = bad[i]\n",
    "        \n",
    "        word_pairs = align_words(true_sent, bad_sent)\n",
    "        \n",
    "        simple_words = []\n",
    "        for correct_word, wrong_word in word_pairs:\n",
    "            corrected = correction(wrong_word) \n",
    "            simple_words.append(corrected)\n",
    "        \n",
    "        grammar_corrected_sent = correct_text_with_grammar(bad_sent, grammar_model)\n",
    "        \n",
    "        grammar_words = extract_clean_words(grammar_corrected_sent)\n",
    "        \n",
    "        final_grammar_words = []\n",
    "        if len(grammar_words) != len(word_pairs):\n",
    "            final_grammar_words = simple_words\n",
    "        else:\n",
    "            final_grammar_words = grammar_words\n",
    "        \n",
    "        for j, (correct_word, wrong_word) in enumerate(word_pairs):\n",
    "            y_true_all.append(correct_word)\n",
    "            y_actual_all.append(wrong_word)\n",
    "            \n",
    "            y_pred_simple.append(simple_words[j])\n",
    "            y_pred_grammar.append(final_grammar_words[j])\n",
    "            \n",
    "    metrics_simple = calculate_metrics(y_true_all, y_actual_all, y_pred_simple)\n",
    "    metrics_grammar = calculate_metrics(y_true_all, y_actual_all, y_pred_grammar)\n",
    "\n",
    "    return {\n",
    "        'metrics_simple': metrics_simple,\n",
    "        'metrics_grammar': metrics_grammar,\n",
    "        'y_true': y_true_all,\n",
    "        'y_actual': y_actual_all,\n",
    "        'y_pred_simple': y_pred_simple,\n",
    "        'y_pred_grammar': y_pred_grammar\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddd4a063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9eb11399cd4c98babd47be96f557b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Обработка пар предложений:   0%|          | 0/915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "анализ метрик моделей\n",
      "Общая точность      | 0.8708       | 0.8709           (Разница: +0.0001)\n",
      "Исправлено ошибок   | 0.5116       | 0.5124           (Разница: +0.0008)\n",
      "Сломано правильных  | 0.0760       | 0.0760           (Разница: +0.0000)\n",
      "расхождений всего: 449\n",
      "\n",
      "Случаи, когда грамматическая модель улучшила результат (исправила ошибку, которую пропустила обычная модель):\n",
      "Пример 1:\n",
      "  Исходное слово с ошибкой: 'основая' (правильное слово: основная)\n",
      "    Простой корректор (P(w)):    'основан'\n",
      "    Грамматический корректор: 'основная' (ВЕРНО)\n",
      "---\n",
      "Пример 2:\n",
      "  Исходное слово с ошибкой: 'сранно' (правильное слово: странно)\n",
      "    Простой корректор (P(w)):    'санно'\n",
      "    Грамматический корректор: 'странно' (ВЕРНО)\n",
      "---\n",
      "Пример 3:\n",
      "  Исходное слово с ошибкой: 'нмного' (правильное слово: немного)\n",
      "    Простой корректор (P(w)):    'много'\n",
      "    Грамматический корректор: 'немного' (ВЕРНО)\n",
      "---\n",
      "Пример 4:\n",
      "  Исходное слово с ошибкой: 'самыи' (правильное слово: самый)\n",
      "    Простой корректор (P(w)):    'самым'\n",
      "    Грамматический корректор: 'самый' (ВЕРНО)\n",
      "---\n",
      "Пример 5:\n",
      "  Исходное слово с ошибкой: 'подрбный' (правильное слово: подробный)\n",
      "    Простой корректор (P(w)):    'подобный'\n",
      "    Грамматический корректор: 'подробный' (ВЕРНО)\n",
      "---\n",
      "\n",
      "Случаи, когда грамматическая модель ухудшила результат:\n",
      "Пример 1:\n",
      "  Исходное слово с ошибкой: 'хороше' (правильное слово: хорошо)\n",
      "    Простой корректор (P(w)):    'хорошо'\n",
      "    Грамматический корректор: 'хорошие' (НЕВЕРНО)\n",
      "---\n",
      "Пример 2:\n",
      "  Исходное слово с ошибкой: 'хороше' (правильное слово: хорошо)\n",
      "    Простой корректор (P(w)):    'хорошо'\n",
      "    Грамматический корректор: 'хорошие' (НЕВЕРНО)\n",
      "---\n",
      "Пример 3:\n",
      "  Исходное слово с ошибкой: 'баръер' (правильное слово: барьер)\n",
      "    Простой корректор (P(w)):    'барьер'\n",
      "    Грамматический корректор: 'баркер' (НЕВЕРНО)\n",
      "---\n",
      "Пример 4:\n",
      "  Исходное слово с ошибкой: 'мущщину' (правильное слово: мужчину)\n",
      "    Простой корректор (P(w)):    'мужчину'\n",
      "    Грамматический корректор: 'мурину' (НЕВЕРНО)\n",
      "---\n",
      "Пример 5:\n",
      "  Исходное слово с ошибкой: 'припораты' (правильное слово: препараты)\n",
      "    Простой корректор (P(w)):    'препараты'\n",
      "    Грамматический корректор: 'приорат' (НЕВЕРНО)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_all_data(grammar_model=grammar_lm)\n",
    "\n",
    "# --- Вывод сравнительных метрик ---\n",
    "metrics_simple = results['metrics_simple']\n",
    "metrics_grammar = results['metrics_grammar']\n",
    "\n",
    "print(\"анализ метрик моделей\")\n",
    "\n",
    "simple_acc = metrics_simple['total_accuracy']\n",
    "grammar_acc = metrics_grammar['total_accuracy']\n",
    "acc_diff = grammar_acc - simple_acc\n",
    "print(f\"Общая точность      | {simple_acc:.4f}       | {grammar_acc:.4f}           (Разница: {acc_diff:+.4f})\")\n",
    "\n",
    "simple_fixed = metrics_simple['fixed_mistakes']\n",
    "grammar_fixed = metrics_grammar['fixed_mistakes']\n",
    "fixed_diff = grammar_fixed - simple_fixed\n",
    "print(f\"Исправлено ошибок   | {simple_fixed:.4f}       | {grammar_fixed:.4f}           (Разница: {fixed_diff:+.4f})\")\n",
    "\n",
    "simple_broken = metrics_simple['broken_correct_words']\n",
    "grammar_broken = metrics_grammar['broken_correct_words']\n",
    "broken_diff = grammar_broken - simple_broken\n",
    "print(f\"Сломано правильных  | {simple_broken:.4f}       | {grammar_broken:.4f}           (Разница: {broken_diff:+.4f})\")\n",
    "\n",
    "\n",
    "#анализ примеров, где грамматическая модель лучше справилась, чем обычная\n",
    "y_true = results['y_true']\n",
    "y_actual = results['y_actual']\n",
    "y_pred_simple = results['y_pred_simple']\n",
    "y_pred_grammar = results['y_pred_grammar']\n",
    "total_words = len(y_true)\n",
    "\n",
    "helpful_examples = []\n",
    "harmful_examples = []\n",
    "differences_found = 0\n",
    "\n",
    "for i in range(total_words):\n",
    "    true_w = y_true[i]\n",
    "    actual_w = y_actual[i]\n",
    "    pred_simple = y_pred_simple[i]\n",
    "    pred_grammar = y_pred_grammar[i]\n",
    "    \n",
    "    if pred_simple != pred_grammar:\n",
    "        differences_found += 1\n",
    "        \n",
    "        simple_correct = (true_w == pred_simple)\n",
    "        grammar_correct = (true_w == pred_grammar)\n",
    "        \n",
    "        if grammar_correct and not simple_correct:\n",
    "            helpful_examples.append({'Ист': true_w, 'Ошб': actual_w, 'Простой': pred_simple, 'Грамматика': pred_grammar})\n",
    "            \n",
    "        elif simple_correct and not grammar_correct:\n",
    "             harmful_examples.append({'Ист': true_w, 'Ошб': actual_w, 'Простой': pred_simple, 'Грамматика': pred_grammar})\n",
    "\n",
    "\n",
    "print(f\"расхождений всего: {differences_found}\")\n",
    "\n",
    "# Функция для вывода примера\n",
    "def print_example(ex, outcome):\n",
    "    print(f\"  Исходное слово с ошибкой: '{ex['Ошб']}' (правильное слово: {ex['Ист']})\")\n",
    "    print(f\"    Простой корректор (P(w)):    '{ex['Простой']}'\")\n",
    "    print(f\"    Грамматический корректор: '{ex['Грамматика']}' ({outcome})\")\n",
    "    print(\"---\")\n",
    "\n",
    "\n",
    "print(\"\\nСлучаи, когда грамматическая модель улучшила результат (исправила ошибку, которую пропустила обычная модель):\")\n",
    "if helpful_examples:\n",
    "    for i, ex in enumerate(helpful_examples[:5]):\n",
    "        print(f\"Пример {i+1}:\")\n",
    "        print_example(ex, \"ВЕРНО\")\n",
    "else:\n",
    "     print(\"Явных примеров улучшения не обнаружено\")\n",
    "\n",
    "\n",
    "print(\"\\nСлучаи, когда грамматическая модель ухудшила результат:\")\n",
    "if harmful_examples:\n",
    "     for i, ex in enumerate(harmful_examples[:5]):\n",
    "        print(f\"Пример {i+1}:\")\n",
    "        print_example(ex, \"НЕВЕРНО\")\n",
    "else:\n",
    "    print(\"Явных примеров ухудшения не обнаружено\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf9985",
   "metadata": {},
   "source": [
    "## 2.  Symspell (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392cc23",
   "metadata": {},
   "source": [
    "Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Он основан только на одной операции - удалении символа. Описание алгоритма по шагам:\n",
    "\n",
    "1) Составляется словарь правильных слов  \n",
    "2) На основе словаря правильных слов составляется словарь удалений - для каждого правильного слова создаются все варианты удалений и создается словарь, где ключ - слово с удалением, а значение - правильное слово  (обратите внимание, что для одного удаления может быть несколько правильных слов!) \n",
    "3) При исправлении слова с опечаткой сначала само слово проверятся по словарю удаления, а затем для этого слова генерируются все варианты удалений, и каждый вариант проверяется по словарю удалений. Если в словаре удалений таким образом находится совпадение, то соответствующее ему правильное слово становится исправлением.\n",
    "Если совпадений несколько, то выбирается наиболее вероятное правильное слово  \n",
    "\n",
    "\n",
    "Оцените качество полученного алгоритма теми же тремя метриками."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee4b28f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a298614f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be0aeb5b5af47afaf6eb0caa83ce02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/368802 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сонце -> конце\n",
      "солнцее -> солнце\n"
     ]
    }
   ],
   "source": [
    "def generate_deletes(word):\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    return set(deletes)\n",
    "\n",
    "symspell_dict = {}\n",
    "\n",
    "for correct_word in tqdm(vocab.keys()):\n",
    "    if correct_word not in symspell_dict:\n",
    "        symspell_dict[correct_word] = set()\n",
    "    symspell_dict[correct_word].add(correct_word)\n",
    "    \n",
    "    deletes = generate_deletes(correct_word)\n",
    "    for deleted_word in deletes:\n",
    "        if deleted_word not in symspell_dict:\n",
    "            symspell_dict[deleted_word] = set()\n",
    "        symspell_dict[deleted_word].add(correct_word)\n",
    "\n",
    "def symspell_correction(word):\n",
    "    if word in vocab:\n",
    "        return word\n",
    "        \n",
    "    candidates = set()\n",
    "    \n",
    "    if word in symspell_dict:\n",
    "        candidates.update(symspell_dict[word])\n",
    "        \n",
    "    word_deletes = generate_deletes(word)\n",
    "    for d in word_deletes:\n",
    "        if d in symspell_dict:\n",
    "            candidates.update(symspell_dict[d])\n",
    "            \n",
    "    if not candidates:\n",
    "        return word\n",
    "        \n",
    "    return max(candidates, key=P)\n",
    "\n",
    "print(f\"сонце -> {symspell_correction('сонце')}\") # удаление\n",
    "print(f\"солнцее -> {symspell_correction('солнцее')}\") # вставка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab3020be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4585d36e9774109ad855e0bbbf49988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SymSpell Evaluation:   0%|          | 0/915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общая точность      : 87.85%\n",
      "Исправлено ошибок   : 40.92%\n",
      "Сломано правильных  : 5.20%\n"
     ]
    }
   ],
   "source": [
    "y_true_sym = []\n",
    "y_actual_sym = []\n",
    "y_pred_sym = []\n",
    "\n",
    "for i in tqdm(range(len(true)), desc=\"SymSpell Evaluation\"):\n",
    "    true_sent = true[i]\n",
    "    bad_sent = bad[i]\n",
    "    \n",
    "    word_pairs = align_words(true_sent, bad_sent)\n",
    "    \n",
    "    bad_tokens = bad_sent.lower().split()\n",
    "    bad_tokens = [t.strip(punctuation) for t in bad_tokens]\n",
    "    bad_tokens = [t for t in bad_tokens if t]\n",
    "    \n",
    "    if len(bad_tokens) != len(word_pairs):\n",
    "        bad_tokens = [pair[1] for pair in word_pairs]\n",
    "\n",
    "    for j, (correct_w, wrong_w) in enumerate(word_pairs):\n",
    "        token_to_fix = bad_tokens[j]\n",
    "        predicted = symspell_correction(token_to_fix)\n",
    "        \n",
    "        y_true_sym.append(correct_w)\n",
    "        y_actual_sym.append(wrong_w)\n",
    "        y_pred_sym.append(predicted)\n",
    "\n",
    "metrics_symspell = calculate_metrics(y_true_sym, y_actual_sym, y_pred_sym)\n",
    "\n",
    "print(f\"Общая точность      : {metrics_symspell['total_accuracy']:.2%}\")\n",
    "print(f\"Исправлено ошибок   : {metrics_symspell['fixed_mistakes']:.2%}\")\n",
    "print(f\"Сломано правильных  : {metrics_symspell['broken_correct_words']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70930cdc",
   "metadata": {},
   "source": [
    "# Задание 3 (2 балла)\n",
    "\n",
    "Используя любой из алгоритмов из семинара или домашки, детально проанализируйте получаемые ошибки. Улучшите алгоритм так, чтобы исправить ошибки. Улучшения в алгоритме должны быть общими, не привязанными к конкретным словам (например, словарь исключений не будет считаться). За каждое улучшение, которое исправляет 5+ ошибок вы получите 0.5 балла (максимум 2 в целом)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac7ea06",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
