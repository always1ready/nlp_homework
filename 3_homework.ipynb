{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (можно брать только часть текста, если считается слишком долго). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг \\<start>  \n",
    "    - можете использовать тот же подход с матрицей вероятностей, но по строкам хронить биграмы, а по колонкам униграммы \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)\n",
    "    - у вас будут словари с индексами биграммов и униграммов, не перепутайте их при переводе индекса в слово - словарь биграммов будет больше словаря униграммов и все индексы из униграммного словаря будут формально подходить для словаря биграммов (не будет ошибки при id2bigram[unigram_id]), но маппинг при этом будет совершенно неправильным "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d078056d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Хабр - Униграмм: 74857, Биграмм: 396196, Триграмм: 544129\n",
      "Лента - Униграмм: 68186, Биграмм: 349724, Триграмм: 501998\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy.sparse import lil_matrix, csr_matrix, csc_matrix\n",
    "\n",
    "with zipfile.ZipFile('habr_texts.txt.zip', 'r') as zip_file:\n",
    "    with zip_file.open('habr_texts.txt', 'r') as txt_file:\n",
    "        habr = txt_file.read().decode('utf-8')\n",
    "\n",
    "news = open('lenta.txt', 'r', encoding='utf-8').read()\n",
    "habr = habr[:12_000_000]\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "\n",
    "norm_habr = normalize(habr)\n",
    "norm_news = normalize(news)\n",
    "\n",
    "vocab_habr = Counter(norm_habr)\n",
    "vocab_news = Counter(norm_news)\n",
    "\n",
    "probas_habr = Counter({word:c/len(norm_habr) for word, c in vocab_habr.items()})\n",
    "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})\n",
    "\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "all_sentences_habr = [['<start>', '<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(habr[:5000000])]\n",
    "all_sentences_news = [['<start>', '<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news[:5000000])]\n",
    "\n",
    "train_size_habr = int(0.9 * len(all_sentences_habr))\n",
    "train_sentences_habr = all_sentences_habr[:train_size_habr]\n",
    "test_sentences_habr = all_sentences_habr[train_size_habr:train_size_habr + 30]\n",
    "\n",
    "\n",
    "train_size_news = int(0.9 * len(all_sentences_news))\n",
    "train_sentences_news = all_sentences_news[:train_size_news]\n",
    "test_sentences_news = all_sentences_news[train_size_news:train_size_news + 30]\n",
    "\n",
    "unigrams_habr = Counter()\n",
    "bigrams_habr = Counter()\n",
    "trigrams_habr = Counter()\n",
    "\n",
    "for sentence in train_sentences_habr:\n",
    "    unigrams_habr.update(sentence)\n",
    "    bigrams_habr.update(ngrammer(sentence, n=2))\n",
    "    trigrams_habr.update(ngrammer(sentence, n=3))\n",
    "\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in train_sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence, n=2))\n",
    "    trigrams_news.update(ngrammer(sentence, n=3))\n",
    "\n",
    "print(f\"Хабр - Униграмм: {len(unigrams_habr)}, Биграмм: {len(bigrams_habr)}, Триграмм: {len(trigrams_habr)}\")\n",
    "print(f\"Лента - Униграмм: {len(unigrams_news)}, Биграмм: {len(bigrams_news)}, Триграмм: {len(trigrams_news)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица хабра: (382458, 74857)\n",
      "Матрица ленты: (338518, 68186)\n"
     ]
    }
   ],
   "source": [
    "bigram_vocab_habr = set()\n",
    "for trigram in trigrams_habr:\n",
    "    words = trigram.split()\n",
    "    if len(words) == 3:\n",
    "        bigram = ' '.join(words[:2])\n",
    "        bigram_vocab_habr.add(bigram)\n",
    "\n",
    "bigram_vocab_news = set()\n",
    "for trigram in trigrams_news:\n",
    "    words = trigram.split()\n",
    "    if len(words) == 3:\n",
    "        bigram = ' '.join(words[:2])\n",
    "        bigram_vocab_news.add(bigram)\n",
    "\n",
    "matrix_trigram_habr = lil_matrix((len(bigram_vocab_habr), len(unigrams_habr)))\n",
    "matrix_trigram_news = lil_matrix((len(bigram_vocab_news), len(unigrams_news)))\n",
    "\n",
    "id2word_habr = list(unigrams_habr)\n",
    "word2id_habr = {word:i for i, word in enumerate(id2word_habr)}\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "id2bigram_habr = list(bigram_vocab_habr)\n",
    "bigram2id_habr = {bigram:i for i, bigram in enumerate(id2bigram_habr)}\n",
    "\n",
    "id2bigram_news = list(bigram_vocab_news)\n",
    "bigram2id_news = {bigram:i for i, bigram in enumerate(id2bigram_news)}\n",
    "\n",
    "for trigram in trigrams_habr:\n",
    "    words = trigram.split()\n",
    "    if len(words) == 3:\n",
    "        word1, word2, word3 = words\n",
    "        bigram = ' '.join([word1, word2])\n",
    "        \n",
    "        if bigram in bigrams_habr and bigrams_habr[bigram] > 0:\n",
    "            prob = trigrams_habr[trigram] / bigrams_habr[bigram]\n",
    "            if bigram in bigram2id_habr and word3 in word2id_habr:\n",
    "                matrix_trigram_habr[bigram2id_habr[bigram], word2id_habr[word3]] = prob\n",
    "\n",
    "for trigram in trigrams_news:\n",
    "    words = trigram.split()\n",
    "    if len(words) == 3:\n",
    "        word1, word2, word3 = words\n",
    "        bigram = ' '.join([word1, word2])\n",
    "        \n",
    "        if bigram in bigrams_news and bigrams_news[bigram] > 0:\n",
    "            prob = trigrams_news[trigram] / bigrams_news[bigram]\n",
    "            if bigram in bigram2id_news and word3 in word2id_news:\n",
    "                matrix_trigram_news[bigram2id_news[bigram], word2id_news[word3]] = prob\n",
    "\n",
    "matrix_habr = csc_matrix(matrix_trigram_habr)\n",
    "matrix_news = csc_matrix(matrix_trigram_news)\n",
    "\n",
    "print(f\"Матрица хабра: {matrix_habr.shape}\")\n",
    "print(f\"Матрица ленты: {matrix_news.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01576e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trigram(matrix_trigram, id2bigram, bigram2id, id2word, word2id, n=100, start_bigram='<start> <start>'):\n",
    "    text = []\n",
    "    \n",
    "    current_bigram = start_bigram\n",
    "    words = current_bigram.split()\n",
    "    text.extend(words)\n",
    "    \n",
    "    for i in range(n):\n",
    "        if current_bigram not in bigram2id:\n",
    "            if len(words) >= 1:\n",
    "                last_word = words[-1]\n",
    "                text.append('<end>')\n",
    "                break\n",
    "            else:\n",
    "                text.append('<end>')\n",
    "                break\n",
    "        \n",
    "        bigram_idx = bigram2id[current_bigram]\n",
    "        \n",
    "        prob_row = matrix_trigram[bigram_idx].toarray()[0]\n",
    "        \n",
    "        if prob_row.sum() == 0:\n",
    "            text.append('<end>')\n",
    "            break\n",
    "        \n",
    "        prob_row = prob_row / prob_row.sum()\n",
    "        \n",
    "        chosen_word_idx = np.random.choice(len(prob_row), p=prob_row)\n",
    "        chosen_word = id2word[chosen_word_idx]\n",
    "        \n",
    "        text.append(chosen_word)\n",
    "        \n",
    "        if chosen_word == '<end>':\n",
    "            break\n",
    "        \n",
    "        words = current_bigram.split()\n",
    "        current_bigram = f\"{words[1]} {chosen_word}\"\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c620536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. <start> <start> кроме того необходимо будет быстро исправить <end>\n",
      "2. <start> <start> таким образом я получаю веб-страницы с других моделях <end>\n",
      "3. <start> <start> уверен что по атрибутам label … указывает использовать ли markdown теги в тексте прошу направлять в лс <end>\n",
      "4. <start> <start> каждый чиллер с насосом трубопроводом и теплообменником образует независимый холодильный узел <end>\n",
      "5. <start> <start> база исполнителей всё время <end>\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    text = generate_trigram(\n",
    "        matrix_habr, \n",
    "        id2bigram_habr, \n",
    "        bigram2id_habr, \n",
    "        id2word_habr, \n",
    "        word2id_habr,\n",
    "        n=20\n",
    "    )\n",
    "    print(f\"{i+1}. {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802509f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. <start> <start> семинар призван определить правила развития застройки земель в россии 6,3 миллиона семей стоят в очереди на жилье <end>\n",
      "2. <start> <start> жалобы фермера в полицию не принесли никаких результатов подтверждающих заявления представителей швейцарских властей <end>\n",
      "3. <start> <start> цик сомневается правильно ли они а также планы развития этих отношений <end>\n",
      "4. <start> <start> за истекшие сутки зарегистрировано 18 дтп в результате два человека погибли и более уместных на выставке демонстрируются последние достижения в\n",
      "5. <start> <start> интересно при этом энергосистема лишь на тех кто напрямую связывает это преступление с борьбой вокруг решения о расширении состава ес\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    text = generate_trigram(\n",
    "        matrix_news, \n",
    "        id2bigram_news, \n",
    "        bigram2id_news, \n",
    "        id2word_news, \n",
    "        word2id_news,\n",
    "        n=20\n",
    "    )\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54004742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_proba_trigram_markov(text, word_counts, bigram_counts, trigram_counts):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    \n",
    "    sequence = ['<start>', '<start>'] + tokens + ['<end>']\n",
    "    \n",
    "    for i in range(2, len(sequence)):\n",
    "        word1, word2, word3 = sequence[i-2], sequence[i-1], sequence[i]\n",
    "        trigram = f\"{word1} {word2} {word3}\"\n",
    "        bigram = f\"{word1} {word2}\"\n",
    "        \n",
    "        if bigram in bigram_counts and bigram_counts[bigram] > 0 and trigram in trigram_counts:\n",
    "            prob += np.log(trigram_counts[trigram] / bigram_counts[bigram])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return np.exp(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0ad1f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перплексия Хабра: 20866.24\n",
      "Перплексия Ленты: 16479.54\n"
     ]
    }
   ],
   "source": [
    "def perplexity(logp, N):\n",
    "    return np.exp((-1/N) * logp)\n",
    "\n",
    "def compute_joint_proba_trigram(text, word_counts, bigram_counts, trigram_counts):\n",
    "\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    \n",
    "    sequence = ['<start>', '<start>'] + tokens + ['<end>']\n",
    "    \n",
    "    for i in range(2, len(sequence)):\n",
    "        word1, word2, word3 = sequence[i-2], sequence[i-1], sequence[i]\n",
    "        trigram = f\"{word1} {word2} {word3}\"\n",
    "        bigram = f\"{word1} {word2}\"\n",
    "        \n",
    "        if bigram in bigram_counts and bigram_counts[bigram] > 0 and trigram in trigram_counts:\n",
    "            prob += np.log(trigram_counts[trigram] / bigram_counts[bigram])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return prob, len(tokens)\n",
    "\n",
    "def calculate_perplexity_on_test_set(test_sentences, word_counts, bigram_counts, trigram_counts):\n",
    "\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "    \n",
    "    for sentence in test_sentences:\n",
    "        text = ' '.join([word for word in sentence if word not in ['<start>', '<end>']])\n",
    "        log_prob, num_words = compute_joint_proba_trigram(text, word_counts, bigram_counts, trigram_counts)\n",
    "        total_log_prob += log_prob\n",
    "        total_words += num_words\n",
    "    \n",
    "    return perplexity(total_log_prob, total_words)\n",
    "\n",
    "perp_habr = calculate_perplexity_on_test_set(test_sentences_habr, unigrams_habr, bigrams_habr, trigrams_habr)\n",
    "perp_news = calculate_perplexity_on_test_set(test_sentences_news, unigrams_news, bigrams_news, trigrams_news)\n",
    "\n",
    "print(f\"Перплексия Хабра: {perp_habr:.2f}\")\n",
    "print(f\"Перплексия Ленты: {perp_news:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2 (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733858c",
   "metadata": {},
   "source": [
    "Измените функцию generate_with_beam_search так, чтобы она работала с моделью, которая учитывает два предыдущих слова. \n",
    "Сравните получаемый результат с первым заданием. \n",
    "Также попробуйте начинать генерацию не с нуля (подавая \\<start> \\<start>), а с какого-то промпта. Но помните, что учитываться будут только два последних слова, так что не делайте длинные промпты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c426746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam:\n",
    "    def __init__(self, sequence: list, score: float):\n",
    "        self.sequence: list = sequence\n",
    "        self.score: float = score\n",
    "\n",
    "\n",
    "def generate_with_beam_search_trigram(matrix_trigram, id2bigram, bigram2id, id2word, word2id, n=100, max_beams=5, start_bigram='<start> <start>'):\n",
    "\n",
    "    initial_sequence = start_bigram.split()\n",
    "    initial_node = Beam(sequence=initial_sequence, score=0.0)\n",
    "    beams = [initial_node]\n",
    "    \n",
    "    for i in range(n):\n",
    "        new_beams = []\n",
    "        \n",
    "        for beam in beams:\n",
    "            if beam.sequence and beam.sequence[-1] == '<end>':\n",
    "                new_beams.append(beam)\n",
    "                continue\n",
    "            \n",
    "            if len(beam.sequence) >= 2:\n",
    "                current_bigram = ' '.join(beam.sequence[-2:])\n",
    "            else:\n",
    "                if len(beam.sequence) == 1:\n",
    "                    current_bigram = f\"<start> {beam.sequence[-1]}\"\n",
    "                else:\n",
    "                    current_bigram = \"<start> <start>\"\n",
    "            \n",
    "            if current_bigram not in bigram2id:\n",
    "                new_beams.append(beam)\n",
    "                continue\n",
    "            \n",
    "            bigram_idx = bigram2id[current_bigram]\n",
    "            probas = matrix_trigram[bigram_idx].toarray()[0]\n",
    "            \n",
    "            valid_indices = np.where(probas > 0)[0]\n",
    "            if len(valid_indices) == 0:\n",
    "                new_beams.append(beam)\n",
    "                continue\n",
    "            \n",
    "            top_idxs = valid_indices[probas[valid_indices].argsort()[::-1][:max_beams]]\n",
    "            \n",
    "            for top_id in top_idxs:\n",
    "                next_word = id2word[top_id]\n",
    "                \n",
    "                new_sequence = beam.sequence + [next_word]\n",
    "                \n",
    "                word_prob = probas[top_id]\n",
    "                new_score = (beam.score * len(beam.sequence) + np.log(word_prob + 1e-10)) / len(new_sequence)\n",
    "                \n",
    "                new_beam = Beam(sequence=new_sequence, score=new_score)\n",
    "                new_beams.append(new_beam)\n",
    "        \n",
    "        if new_beams:\n",
    "            beams = sorted(new_beams, key=lambda x: x.score, reverse=True)[:max_beams]\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    sorted_beams = sorted(beams, key=lambda x: x.score, reverse=True)\n",
    "    return [\" \".join(beam.sequence) for beam in sorted_beams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7db90bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. <start> <start> как сообщает риа новости со ссылкой на источники в правоохранительных органах города интерфаксу сообщили в\n",
      "2. <start> <start> как сообщает риа новости <end>\n",
      "3. <start> <start> как сообщает риа новости со ссылкой на источники в правоохранительных органах города интерфаксу сообщили что\n"
     ]
    }
   ],
   "source": [
    "results_news = generate_with_beam_search_trigram(\n",
    "    matrix_news, id2bigram_news, bigram2id_news, id2word_news, word2id_news,\n",
    "    n=15, max_beams=5\n",
    ")\n",
    "for i, text in enumerate(results_news[:3]):\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5230565b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. <start> <start> если у вас есть таблица состоящая из выпускников и студентов междисциплинарных программ вандербильта и факультета\n",
      "2. <start> <start> если у вас есть таблица состоящая из выпускников и студентов междисциплинарных программ вандербильта и медицинской\n",
      "3. <start> <start> если у вас есть опыт по внедрению оценке и использованию систем геймификации в профильных сообществах\n"
     ]
    }
   ],
   "source": [
    "results_habr = generate_with_beam_search_trigram(\n",
    "    matrix_habr, id2bigram_habr, bigram2id_habr, id2word_habr, word2id_habr,\n",
    "    n=15, max_beams=5\n",
    ")\n",
    "for i, text in enumerate(results_habr[:3]):  # Показываем топ-3\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "405aafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_prompt_trigram(matrix_trigram, id2bigram, bigram2id, id2word, word2id, prompt_text, n=50, max_beams=5):\n",
    "\n",
    "    prompt_tokens = normalize(prompt_text)\n",
    "    \n",
    "    if len(prompt_tokens) >= 2:\n",
    "        start_bigram = ' '.join(prompt_tokens[-2:])\n",
    "    elif len(prompt_tokens) == 1:\n",
    "        start_bigram = f\"<start> {prompt_tokens[0]}\"\n",
    "    else:\n",
    "        start_bigram = \"<start> <start>\"\n",
    "    \n",
    "    return generate_with_beam_search_trigram(\n",
    "        matrix_trigram, id2bigram, bigram2id, id2word, word2id,\n",
    "        n=n, max_beams=max_beams, start_bigram=start_bigram\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70b04f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"сегодня\",\n",
    "    \"ежедневно в\",\n",
    "    \"пришлось\",\n",
    "    \"вероятно\",\n",
    "    \"россия\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c012d8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Промпт: 'сегодня'\n",
      "  1. <start> сегодня же скупщина черногории приняла закон о внесении изменений и дополнений\n",
      "  2. <start> сегодня же скупщина черногории приняла закон о гарантировании вкладов граждан в\n",
      "  3. <start> сегодня же скупщина черногории приняла закон о гарантировании вкладов граждан вбанках\n",
      "\n",
      "Промпт: 'ежедневно в'\n",
      "  1. ежедневно в освобожденные районы возвращаются по 600-800 человек <end>\n",
      "  2. ежедневно в освобожденные районы чечни <end>\n",
      "  3. ежедневно в освобожденные села <end>\n",
      "\n",
      "Промпт: 'пришлось'\n",
      "  1. <start> пришлось отменить сотни авиарейсов в города флориды и джорджии спешат уехать\n",
      "  2. <start> пришлось отменить сотни авиарейсов в города флориды и джорджии а также\n",
      "  3. <start> пришлось отменить сотни авиарейсов непростая ситуация сложилась на рынке операционных систем\n",
      "\n",
      "Промпт: 'вероятно'\n",
      "  1. <start> вероятно окончательное число погибших достигло 36 человек в том числе и\n",
      "  2. <start> вероятно окончательное число погибших достигло 36 человек в том числе в\n",
      "  3. <start> вероятно окончательное число погибших достигло 36 человек в том числе с\n",
      "\n",
      "Промпт: 'россия'\n",
      "  1. <start> россия неоднократно высказывалась против подобного переписывания договора <end>\n",
      "  2. <start> россия неоднократно напоминала что развертывание нпро нарушит договор <end>\n",
      "  3. <start> россия неоднократно напоминала что развертывание нпро <end>\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    print(f\"\\nПромпт: '{prompt}'\")\n",
    "    results = generate_with_prompt_trigram(\n",
    "        matrix_news, id2bigram_news, bigram2id_news, id2word_news, word2id_news,\n",
    "        prompt, n=10, max_beams=3\n",
    "    )\n",
    "    for i, text in enumerate(results[:3]):\n",
    "        print(f\"  {i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7b8dd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Промпт: 'сегодня'\n",
      "  1. <start> сегодня это 1 nbsp 190 долларов <end>\n",
      "  2. <start> сегодня это 1 nbsp 254 долларов <end>\n",
      "  3. <start> сегодня это 1 nbsp 000 долларов <end>\n",
      "\n",
      "Промпт: 'ежедневно в'\n",
      "  1. ежедневно в мире telegram-банк онлайн-образование пятизвездочный отель на берегу теплого моря и поиск друзей hola приглашает full-stack\n",
      "  2. ежедневно в мире telegram-банк онлайн-образование пятизвездочный отель на берегу теплого моря и поиск гениев « гении равномерно\n",
      "  3. ежедневно в мире telegram-банк онлайн-образование пятизвездочный отель на берегу теплого моря и поиск « живых » ipv\n",
      "\n",
      "Промпт: 'пришлось'\n",
      "  1. <start> пришлось критически пересмотреть всё имеющееся научное знание по этой причине власти великобритании предлагают выделить £ 1,9\n",
      "  2. <start> пришлось реализовывать обертку которая разбирает вывод утилиты в stdout на ошибки в pri потоке приведут к\n",
      "  3. <start> пришлось критически пересмотреть всё имеющееся научное знание по этой ссылке <end>\n",
      "\n",
      "Промпт: 'вероятно'\n",
      "  1. <start> вероятно будущая микросхема nvidia на 16-нм техпроцесе finfet без труда найдёт высокооплачиваемую работу <end>\n",
      "  2. <start> вероятно математический аппарат программистам давно закончившим вуз <end>\n",
      "  3. <start> вероятно будущая микросхема nvidia на 16-нм техпроцесе finfet без труда прочесть буквы и цифры солнечный день\n",
      "\n",
      "Промпт: 'россия'\n",
      "  1. <start> россия обладает самой высокой производительностью которые потребляют много энергии — перегрев означает замедление работы поломку или\n",
      "  2. <start> россия сильно отличается конверсия чистого сегмента 20 от сегмента с работающим сервисом hucksterbot <end>\n",
      "  3. <start> россия обладает самой высокой производительностью которые потребляют много энергии <end>\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    print(f\"\\nПромпт: '{prompt}'\")\n",
    "    results = generate_with_prompt_trigram(matrix_habr, id2bigram_habr, bigram2id_habr, id2word_habr, word2id_habr, prompt, n=15, max_beams=5)\n",
    "    for i, text in enumerate(results[:3]):\n",
    "        print(f\"  {i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee1e8ae",
   "metadata": {},
   "source": [
    "По сравнению с генерациями в первом задании beam_search приводит к однотипным предложениям, но более связным. Возможности beam search ограничены, в моём случае с длинными промптами такая генерация не работает"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
