{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a98604cb",
      "metadata": {
        "id": "a98604cb"
      },
      "source": [
        "Домашку будет легче делать в колабе (убедитесь, что у вас runtype с gpu)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c422aa0",
      "metadata": {
        "id": "9c422aa0"
      },
      "source": [
        "# Задание 1 (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4a72790",
      "metadata": {
        "id": "e4a72790"
      },
      "source": [
        "Обучите word2vec модели с негативным семплированием (cbow и skip-gram) аналогично тому, как это было сделано в семинаре. Вам нужно изменить следующие пункты:\n",
        "1) добавьте лемматизацию в предобработку (любым способом)  \n",
        "2) измените размер окна в большую или меньшую сторону\n",
        "3) измените размерность итоговых векторов\n",
        "\n",
        "Выберете несколько не похожих по смыслу слов (не таких как в семинаре), и протестируйте полученные эмбединги (найдите ближайшие слова и оцените качество, как в семинаре).\n",
        "Постарайтесь обучать модели как можно дольше и на как можно большем количестве данных. (Но если у вас мало времени или ресурсов, то допустимо взять поменьше данных и поставить меньше эпох)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Qm1m08kFNJA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qm1m08kFNJA",
        "outputId": "73e65066-fbd8-433c-dabe-aa5b2b72e2ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install razdel\n",
        "!pip install pymorphy3\n",
        "!pip install regex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmhC2NpRt_tY",
        "outputId": "7005203b-7a73-4b25-a691-2e67ac5d5bd4"
      },
      "id": "LmhC2NpRt_tY",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting razdel\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n",
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.6-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3) (75.2.0)\n",
            "Downloading pymorphy3-2.0.6-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.6 pymorphy3-dicts-ru-2.4.417150.4580142\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2025.11.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cde5fd96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cde5fd96",
        "outputId": "e26fcaf5-bf83-441f-db3d-cf1c4d323a7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Torch version: 2.9.0+cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import razdel\n",
        "import pymorphy3\n",
        "import regex as re\n",
        "from string import punctuation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "print(\"Torch version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "IjG6R-NqaB-V",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjG6R-NqaB-V",
        "outputId": "1149c6da-e5d9-4f84-de1b-3ad97dddb44e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20003"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "wiki = open('sample_data/wiki_data.txt').read().split('\\n')\n",
        "len(wiki)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "98Uy584-TSaU",
      "metadata": {
        "id": "98Uy584-TSaU"
      },
      "outputs": [],
      "source": [
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "def preprocess_with_lemma(text):\n",
        "    text = text.lower()\n",
        "    tokens = [token.text for token in razdel.tokenize(text)]\n",
        "\n",
        "    lemmatized_tokens = []\n",
        "    for token in tokens:\n",
        "        clean_token = re.sub(r'[^а-яё]', '', token)\n",
        "        if clean_token:\n",
        "            lemma = morph.parse(clean_token)[0].normal_form\n",
        "            lemmatized_tokens.append(lemma)\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "lemmas = [preprocess_with_lemma(sent) for sent in wiki]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "blbYM3MXd5He",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blbYM3MXd5He",
        "outputId": "36c29ec9-eadf-4492-b5c4-afe9f969f346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11693\n",
            "['усик', 'афины', 'отделиться', 'сандра']\n"
          ]
        }
      ],
      "source": [
        "vocab = Counter()\n",
        "for sent in lemmas:\n",
        "    vocab.update(sent)\n",
        "\n",
        "filtered_vocab = set()\n",
        "for word in vocab:\n",
        "    if vocab[word] > 30:\n",
        "        filtered_vocab.add(word)\n",
        "\n",
        "print(len(filtered_vocab))\n",
        "print(list(filtered_vocab)[:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61448x8kaXOo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61448x8kaXOo",
        "outputId": "21b3d024-891f-47b0-82e5-18196923946a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20003\n",
            "[[0, 3796, 4449, 0, 9197, 8360, 11201, 6714, 11671, 3796, 4449, 4908, 11201, 9826, 0, 8589, 8360, 3529, 11201, 5103, 6990, 11202, 1737, 7573, 0, 6130, 5103, 6990, 10169, 1737, 11076, 0, 3114, 3014, 7313, 4966, 0, 4396, 11254, 4966, 8099, 5615, 5724, 5919, 6726, 0, 750, 5936, 8360, 0, 5103, 6130, 6235, 0, 750, 5936, 8360, 8099, 5103, 8000, 0, 8273, 0, 4297, 2922, 11464, 6793, 5656, 578, 8000, 0, 2385, 7315, 8575, 3114, 0, 11065, 6793, 8000, 9781, 0, 3209, 11201, 7903, 6271, 878, 7497, 4089, 7573, 0, 6130, 0, 3529, 1219, 11201, 6130, 5103, 6990, 10169, 11201, 8025, 11671, 4746, 11201, 8360, 1127, 7115, 9132, 8201, 1975, 11201, 8360, 8099, 368, 8360, 3529, 1295, 4037, 8547, 2085, 0, 746, 6130, 0, 6578, 11201, 4321, 1217, 5423, 2243, 2168, 331, 6271, 8360, 0, 2639, 9667, 0, 5687, 9241, 10071, 6130, 4192, 5931, 8729, 7497, 2658, 6130, 0, 2555, 0, 8619, 11201, 6271, 11201, 8360, 9241, 10071, 6130, 4192, 7497, 10091, 11596, 11072, 3114, 212, 6271, 11201, 8360, 8893, 10071, 6130, 4192, 11201, 4321, 1217, 8360, 9953, 3839, 6906, 8202, 6340, 6130, 10775, 4299, 5684, 4623, 8272, 6990, 9533, 1089, 11201, 0, 3529, 3296, 1126, 200, 11201, 5103, 1089, 9533, 11131, 0, 11201, 8360, 3529, 0, 6714, 1963, 11201, 6271, 1727, 10980, 0, 7609, 11201, 694, 11131, 0], [0, 3493, 0, 9248, 3388, 1383, 3493, 4480, 11126, 0, 6130, 0, 0, 3493, 8615, 2168, 6397, 2160, 0, 3910, 9798, 11201, 9094, 11201, 6271, 3493, 6228, 1076, 4340, 3493, 8615, 2168, 6397, 8947, 9256, 10442, 3493, 4283, 7138, 7716, 7582, 50, 0, 6130, 0, 0, 0, 8947, 9256, 10442], [7618, 6360, 1228, 0, 11044, 7115, 7618, 4932, 2680, 3812, 6500, 7138, 6360, 1228, 0, 1020, 5288, 3976, 2213, 6360, 0, 9953, 1910, 6360, 2972, 3114, 4833, 805, 6921, 8746, 9603, 6360, 9106, 9728, 7184, 6360, 596, 9603, 6360, 6990, 11202, 1737, 6360, 0], [0, 0, 0, 5542, 10775, 9381, 11201, 11487, 8140], [0, 1475, 9878, 8411, 0, 1475, 9878, 8411, 1475, 6191, 38, 1678, 5876, 0, 6191, 3535, 1475, 9878, 8411, 331, 10756, 9180, 4932, 6191, 10091, 7852, 0, 181, 3481, 7634, 4093, 2726, 9857, 7100, 1333, 1095, 0, 11201, 9902, 1541, 4801, 7892, 7696, 0, 9878, 3676, 7048, 6248, 7389, 3961, 9746, 330, 11201, 0, 9396, 6130, 10775, 9632, 2922, 7226, 6088, 9484, 11501, 11458, 9878, 8542, 4932, 0, 3242, 8273, 4597, 9505, 0, 7981, 6130, 0, 5367, 6368, 9878, 8542, 2751, 0, 5936, 0, 3481, 9857, 1475, 10091, 5430, 3676, 0, 2727, 10815, 2168, 6609, 5936, 10815, 9878, 8542, 7048, 6248, 4905, 0, 7226, 2593, 9878, 3676, 9857, 0, 3242, 6865, 0, 1886, 9600, 11221, 9857, 1475, 9902, 6191, 11201, 0, 1069, 3961, 2694, 0, 11201, 0, 6130, 4776, 0, 3586, 566, 0, 0, 6130, 7981, 5074], [0, 2733, 0, 8300, 10486, 3885, 2168, 2964, 9093, 11155, 4749, 3980, 11201, 6271, 11201, 5204, 5936, 2168, 6271, 2996, 4340, 424, 3885, 1938, 424, 5954, 11201, 673, 6271, 6340, 11201, 7762, 5936, 6027, 3145, 7497, 8897, 11201, 5780, 7626, 3839, 9985, 4807, 6368, 6130, 3392, 6340, 2922, 8548, 9037, 0, 10564, 6969, 3961, 10433, 8496, 6130, 0, 8121, 11201, 2882, 7021, 8768, 4528, 9037, 0, 10091, 7086, 0, 10939, 6324, 4623, 2174, 4932, 9920, 6578, 11175, 2882, 1589, 0, 0, 6130, 10939, 10158, 3114, 9985, 4807, 5936, 6563, 5029, 6130, 3839, 9293, 468, 2054, 2681, 2733, 13, 0, 6130, 0, 2694, 10433, 8496, 0, 0, 10834, 4478, 5936, 5919, 6130, 7091, 11201, 7480, 6794, 11076, 1727, 6012, 0, 3114, 1502, 0, 11201, 6890, 8849, 11451, 0, 6626, 3114, 4807, 200, 7091, 11201, 5919, 2694, 505, 1737, 2509, 4226, 3812, 1839, 9082, 8673, 5075, 9953, 2943, 1737, 0, 0, 8776, 3984, 984, 3044, 0, 6130, 8686, 8851, 0, 8686, 3208, 6907, 6130, 5886, 3363, 6130, 10023, 4192, 0, 0, 1578, 0, 2943, 8585, 2050, 6153, 7526, 6130, 0, 7276, 0, 1578, 8273, 1624, 8673, 6012, 0, 2973, 0, 0, 2050, 8619, 0, 10527, 4623, 3171, 943, 4900, 9532, 3812, 1839, 0, 11201, 7309, 6361, 6130, 8776, 9853, 0, 8851, 4192, 9363, 11201, 0, 6941, 0, 9106, 9902, 4614, 5936, 2368, 0, 9325, 4641, 1578, 7864, 5080, 6990, 1975, 2608, 4076, 4807, 10915, 4076, 10552, 0, 6729, 1410, 4641, 11108, 7497, 7592, 7912, 3842, 6130, 7594, 8991, 4292, 3386, 6340, 5936, 1886, 8001, 3617, 5303, 5687, 1604, 8429, 1502, 4576, 6130, 6361, 4958, 3812, 1839, 5367, 8776, 3114, 1410, 4576, 8746, 4093, 11043, 5852, 200, 4192, 3143, 8429, 5936, 4576, 11043, 7037, 0, 1578, 6946, 1076, 1886, 2168, 9236, 146, 11201, 1910, 19, 11108, 7497, 2612, 3208, 6291, 6130, 2070, 10508, 11076, 4192, 0, 0, 6130, 0, 3812, 1839, 2759, 3114, 9279, 6667, 9953, 10312, 0, 4192, 7003, 0, 6291, 7297, 3961, 2694, 1127, 4076, 5074, 6946, 7048, 0, 9953, 6311, 9973, 899, 3469, 10841, 11076, 9953, 0, 6601, 7048, 325, 4457, 4192, 8721, 5820, 0, 3114, 867, 4932, 10065, 9403, 9985, 4807, 5844, 0, 1578, 8673, 5936, 1076, 9848, 11172, 1679, 6921, 1220, 5247, 4852, 3687, 1502, 3812, 5275, 10050, 2268, 0, 4093, 10065, 4728, 6340, 0, 3961, 11223, 3812, 1839, 6729, 7497, 1410, 0, 3074, 1502, 0, 4093, 0, 4192, 5844, 0, 9363, 0, 134, 1127, 10613, 3469, 4428, 11201, 0, 7027, 6946, 5844, 0, 10527, 1076, 3469, 9305, 11464, 6340, 11136, 1910, 8121, 8196, 0, 0, 1578, 8673, 5982, 11172, 1910, 146, 11142, 10780, 5110, 6130, 1911, 3812, 1839, 6601, 3535, 5390, 1737, 7826, 8851, 5852, 6601, 10023, 11108, 7497, 5110, 6130, 10780, 4192, 0, 10091, 11223, 8429, 11201, 0, 8389, 6990, 7826, 6130, 0, 4543, 9984, 0, 6907, 2882, 0, 4900, 9037, 5936, 1588, 0, 4900, 7021, 0, 6793, 4623, 5031, 1589, 6088, 2638, 8411, 11201, 7309, 4900, 572, 3114, 1093, 4874, 1797, 10992, 4872, 1410, 4093, 2070, 9488, 200, 5367, 2125, 3114, 8441, 0, 4900, 6228, 2208, 10110, 10091, 11426, 363, 8846, 1886, 10446, 4900, 8278, 10087, 4162, 1415, 6810, 4093, 7852, 0, 9037, 2751, 0, 6130, 0, 0, 0, 7868, 7526, 8379, 6578, 8776, 10954, 8658, 7365, 11313, 6793, 7194, 253, 8374, 10284, 8776, 6157, 8219, 11201, 3885, 9267, 3219, 2076, 10613, 3812, 1839, 1727, 6696, 3114, 5870, 4093, 791, 2168, 9044, 9037, 926, 547, 10424, 5936, 7928, 5620, 6130, 10537, 8121, 4623, 3812, 1839, 5878, 11044, 6153, 9205, 11201, 876, 1737, 3321, 10424, 6601, 6189, 8121, 0, 11201, 791, 6601, 2667, 6500, 9119, 3812, 1839, 10050, 10010, 6130, 8515, 6340, 3961, 5029, 6311, 3382, 1737, 5029, 3607, 3586, 3961, 10698, 3114, 8930, 1839, 0, 3812, 1839, 4932, 4519, 5029, 6130, 2131, 9344, 3114, 8930, 1839, 3812, 4380, 3607, 0, 5936, 382, 10091, 3607, 1492, 9267, 8121, 8435, 9745, 3812, 7459, 5029, 0, 0, 3876, 547, 8748, 5166, 11201, 1410, 7194, 1589, 8374, 0, 8990, 10613, 7507, 5620, 11035, 0, 3920, 0, 8028, 5620, 11183, 6130, 3410, 5844, 11176, 10756, 6758, 0, 873, 6340, 10050, 4558, 11553, 9667, 9037, 3961, 0, 8501, 4093, 9458, 5823, 0, 3920, 3209, 233, 1765, 0, 3920, 6729, 1410, 6723, 5936, 0, 0, 1551, 8273, 11418, 11201, 2882, 6130, 8673, 3564, 6612, 10815, 9037, 11201, 9037, 3920, 7086, 0, 7086, 7508, 10091, 435, 6921, 8760, 0, 11133, 6990, 3930, 6578, 1839, 6130, 4077, 0, 11201, 2624, 8542, 11325, 6663, 175, 0, 1997, 311, 6578, 1839, 6578, 8990, 3705, 5029, 6130, 1589, 3233, 2054, 9674, 11201, 3812, 2882, 1910, 3961, 8374, 1325, 4623, 9985, 4807, 7478, 1818, 2054, 2681, 2085, 3961, 9106, 211, 11176, 10284, 1585, 10284, 9037, 3920, 1938, 424, 6090, 9953, 8776, 9433, 4297, 9433, 7101, 596, 1043], [0, 3796, 4449, 0, 9197, 8360, 11201, 6714, 11671, 3796, 4449, 4908, 11201, 9826, 0, 8589, 3529, 11201, 9464, 6990, 11202, 1737, 7573, 0, 3114, 3014, 7313, 4966, 8099, 5615, 5724, 5919, 6726, 6235, 0, 750, 5936, 8360, 8099, 5103, 6130, 7573, 0, 5103, 11201, 9464, 3529, 0, 7120, 5916, 7674, 516, 6130, 3386, 1998, 3208, 8273, 3535, 4192, 11076, 0, 2385, 7315, 8575, 3114, 0, 3746, 0, 8771, 0, 7864, 9325, 3357, 10181, 6269, 0, 4093, 3357, 3114, 0, 9127, 11201, 6271, 1325, 0, 7573, 0, 8059, 3001, 200, 11201, 6271, 10082, 11201, 2613, 0, 507, 10906, 4889, 6135, 3357, 0, 11201, 9325, 0, 11325, 6500, 5823, 11201, 10324, 200, 11201, 6271, 2858, 11201, 4340, 8940, 7903, 6271, 3209, 4192, 4089, 7573, 0, 6130, 0, 8025, 11671, 4746, 11201, 8360, 1127, 7115, 9132, 8201, 6130, 0, 3529, 11201, 8360, 8099, 368, 8360, 3529, 1295, 4037, 2085, 0, 11201, 4321, 1217, 5423, 2243, 2168, 331, 6271, 11201, 8360, 0, 9241, 10071, 6130, 4192, 5931, 7497, 2658, 6130, 0, 2555, 0, 8619, 11201, 6271, 11201, 8360, 9241, 10071, 6130, 4192, 9441, 0, 11072, 3114, 212, 6271, 11201, 8360, 8893, 9209, 6130, 4192, 11201, 4321, 1217, 957, 4192, 8619, 8360, 0, 7141, 10071, 11106, 9667, 6720, 8893, 1031, 0, 11201, 7449, 0, 1737, 8360, 3529, 10613, 7497, 0, 9533, 11131, 0], [0, 0, 4966, 11201, 8790, 3470, 5103, 9236, 1077, 1659, 5103, 9564, 1848, 11201, 1911, 9746, 7661, 7543, 11201, 3568, 414, 5074, 4966, 11106, 4932, 0], [0, 9197, 4271, 2752, 4449, 0, 9197, 4271, 5577, 11017, 8681, 11201, 0, 11671, 2752, 4449, 11487, 11564, 2102, 1975, 8360, 0, 0, 9197, 4271, 9656, 10000, 6271, 11201, 11089, 5936, 115, 2752, 4449, 11201, 8121, 9826, 8916, 11451, 0, 6130, 0, 9197, 4015, 115, 2752, 4449, 1737, 9905, 6271, 212, 6271, 9441, 11017, 8681, 0, 11017, 11671, 0, 118, 4271, 0, 0, 0, 9197, 4271, 6130, 9197, 4271, 0, 2580, 1727, 11535, 5876, 3961, 9793, 11201, 0, 118, 4015, 3114, 212, 6271, 7497, 6595, 9305, 4192, 11201, 8568, 8568, 11201, 11072, 1737, 2509, 6271, 11596, 11464, 8568, 8846, 8568, 2379, 8568, 3632, 10841, 11596, 11464, 10315, 7142, 3311, 11017, 8681, 11201, 9826, 9197, 4271, 4908, 6466, 521, 9236, 4271, 5529, 7442, 9857, 11044, 5919, 4221, 10356, 7442, 9236, 8116, 11083, 10356, 5438, 7442, 3114, 11451, 11017, 8681, 4626, 7404, 8443, 0, 10593, 6544, 5247, 0, 8893, 3216, 7058, 38, 11451, 9197, 4271, 6294, 2753, 6570, 11523, 0, 0, 11523, 0, 11523, 0, 11523, 0, 11523, 0, 6130, 11523, 0, 38, 0, 11523, 0, 200, 5367, 6736, 5498, 11523, 0, 8392, 6130, 11523, 0, 5936, 11318, 11201, 8360, 0, 0, 0, 10095, 0], [0, 3671, 8545, 0, 7573, 3114, 4966, 0, 11201, 5103, 9728, 0, 3696, 1737, 5769, 11325, 11201, 0, 4908, 11201, 9826, 0, 9197, 4271, 0, 11671, 3671, 8545, 4312, 806, 9667, 8771, 0, 11201, 5594, 11201, 11044, 984, 6271, 581, 2006, 1737, 3703, 7497, 8938, 5562, 4046, 3839, 3632, 3973, 11201, 6271, 7728, 0, 4312, 1727, 4823, 0, 3207, 0, 1786, 4107, 2156, 7497, 0, 8252, 11464, 0, 7142, 6271]]\n"
          ]
        }
      ],
      "source": [
        "word2id = {'UNK': 0}\n",
        "for word in filtered_vocab:\n",
        "    word2id[word] = len(word2id)\n",
        "\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "\n",
        "indexed_wiki = []\n",
        "for sent in lemmas:\n",
        "    indexed_sent = [word2id.get(word, word2id['UNK']) for word in sent]\n",
        "    indexed_wiki.append(indexed_sent)\n",
        "\n",
        "print(len(indexed_wiki))\n",
        "print(indexed_wiki[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5UPBVeb6XFdU",
      "metadata": {
        "id": "5UPBVeb6XFdU"
      },
      "source": [
        "cbow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb9DroXKWdyJ",
      "metadata": {
        "id": "fb9DroXKWdyJ"
      },
      "outputs": [],
      "source": [
        "def gen_batches_cbow_neg(indexed_wiki, window_size=3, batch_size=512, n_neg=5):\n",
        "    X_context = []\n",
        "    y_target = []\n",
        "    labels = []\n",
        "\n",
        "    for sent in indexed_wiki:\n",
        "        for i in range(len(sent)):\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(sent), i + window_size + 1)\n",
        "\n",
        "            context = sent[start:i] + sent[i+1:end]\n",
        "\n",
        "            if len(context) < window_size * 2:\n",
        "                continue\n",
        "\n",
        "            X_context.append(context)\n",
        "            y_target.append(sent[i])\n",
        "            labels.append(1)\n",
        "\n",
        "            for _ in range(n_neg):\n",
        "                X_context.append(context)\n",
        "                y_target.append(np.random.randint(0, len(word2id)))\n",
        "                labels.append(0)\n",
        "\n",
        "            if len(X_context) >= batch_size:\n",
        "                yield (torch.LongTensor(X_context),\n",
        "                       torch.LongTensor(y_target),\n",
        "                       torch.FloatTensor(labels))\n",
        "                X_context, y_target, labels = [], [], []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gc2AOV6qfNUJ",
      "metadata": {
        "id": "Gc2AOV6qfNUJ"
      },
      "outputs": [],
      "source": [
        "class CBOWModelNeg(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=256):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.out_embeddings = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "    def forward(self, context, target):\n",
        "        context_emb = self.embeddings(context).mean(dim=1)\n",
        "        target_emb = self.out_embeddings(target)\n",
        "\n",
        "        scores = (context_emb * target_emb).sum(dim=1)\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oneMQFoLgAt-",
      "metadata": {
        "id": "oneMQFoLgAt-"
      },
      "outputs": [],
      "source": [
        "EMB_DIM = 256\n",
        "WINDOW_SIZE = 3\n",
        "VOCAB_SIZE = len(word2id)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "cbow_model = CBOWModelNeg(VOCAB_SIZE, EMB_DIM).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(cbow_model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a6Ir-sXgOKe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a6Ir-sXgOKe",
        "outputId": "13de979e-1c15-4e85-bc0a-169fda8053d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CBOW | Epoch 1 | Batch 500 | Loss: 2.5747\n",
            "CBOW | Epoch 1 | Batch 1000 | Loss: 2.2913\n",
            "CBOW | Epoch 1 | Batch 1500 | Loss: 1.9957\n",
            "CBOW | Epoch 1 | Batch 2000 | Loss: 1.7838\n",
            "CBOW | Epoch 1 | Batch 2500 | Loss: 1.5694\n",
            "CBOW | Epoch 1 | Batch 3000 | Loss: 1.3176\n",
            "CBOW | Epoch 1 | Batch 3500 | Loss: 1.1676\n",
            "CBOW | Epoch 1 | Batch 4000 | Loss: 1.0492\n",
            "CBOW | Epoch 1 | Batch 4500 | Loss: 0.9737\n",
            "CBOW | Epoch 1 | Batch 5000 | Loss: 0.9472\n",
            "CBOW | Epoch 1 | Batch 5500 | Loss: 0.9390\n",
            "CBOW | Epoch 1 | Batch 6000 | Loss: 0.7155\n",
            "CBOW | Epoch 1 | Batch 6500 | Loss: 0.8449\n",
            "CBOW | Epoch 1 | Batch 7000 | Loss: 0.8173\n",
            "CBOW | Epoch 1 | Batch 7500 | Loss: 0.8422\n",
            "CBOW | Epoch 1 | Batch 8000 | Loss: 0.7902\n",
            "CBOW | Epoch 1 | Batch 8500 | Loss: 0.7830\n",
            "CBOW | Epoch 1 | Batch 9000 | Loss: 0.7791\n",
            "CBOW | Epoch 1 | Batch 9500 | Loss: 0.7212\n",
            "CBOW | Epoch 1 | Batch 10000 | Loss: 0.7292\n",
            "CBOW | Epoch 1 | Batch 10500 | Loss: 0.7312\n",
            "CBOW | Epoch 1 | Batch 11000 | Loss: 0.7259\n",
            "CBOW | Epoch 1 | Batch 11500 | Loss: 0.7141\n",
            "CBOW | Epoch 1 | Batch 12000 | Loss: 0.7081\n",
            "CBOW | Epoch 1 | Batch 12500 | Loss: 0.6048\n",
            "CBOW | Epoch 1 | Batch 13000 | Loss: 0.6649\n",
            "CBOW | Epoch 1 | Batch 13500 | Loss: 0.6131\n",
            "CBOW | Epoch 1 | Batch 14000 | Loss: 0.6112\n",
            "CBOW | Epoch 1 | Batch 14500 | Loss: 0.6428\n",
            "CBOW | Epoch 1 | Batch 15000 | Loss: 0.6241\n",
            "CBOW | Epoch 1 | Batch 15500 | Loss: 0.6106\n",
            "CBOW | Epoch 1 | Batch 16000 | Loss: 0.6271\n",
            "CBOW | Epoch 1 | Batch 16500 | Loss: 0.6293\n",
            "CBOW | Epoch 1 | Batch 17000 | Loss: 0.6257\n",
            "CBOW | Epoch 1 | Batch 17500 | Loss: 0.6109\n",
            "CBOW | Epoch 1 | Batch 18000 | Loss: 0.5943\n",
            "CBOW | Epoch 1 | Batch 18500 | Loss: 0.6142\n",
            "CBOW | Epoch 1 | Batch 19000 | Loss: 0.5779\n",
            "CBOW | Epoch 1 | Batch 19500 | Loss: 0.5661\n",
            "CBOW | Epoch 1 | Batch 20000 | Loss: 0.5677\n",
            "CBOW | Epoch 1 | Batch 20500 | Loss: 0.3215\n",
            "CBOW | Epoch 1 | Batch 21000 | Loss: 0.4641\n",
            "CBOW | Epoch 1 | Batch 21500 | Loss: 0.5536\n",
            "CBOW | Epoch 1 | Batch 22000 | Loss: 0.5025\n",
            "CBOW | Epoch 1 | Batch 22500 | Loss: 0.5362\n",
            "CBOW | Epoch 1 | Batch 23000 | Loss: 0.5475\n",
            "CBOW | Epoch 1 | Batch 23500 | Loss: 0.5324\n",
            "CBOW | Epoch 1 | Batch 24000 | Loss: 0.5277\n",
            "CBOW | Epoch 1 | Batch 24500 | Loss: 0.5226\n",
            "CBOW | Epoch 1 | Batch 25000 | Loss: 0.5336\n",
            "CBOW | Epoch 1 | Batch 25500 | Loss: 0.5102\n",
            "CBOW | Epoch 1 | Batch 26000 | Loss: 0.5055\n",
            "CBOW | Epoch 1 | Batch 26500 | Loss: 0.4987\n",
            "CBOW | Epoch 1 | Batch 27000 | Loss: 0.5088\n",
            "CBOW | Epoch 1 | Batch 27500 | Loss: 0.4826\n",
            "CBOW | Epoch 1 | Batch 28000 | Loss: 0.4794\n",
            "CBOW | Epoch 1 | Batch 28500 | Loss: 0.5033\n",
            "CBOW | Epoch 1 | Batch 29000 | Loss: 0.4894\n",
            "CBOW | Epoch 1 | Batch 29500 | Loss: 0.4721\n",
            "CBOW | Epoch 1 | Batch 30000 | Loss: 0.4512\n",
            "CBOW | Epoch 1 | Batch 30500 | Loss: 0.4670\n",
            "CBOW | Epoch 1 | Batch 31000 | Loss: 0.4746\n",
            "CBOW | Epoch 1 | Batch 31500 | Loss: 0.4636\n",
            "CBOW | Epoch 1 | Batch 32000 | Loss: 0.4571\n",
            "CBOW | Epoch 1 | Batch 32500 | Loss: 0.4348\n",
            "CBOW | Epoch 1 | Batch 33000 | Loss: 0.4188\n",
            "CBOW | Epoch 1 | Batch 33500 | Loss: 0.4634\n",
            "CBOW | Epoch 1 | Batch 34000 | Loss: 0.4588\n",
            "CBOW | Epoch 1 | Batch 34500 | Loss: 0.4474\n",
            "CBOW | Epoch 1 | Batch 35000 | Loss: 0.3821\n",
            "CBOW | Epoch 1 | Batch 35500 | Loss: 0.4236\n",
            "CBOW | Epoch 1 | Batch 36000 | Loss: 0.4352\n",
            "CBOW | Epoch 1 | Batch 36500 | Loss: 0.4130\n",
            "CBOW | Epoch 1 | Batch 37000 | Loss: 0.4209\n",
            "CBOW | Epoch 1 | Batch 37500 | Loss: 0.4123\n",
            "CBOW | Epoch 1 | Batch 38000 | Loss: 0.4011\n",
            "CBOW | Epoch 1 | Batch 38500 | Loss: 0.4031\n",
            "CBOW | Epoch 1 | Batch 39000 | Loss: 0.4168\n",
            "CBOW | Epoch 1 | Batch 39500 | Loss: 0.4034\n",
            "CBOW | Epoch 1 | Batch 40000 | Loss: 0.3928\n",
            "CBOW | Epoch 1 | Batch 40500 | Loss: 0.4226\n",
            "CBOW | Epoch 1 | Batch 41000 | Loss: 0.3859\n",
            "CBOW | Epoch 1 | Batch 41500 | Loss: 0.3976\n",
            "CBOW | Epoch 1 | Batch 42000 | Loss: 0.3897\n",
            "CBOW | Epoch 1 | Batch 42500 | Loss: 0.3599\n",
            "CBOW | Epoch 1 | Batch 43000 | Loss: 0.3923\n",
            "CBOW | Epoch 1 | Batch 43500 | Loss: 0.4015\n",
            "CBOW | Epoch 1 | Batch 44000 | Loss: 0.3803\n",
            "CBOW | Epoch 1 | Batch 44500 | Loss: 0.3901\n",
            "CBOW | Epoch 1 | Batch 45000 | Loss: 0.3588\n",
            "CBOW | Epoch 1 | Batch 45500 | Loss: 0.3727\n",
            "CBOW | Epoch 1 | Batch 46000 | Loss: 0.3618\n",
            "CBOW | Epoch 1 | Batch 46500 | Loss: 0.3335\n",
            "CBOW | Epoch 1 | Batch 47000 | Loss: 0.3629\n",
            "CBOW | Epoch 1 | Batch 47500 | Loss: 0.3603\n",
            "CBOW | Epoch 1 | Batch 48000 | Loss: 0.3513\n",
            "CBOW | Epoch 1 | Batch 48500 | Loss: 0.3587\n",
            "CBOW | Epoch 1 | Batch 49000 | Loss: 0.3580\n",
            "CBOW | Epoch 1 | Batch 49500 | Loss: 0.3545\n",
            "CBOW | Epoch 1 | Batch 50000 | Loss: 0.3543\n",
            "CBOW | Epoch 1 | Batch 50500 | Loss: 0.3396\n",
            "CBOW | Epoch 1 | Batch 51000 | Loss: 0.3601\n",
            "CBOW | Epoch 1 | Batch 51500 | Loss: 0.3615\n",
            "CBOW | Epoch 1 | Batch 52000 | Loss: 0.3293\n",
            "CBOW | Epoch 1 | Batch 52500 | Loss: 0.3480\n",
            "CBOW | Epoch 1 | Batch 53000 | Loss: 0.3459\n",
            "CBOW Epoch 1. Средний Loss: 0.6069\n",
            "CBOW | Epoch 2 | Batch 500 | Loss: 0.3134\n",
            "CBOW | Epoch 2 | Batch 1000 | Loss: 0.3242\n",
            "CBOW | Epoch 2 | Batch 1500 | Loss: 0.3031\n",
            "CBOW | Epoch 2 | Batch 2000 | Loss: 0.3281\n",
            "CBOW | Epoch 2 | Batch 2500 | Loss: 0.3175\n",
            "CBOW | Epoch 2 | Batch 3000 | Loss: 0.3013\n",
            "CBOW | Epoch 2 | Batch 3500 | Loss: 0.3037\n",
            "CBOW | Epoch 2 | Batch 4000 | Loss: 0.3075\n",
            "CBOW | Epoch 2 | Batch 4500 | Loss: 0.2943\n",
            "CBOW | Epoch 2 | Batch 5000 | Loss: 0.2992\n",
            "CBOW | Epoch 2 | Batch 5500 | Loss: 0.3166\n",
            "CBOW | Epoch 2 | Batch 6000 | Loss: 0.2092\n",
            "CBOW | Epoch 2 | Batch 6500 | Loss: 0.2889\n",
            "CBOW | Epoch 2 | Batch 7000 | Loss: 0.2803\n",
            "CBOW | Epoch 2 | Batch 7500 | Loss: 0.3067\n",
            "CBOW | Epoch 2 | Batch 8000 | Loss: 0.2907\n",
            "CBOW | Epoch 2 | Batch 8500 | Loss: 0.2877\n",
            "CBOW | Epoch 2 | Batch 9000 | Loss: 0.2907\n",
            "CBOW | Epoch 2 | Batch 9500 | Loss: 0.2658\n",
            "CBOW | Epoch 2 | Batch 10000 | Loss: 0.2725\n",
            "CBOW | Epoch 2 | Batch 10500 | Loss: 0.2848\n",
            "CBOW | Epoch 2 | Batch 11000 | Loss: 0.2861\n",
            "CBOW | Epoch 2 | Batch 11500 | Loss: 0.2818\n",
            "CBOW | Epoch 2 | Batch 12000 | Loss: 0.2725\n",
            "CBOW | Epoch 2 | Batch 12500 | Loss: 0.2437\n",
            "CBOW | Epoch 2 | Batch 13000 | Loss: 0.2680\n",
            "CBOW | Epoch 2 | Batch 13500 | Loss: 0.2453\n",
            "CBOW | Epoch 2 | Batch 14000 | Loss: 0.2478\n",
            "CBOW | Epoch 2 | Batch 14500 | Loss: 0.2733\n",
            "CBOW | Epoch 2 | Batch 15000 | Loss: 0.2599\n",
            "CBOW | Epoch 2 | Batch 15500 | Loss: 0.2555\n",
            "CBOW | Epoch 2 | Batch 16000 | Loss: 0.2685\n",
            "CBOW | Epoch 2 | Batch 16500 | Loss: 0.2773\n",
            "CBOW | Epoch 2 | Batch 17000 | Loss: 0.2675\n",
            "CBOW | Epoch 2 | Batch 17500 | Loss: 0.2723\n",
            "CBOW | Epoch 2 | Batch 18000 | Loss: 0.2623\n",
            "CBOW | Epoch 2 | Batch 18500 | Loss: 0.2825\n",
            "CBOW | Epoch 2 | Batch 19000 | Loss: 0.2616\n",
            "CBOW | Epoch 2 | Batch 19500 | Loss: 0.2531\n",
            "CBOW | Epoch 2 | Batch 20000 | Loss: 0.2657\n",
            "CBOW | Epoch 2 | Batch 20500 | Loss: 0.1253\n",
            "CBOW | Epoch 2 | Batch 21000 | Loss: 0.2153\n",
            "CBOW | Epoch 2 | Batch 21500 | Loss: 0.2565\n",
            "CBOW | Epoch 2 | Batch 22000 | Loss: 0.2222\n",
            "CBOW | Epoch 2 | Batch 22500 | Loss: 0.2521\n",
            "CBOW | Epoch 2 | Batch 23000 | Loss: 0.2493\n",
            "CBOW | Epoch 2 | Batch 23500 | Loss: 0.2472\n",
            "CBOW | Epoch 2 | Batch 24000 | Loss: 0.2537\n",
            "CBOW | Epoch 2 | Batch 24500 | Loss: 0.2510\n",
            "CBOW | Epoch 2 | Batch 25000 | Loss: 0.2632\n",
            "CBOW | Epoch 2 | Batch 25500 | Loss: 0.2504\n",
            "CBOW | Epoch 2 | Batch 26000 | Loss: 0.2482\n",
            "CBOW | Epoch 2 | Batch 26500 | Loss: 0.2383\n",
            "CBOW | Epoch 2 | Batch 27000 | Loss: 0.2494\n",
            "CBOW | Epoch 2 | Batch 27500 | Loss: 0.2320\n",
            "CBOW | Epoch 2 | Batch 28000 | Loss: 0.2361\n",
            "CBOW | Epoch 2 | Batch 28500 | Loss: 0.2522\n",
            "CBOW | Epoch 2 | Batch 29000 | Loss: 0.2418\n",
            "CBOW | Epoch 2 | Batch 29500 | Loss: 0.2396\n",
            "CBOW | Epoch 2 | Batch 30000 | Loss: 0.2273\n",
            "CBOW | Epoch 2 | Batch 30500 | Loss: 0.2397\n",
            "CBOW | Epoch 2 | Batch 31000 | Loss: 0.2414\n",
            "CBOW | Epoch 2 | Batch 31500 | Loss: 0.2421\n",
            "CBOW | Epoch 2 | Batch 32000 | Loss: 0.2443\n",
            "CBOW | Epoch 2 | Batch 32500 | Loss: 0.2279\n",
            "CBOW | Epoch 2 | Batch 33000 | Loss: 0.2194\n",
            "CBOW | Epoch 2 | Batch 33500 | Loss: 0.2457\n",
            "CBOW | Epoch 2 | Batch 34000 | Loss: 0.2418\n",
            "CBOW | Epoch 2 | Batch 34500 | Loss: 0.2389\n",
            "CBOW | Epoch 2 | Batch 35000 | Loss: 0.1951\n",
            "CBOW | Epoch 2 | Batch 35500 | Loss: 0.2202\n",
            "CBOW | Epoch 2 | Batch 36000 | Loss: 0.2335\n",
            "CBOW | Epoch 2 | Batch 36500 | Loss: 0.2246\n",
            "CBOW | Epoch 2 | Batch 37000 | Loss: 0.2273\n",
            "CBOW | Epoch 2 | Batch 37500 | Loss: 0.2190\n",
            "CBOW | Epoch 2 | Batch 38000 | Loss: 0.2150\n",
            "CBOW | Epoch 2 | Batch 38500 | Loss: 0.2169\n",
            "CBOW | Epoch 2 | Batch 39000 | Loss: 0.2256\n",
            "CBOW | Epoch 2 | Batch 39500 | Loss: 0.2262\n",
            "CBOW | Epoch 2 | Batch 40000 | Loss: 0.2171\n",
            "CBOW | Epoch 2 | Batch 40500 | Loss: 0.2347\n",
            "CBOW | Epoch 2 | Batch 41000 | Loss: 0.2147\n",
            "CBOW | Epoch 2 | Batch 41500 | Loss: 0.2226\n",
            "CBOW | Epoch 2 | Batch 42000 | Loss: 0.2169\n",
            "CBOW | Epoch 2 | Batch 42500 | Loss: 0.2037\n",
            "CBOW | Epoch 2 | Batch 43000 | Loss: 0.2215\n",
            "CBOW | Epoch 2 | Batch 43500 | Loss: 0.2307\n",
            "CBOW | Epoch 2 | Batch 44000 | Loss: 0.2125\n",
            "CBOW | Epoch 2 | Batch 44500 | Loss: 0.2247\n",
            "CBOW | Epoch 2 | Batch 45000 | Loss: 0.2022\n",
            "CBOW | Epoch 2 | Batch 45500 | Loss: 0.2122\n",
            "CBOW | Epoch 2 | Batch 46000 | Loss: 0.2083\n",
            "CBOW | Epoch 2 | Batch 46500 | Loss: 0.1929\n",
            "CBOW | Epoch 2 | Batch 47000 | Loss: 0.2108\n",
            "CBOW | Epoch 2 | Batch 47500 | Loss: 0.2086\n",
            "CBOW | Epoch 2 | Batch 48000 | Loss: 0.2053\n",
            "CBOW | Epoch 2 | Batch 48500 | Loss: 0.2081\n",
            "CBOW | Epoch 2 | Batch 49000 | Loss: 0.2185\n",
            "CBOW | Epoch 2 | Batch 49500 | Loss: 0.2093\n",
            "CBOW | Epoch 2 | Batch 50000 | Loss: 0.2133\n",
            "CBOW | Epoch 2 | Batch 50500 | Loss: 0.2021\n",
            "CBOW | Epoch 2 | Batch 51000 | Loss: 0.2154\n",
            "CBOW | Epoch 2 | Batch 51500 | Loss: 0.2212\n",
            "CBOW | Epoch 2 | Batch 52000 | Loss: 0.1999\n",
            "CBOW | Epoch 2 | Batch 52500 | Loss: 0.2127\n",
            "CBOW | Epoch 2 | Batch 53000 | Loss: 0.2088\n",
            "CBOW Epoch 2. Средний Loss: 0.2460\n",
            "CBOW | Epoch 3 | Batch 500 | Loss: 0.1942\n",
            "CBOW | Epoch 3 | Batch 1000 | Loss: 0.2017\n",
            "CBOW | Epoch 3 | Batch 1500 | Loss: 0.1912\n",
            "CBOW | Epoch 3 | Batch 2000 | Loss: 0.2050\n",
            "CBOW | Epoch 3 | Batch 2500 | Loss: 0.2023\n",
            "CBOW | Epoch 3 | Batch 3000 | Loss: 0.1955\n",
            "CBOW | Epoch 3 | Batch 3500 | Loss: 0.1935\n",
            "CBOW | Epoch 3 | Batch 4000 | Loss: 0.1987\n",
            "CBOW | Epoch 3 | Batch 4500 | Loss: 0.1902\n",
            "CBOW | Epoch 3 | Batch 5000 | Loss: 0.1912\n",
            "CBOW | Epoch 3 | Batch 5500 | Loss: 0.1987\n",
            "CBOW | Epoch 3 | Batch 6000 | Loss: 0.1342\n",
            "CBOW | Epoch 3 | Batch 6500 | Loss: 0.1892\n",
            "CBOW | Epoch 3 | Batch 7000 | Loss: 0.1820\n",
            "CBOW | Epoch 3 | Batch 7500 | Loss: 0.2014\n",
            "CBOW | Epoch 3 | Batch 8000 | Loss: 0.1929\n",
            "CBOW | Epoch 3 | Batch 8500 | Loss: 0.1880\n",
            "CBOW | Epoch 3 | Batch 9000 | Loss: 0.1911\n",
            "CBOW | Epoch 3 | Batch 9500 | Loss: 0.1729\n",
            "CBOW | Epoch 3 | Batch 10000 | Loss: 0.1820\n",
            "CBOW | Epoch 3 | Batch 10500 | Loss: 0.1919\n",
            "CBOW | Epoch 3 | Batch 11000 | Loss: 0.1883\n",
            "CBOW | Epoch 3 | Batch 11500 | Loss: 0.1886\n",
            "CBOW | Epoch 3 | Batch 12000 | Loss: 0.1795\n",
            "CBOW | Epoch 3 | Batch 12500 | Loss: 0.1654\n",
            "CBOW | Epoch 3 | Batch 13000 | Loss: 0.1804\n",
            "CBOW | Epoch 3 | Batch 13500 | Loss: 0.1675\n",
            "CBOW | Epoch 3 | Batch 14000 | Loss: 0.1666\n",
            "CBOW | Epoch 3 | Batch 14500 | Loss: 0.1869\n",
            "CBOW | Epoch 3 | Batch 15000 | Loss: 0.1792\n",
            "CBOW | Epoch 3 | Batch 15500 | Loss: 0.1740\n",
            "CBOW | Epoch 3 | Batch 16000 | Loss: 0.1834\n",
            "CBOW | Epoch 3 | Batch 16500 | Loss: 0.1916\n",
            "CBOW | Epoch 3 | Batch 17000 | Loss: 0.1820\n",
            "CBOW | Epoch 3 | Batch 17500 | Loss: 0.1867\n",
            "CBOW | Epoch 3 | Batch 18000 | Loss: 0.1819\n",
            "CBOW | Epoch 3 | Batch 18500 | Loss: 0.1982\n",
            "CBOW | Epoch 3 | Batch 19000 | Loss: 0.1831\n",
            "CBOW | Epoch 3 | Batch 19500 | Loss: 0.1768\n",
            "CBOW | Epoch 3 | Batch 20000 | Loss: 0.1895\n",
            "CBOW | Epoch 3 | Batch 20500 | Loss: 0.0856\n",
            "CBOW | Epoch 3 | Batch 21000 | Loss: 0.1506\n",
            "CBOW | Epoch 3 | Batch 21500 | Loss: 0.1814\n",
            "CBOW | Epoch 3 | Batch 22000 | Loss: 0.1565\n",
            "CBOW | Epoch 3 | Batch 22500 | Loss: 0.1775\n",
            "CBOW | Epoch 3 | Batch 23000 | Loss: 0.1758\n",
            "CBOW | Epoch 3 | Batch 23500 | Loss: 0.1773\n",
            "CBOW | Epoch 3 | Batch 24000 | Loss: 0.1811\n",
            "CBOW | Epoch 3 | Batch 24500 | Loss: 0.1803\n",
            "CBOW | Epoch 3 | Batch 25000 | Loss: 0.1896\n",
            "CBOW | Epoch 3 | Batch 25500 | Loss: 0.1787\n",
            "CBOW | Epoch 3 | Batch 26000 | Loss: 0.1802\n",
            "CBOW | Epoch 3 | Batch 26500 | Loss: 0.1705\n",
            "CBOW | Epoch 3 | Batch 27000 | Loss: 0.1802\n",
            "CBOW | Epoch 3 | Batch 27500 | Loss: 0.1657\n",
            "CBOW | Epoch 3 | Batch 28000 | Loss: 0.1716\n",
            "CBOW | Epoch 3 | Batch 28500 | Loss: 0.1822\n",
            "CBOW | Epoch 3 | Batch 29000 | Loss: 0.1761\n",
            "CBOW | Epoch 3 | Batch 29500 | Loss: 0.1744\n",
            "CBOW | Epoch 3 | Batch 30000 | Loss: 0.1680\n",
            "CBOW | Epoch 3 | Batch 30500 | Loss: 0.1779\n",
            "CBOW | Epoch 3 | Batch 31000 | Loss: 0.1755\n",
            "CBOW | Epoch 3 | Batch 31500 | Loss: 0.1816\n",
            "CBOW | Epoch 3 | Batch 32000 | Loss: 0.1837\n",
            "CBOW | Epoch 3 | Batch 32500 | Loss: 0.1676\n",
            "CBOW | Epoch 3 | Batch 33000 | Loss: 0.1623\n",
            "CBOW | Epoch 3 | Batch 33500 | Loss: 0.1821\n",
            "CBOW | Epoch 3 | Batch 34000 | Loss: 0.1798\n",
            "CBOW | Epoch 3 | Batch 34500 | Loss: 0.1775\n",
            "CBOW | Epoch 3 | Batch 35000 | Loss: 0.1446\n",
            "CBOW | Epoch 3 | Batch 35500 | Loss: 0.1642\n",
            "CBOW | Epoch 3 | Batch 36000 | Loss: 0.1757\n",
            "CBOW | Epoch 3 | Batch 36500 | Loss: 0.1671\n",
            "CBOW | Epoch 3 | Batch 37000 | Loss: 0.1690\n",
            "CBOW | Epoch 3 | Batch 37500 | Loss: 0.1619\n",
            "CBOW | Epoch 3 | Batch 38000 | Loss: 0.1631\n",
            "CBOW | Epoch 3 | Batch 38500 | Loss: 0.1628\n",
            "CBOW | Epoch 3 | Batch 39000 | Loss: 0.1690\n",
            "CBOW | Epoch 3 | Batch 39500 | Loss: 0.1703\n",
            "CBOW | Epoch 3 | Batch 40000 | Loss: 0.1634\n",
            "CBOW | Epoch 3 | Batch 40500 | Loss: 0.1777\n",
            "CBOW | Epoch 3 | Batch 41000 | Loss: 0.1632\n",
            "CBOW | Epoch 3 | Batch 41500 | Loss: 0.1702\n",
            "CBOW | Epoch 3 | Batch 42000 | Loss: 0.1662\n",
            "CBOW | Epoch 3 | Batch 42500 | Loss: 0.1567\n",
            "CBOW | Epoch 3 | Batch 43000 | Loss: 0.1678\n",
            "CBOW | Epoch 3 | Batch 43500 | Loss: 0.1741\n",
            "CBOW | Epoch 3 | Batch 44000 | Loss: 0.1609\n",
            "CBOW | Epoch 3 | Batch 44500 | Loss: 0.1723\n",
            "CBOW | Epoch 3 | Batch 45000 | Loss: 0.1544\n",
            "CBOW | Epoch 3 | Batch 45500 | Loss: 0.1609\n",
            "CBOW | Epoch 3 | Batch 46000 | Loss: 0.1618\n",
            "CBOW | Epoch 3 | Batch 46500 | Loss: 0.1502\n",
            "CBOW | Epoch 3 | Batch 47000 | Loss: 0.1611\n",
            "CBOW | Epoch 3 | Batch 47500 | Loss: 0.1596\n",
            "CBOW | Epoch 3 | Batch 48000 | Loss: 0.1572\n",
            "CBOW | Epoch 3 | Batch 48500 | Loss: 0.1615\n",
            "CBOW | Epoch 3 | Batch 49000 | Loss: 0.1724\n",
            "CBOW | Epoch 3 | Batch 49500 | Loss: 0.1613\n",
            "CBOW | Epoch 3 | Batch 50000 | Loss: 0.1673\n",
            "CBOW | Epoch 3 | Batch 50500 | Loss: 0.1570\n",
            "CBOW | Epoch 3 | Batch 51000 | Loss: 0.1658\n",
            "CBOW | Epoch 3 | Batch 51500 | Loss: 0.1735\n",
            "CBOW | Epoch 3 | Batch 52000 | Loss: 0.1580\n",
            "CBOW | Epoch 3 | Batch 52500 | Loss: 0.1678\n",
            "CBOW | Epoch 3 | Batch 53000 | Loss: 0.1638\n",
            "CBOW Epoch 3. Средний Loss: 0.1746\n",
            "CBOW | Epoch 4 | Batch 500 | Loss: 0.1543\n",
            "CBOW | Epoch 4 | Batch 1000 | Loss: 0.1613\n",
            "CBOW | Epoch 4 | Batch 1500 | Loss: 0.1538\n",
            "CBOW | Epoch 4 | Batch 2000 | Loss: 0.1632\n",
            "CBOW | Epoch 4 | Batch 2500 | Loss: 0.1641\n",
            "CBOW | Epoch 4 | Batch 3000 | Loss: 0.1584\n",
            "CBOW | Epoch 4 | Batch 3500 | Loss: 0.1556\n",
            "CBOW | Epoch 4 | Batch 4000 | Loss: 0.1602\n",
            "CBOW | Epoch 4 | Batch 4500 | Loss: 0.1536\n",
            "CBOW | Epoch 4 | Batch 5000 | Loss: 0.1543\n",
            "CBOW | Epoch 4 | Batch 5500 | Loss: 0.1604\n",
            "CBOW | Epoch 4 | Batch 6000 | Loss: 0.1069\n",
            "CBOW | Epoch 4 | Batch 6500 | Loss: 0.1538\n",
            "CBOW | Epoch 4 | Batch 7000 | Loss: 0.1471\n",
            "CBOW | Epoch 4 | Batch 7500 | Loss: 0.1631\n",
            "CBOW | Epoch 4 | Batch 8000 | Loss: 0.1557\n",
            "CBOW | Epoch 4 | Batch 8500 | Loss: 0.1522\n",
            "CBOW | Epoch 4 | Batch 9000 | Loss: 0.1568\n",
            "CBOW | Epoch 4 | Batch 9500 | Loss: 0.1439\n",
            "CBOW | Epoch 4 | Batch 10000 | Loss: 0.1499\n",
            "CBOW | Epoch 4 | Batch 10500 | Loss: 0.1566\n",
            "CBOW | Epoch 4 | Batch 11000 | Loss: 0.1548\n",
            "CBOW | Epoch 4 | Batch 11500 | Loss: 0.1542\n",
            "CBOW | Epoch 4 | Batch 12000 | Loss: 0.1472\n",
            "CBOW | Epoch 4 | Batch 12500 | Loss: 0.1364\n",
            "CBOW | Epoch 4 | Batch 13000 | Loss: 0.1470\n",
            "CBOW | Epoch 4 | Batch 13500 | Loss: 0.1375\n",
            "CBOW | Epoch 4 | Batch 14000 | Loss: 0.1397\n",
            "CBOW | Epoch 4 | Batch 14500 | Loss: 0.1552\n",
            "CBOW | Epoch 4 | Batch 15000 | Loss: 0.1483\n",
            "CBOW | Epoch 4 | Batch 15500 | Loss: 0.1447\n",
            "CBOW | Epoch 4 | Batch 16000 | Loss: 0.1515\n",
            "CBOW | Epoch 4 | Batch 16500 | Loss: 0.1597\n",
            "CBOW | Epoch 4 | Batch 17000 | Loss: 0.1513\n",
            "CBOW | Epoch 4 | Batch 17500 | Loss: 0.1568\n",
            "CBOW | Epoch 4 | Batch 18000 | Loss: 0.1509\n",
            "CBOW | Epoch 4 | Batch 18500 | Loss: 0.1634\n",
            "CBOW | Epoch 4 | Batch 19000 | Loss: 0.1524\n",
            "CBOW | Epoch 4 | Batch 19500 | Loss: 0.1480\n",
            "CBOW | Epoch 4 | Batch 20000 | Loss: 0.1575\n",
            "CBOW | Epoch 4 | Batch 20500 | Loss: 0.0709\n",
            "CBOW | Epoch 4 | Batch 21000 | Loss: 0.1277\n",
            "CBOW | Epoch 4 | Batch 21500 | Loss: 0.1526\n",
            "CBOW | Epoch 4 | Batch 22000 | Loss: 0.1306\n",
            "CBOW | Epoch 4 | Batch 22500 | Loss: 0.1485\n",
            "CBOW | Epoch 4 | Batch 23000 | Loss: 0.1474\n",
            "CBOW | Epoch 4 | Batch 23500 | Loss: 0.1474\n",
            "CBOW | Epoch 4 | Batch 24000 | Loss: 0.1525\n",
            "CBOW | Epoch 4 | Batch 24500 | Loss: 0.1513\n",
            "CBOW | Epoch 4 | Batch 25000 | Loss: 0.1583\n",
            "CBOW | Epoch 4 | Batch 25500 | Loss: 0.1522\n",
            "CBOW | Epoch 4 | Batch 26000 | Loss: 0.1532\n",
            "CBOW | Epoch 4 | Batch 26500 | Loss: 0.1444\n",
            "CBOW | Epoch 4 | Batch 27000 | Loss: 0.1510\n",
            "CBOW | Epoch 4 | Batch 27500 | Loss: 0.1400\n",
            "CBOW | Epoch 4 | Batch 28000 | Loss: 0.1450\n",
            "CBOW | Epoch 4 | Batch 28500 | Loss: 0.1543\n",
            "CBOW | Epoch 4 | Batch 29000 | Loss: 0.1494\n",
            "CBOW | Epoch 4 | Batch 29500 | Loss: 0.1499\n",
            "CBOW | Epoch 4 | Batch 30000 | Loss: 0.1425\n",
            "CBOW | Epoch 4 | Batch 30500 | Loss: 0.1514\n",
            "CBOW | Epoch 4 | Batch 31000 | Loss: 0.1482\n",
            "CBOW | Epoch 4 | Batch 31500 | Loss: 0.1560\n",
            "CBOW | Epoch 4 | Batch 32000 | Loss: 0.1575\n",
            "CBOW | Epoch 4 | Batch 32500 | Loss: 0.1447\n",
            "CBOW | Epoch 4 | Batch 33000 | Loss: 0.1377\n",
            "CBOW | Epoch 4 | Batch 33500 | Loss: 0.1552\n",
            "CBOW | Epoch 4 | Batch 34000 | Loss: 0.1526\n",
            "CBOW | Epoch 4 | Batch 34500 | Loss: 0.1510\n",
            "CBOW | Epoch 4 | Batch 35000 | Loss: 0.1218\n",
            "CBOW | Epoch 4 | Batch 35500 | Loss: 0.1399\n",
            "CBOW | Epoch 4 | Batch 36000 | Loss: 0.1509\n",
            "CBOW | Epoch 4 | Batch 36500 | Loss: 0.1453\n",
            "CBOW | Epoch 4 | Batch 37000 | Loss: 0.1448\n",
            "CBOW | Epoch 4 | Batch 37500 | Loss: 0.1386\n",
            "CBOW | Epoch 4 | Batch 38000 | Loss: 0.1393\n",
            "CBOW | Epoch 4 | Batch 38500 | Loss: 0.1384\n",
            "CBOW | Epoch 4 | Batch 39000 | Loss: 0.1443\n",
            "CBOW | Epoch 4 | Batch 39500 | Loss: 0.1478\n",
            "CBOW | Epoch 4 | Batch 40000 | Loss: 0.1409\n",
            "CBOW | Epoch 4 | Batch 40500 | Loss: 0.1529\n",
            "CBOW | Epoch 4 | Batch 41000 | Loss: 0.1400\n",
            "CBOW | Epoch 4 | Batch 41500 | Loss: 0.1485\n",
            "CBOW | Epoch 4 | Batch 42000 | Loss: 0.1440\n",
            "CBOW | Epoch 4 | Batch 42500 | Loss: 0.1361\n",
            "CBOW | Epoch 4 | Batch 43000 | Loss: 0.1460\n",
            "CBOW | Epoch 4 | Batch 43500 | Loss: 0.1500\n",
            "CBOW | Epoch 4 | Batch 44000 | Loss: 0.1388\n",
            "CBOW | Epoch 4 | Batch 44500 | Loss: 0.1492\n",
            "CBOW | Epoch 4 | Batch 45000 | Loss: 0.1353\n",
            "CBOW | Epoch 4 | Batch 45500 | Loss: 0.1397\n",
            "CBOW | Epoch 4 | Batch 46000 | Loss: 0.1425\n",
            "CBOW | Epoch 4 | Batch 46500 | Loss: 0.1322\n",
            "CBOW | Epoch 4 | Batch 47000 | Loss: 0.1403\n",
            "CBOW | Epoch 4 | Batch 47500 | Loss: 0.1382\n",
            "CBOW | Epoch 4 | Batch 48000 | Loss: 0.1363\n",
            "CBOW | Epoch 4 | Batch 48500 | Loss: 0.1398\n",
            "CBOW | Epoch 4 | Batch 49000 | Loss: 0.1498\n",
            "CBOW | Epoch 4 | Batch 49500 | Loss: 0.1399\n",
            "CBOW | Epoch 4 | Batch 50000 | Loss: 0.1446\n",
            "CBOW | Epoch 4 | Batch 50500 | Loss: 0.1371\n",
            "CBOW | Epoch 4 | Batch 51000 | Loss: 0.1449\n",
            "CBOW | Epoch 4 | Batch 51500 | Loss: 0.1511\n",
            "CBOW | Epoch 4 | Batch 52000 | Loss: 0.1382\n",
            "CBOW | Epoch 4 | Batch 52500 | Loss: 0.1474\n",
            "CBOW | Epoch 4 | Batch 53000 | Loss: 0.1428\n",
            "CBOW Epoch 4. Средний Loss: 0.1469\n",
            "CBOW | Epoch 5 | Batch 500 | Loss: 0.1361\n",
            "CBOW | Epoch 5 | Batch 1000 | Loss: 0.1408\n",
            "CBOW | Epoch 5 | Batch 1500 | Loss: 0.1350\n",
            "CBOW | Epoch 5 | Batch 2000 | Loss: 0.1437\n",
            "CBOW | Epoch 5 | Batch 2500 | Loss: 0.1454\n",
            "CBOW | Epoch 5 | Batch 3000 | Loss: 0.1408\n",
            "CBOW | Epoch 5 | Batch 3500 | Loss: 0.1397\n",
            "CBOW | Epoch 5 | Batch 4000 | Loss: 0.1424\n",
            "CBOW | Epoch 5 | Batch 4500 | Loss: 0.1372\n",
            "CBOW | Epoch 5 | Batch 5000 | Loss: 0.1379\n",
            "CBOW | Epoch 5 | Batch 5500 | Loss: 0.1408\n",
            "CBOW | Epoch 5 | Batch 6000 | Loss: 0.0959\n",
            "CBOW | Epoch 5 | Batch 6500 | Loss: 0.1377\n",
            "CBOW | Epoch 5 | Batch 7000 | Loss: 0.1311\n",
            "CBOW | Epoch 5 | Batch 7500 | Loss: 0.1444\n",
            "CBOW | Epoch 5 | Batch 8000 | Loss: 0.1410\n",
            "CBOW | Epoch 5 | Batch 8500 | Loss: 0.1364\n",
            "CBOW | Epoch 5 | Batch 9000 | Loss: 0.1405\n",
            "CBOW | Epoch 5 | Batch 9500 | Loss: 0.1287\n",
            "CBOW | Epoch 5 | Batch 10000 | Loss: 0.1346\n",
            "CBOW | Epoch 5 | Batch 10500 | Loss: 0.1404\n",
            "CBOW | Epoch 5 | Batch 11000 | Loss: 0.1365\n",
            "CBOW | Epoch 5 | Batch 11500 | Loss: 0.1371\n",
            "CBOW | Epoch 5 | Batch 12000 | Loss: 0.1307\n",
            "CBOW | Epoch 5 | Batch 12500 | Loss: 0.1224\n",
            "CBOW | Epoch 5 | Batch 13000 | Loss: 0.1312\n",
            "CBOW | Epoch 5 | Batch 13500 | Loss: 0.1240\n",
            "CBOW | Epoch 5 | Batch 14000 | Loss: 0.1263\n",
            "CBOW | Epoch 5 | Batch 14500 | Loss: 0.1385\n",
            "CBOW | Epoch 5 | Batch 15000 | Loss: 0.1334\n",
            "CBOW | Epoch 5 | Batch 15500 | Loss: 0.1316\n",
            "CBOW | Epoch 5 | Batch 16000 | Loss: 0.1356\n",
            "CBOW | Epoch 5 | Batch 16500 | Loss: 0.1428\n",
            "CBOW | Epoch 5 | Batch 17000 | Loss: 0.1343\n",
            "CBOW | Epoch 5 | Batch 17500 | Loss: 0.1403\n",
            "CBOW | Epoch 5 | Batch 18000 | Loss: 0.1342\n",
            "CBOW | Epoch 5 | Batch 18500 | Loss: 0.1466\n",
            "CBOW | Epoch 5 | Batch 19000 | Loss: 0.1385\n",
            "CBOW | Epoch 5 | Batch 19500 | Loss: 0.1324\n",
            "CBOW | Epoch 5 | Batch 20000 | Loss: 0.1419\n",
            "CBOW | Epoch 5 | Batch 20500 | Loss: 0.0640\n",
            "CBOW | Epoch 5 | Batch 21000 | Loss: 0.1140\n",
            "CBOW | Epoch 5 | Batch 21500 | Loss: 0.1366\n",
            "CBOW | Epoch 5 | Batch 22000 | Loss: 0.1179\n",
            "CBOW | Epoch 5 | Batch 22500 | Loss: 0.1361\n",
            "CBOW | Epoch 5 | Batch 23000 | Loss: 0.1328\n",
            "CBOW | Epoch 5 | Batch 23500 | Loss: 0.1334\n",
            "CBOW | Epoch 5 | Batch 24000 | Loss: 0.1385\n",
            "CBOW | Epoch 5 | Batch 24500 | Loss: 0.1364\n",
            "CBOW | Epoch 5 | Batch 25000 | Loss: 0.1435\n",
            "CBOW | Epoch 5 | Batch 25500 | Loss: 0.1380\n",
            "CBOW | Epoch 5 | Batch 26000 | Loss: 0.1371\n",
            "CBOW | Epoch 5 | Batch 26500 | Loss: 0.1307\n",
            "CBOW | Epoch 5 | Batch 27000 | Loss: 0.1360\n",
            "CBOW | Epoch 5 | Batch 27500 | Loss: 0.1266\n",
            "CBOW | Epoch 5 | Batch 28000 | Loss: 0.1317\n",
            "CBOW | Epoch 5 | Batch 28500 | Loss: 0.1391\n",
            "CBOW | Epoch 5 | Batch 29000 | Loss: 0.1370\n",
            "CBOW | Epoch 5 | Batch 29500 | Loss: 0.1353\n",
            "CBOW | Epoch 5 | Batch 30000 | Loss: 0.1302\n",
            "CBOW | Epoch 5 | Batch 30500 | Loss: 0.1382\n",
            "CBOW | Epoch 5 | Batch 31000 | Loss: 0.1350\n",
            "CBOW | Epoch 5 | Batch 31500 | Loss: 0.1420\n",
            "CBOW | Epoch 5 | Batch 32000 | Loss: 0.1423\n",
            "CBOW | Epoch 5 | Batch 32500 | Loss: 0.1323\n",
            "CBOW | Epoch 5 | Batch 33000 | Loss: 0.1244\n",
            "CBOW | Epoch 5 | Batch 33500 | Loss: 0.1407\n",
            "CBOW | Epoch 5 | Batch 34000 | Loss: 0.1374\n",
            "CBOW | Epoch 5 | Batch 34500 | Loss: 0.1383\n",
            "CBOW | Epoch 5 | Batch 35000 | Loss: 0.1115\n",
            "CBOW | Epoch 5 | Batch 35500 | Loss: 0.1277\n",
            "CBOW | Epoch 5 | Batch 36000 | Loss: 0.1378\n",
            "CBOW | Epoch 5 | Batch 36500 | Loss: 0.1308\n",
            "CBOW | Epoch 5 | Batch 37000 | Loss: 0.1324\n",
            "CBOW | Epoch 5 | Batch 37500 | Loss: 0.1263\n",
            "CBOW | Epoch 5 | Batch 38000 | Loss: 0.1264\n",
            "CBOW | Epoch 5 | Batch 38500 | Loss: 0.1272\n",
            "CBOW | Epoch 5 | Batch 39000 | Loss: 0.1320\n",
            "CBOW | Epoch 5 | Batch 39500 | Loss: 0.1359\n",
            "CBOW | Epoch 5 | Batch 40000 | Loss: 0.1286\n",
            "CBOW | Epoch 5 | Batch 40500 | Loss: 0.1383\n",
            "CBOW | Epoch 5 | Batch 41000 | Loss: 0.1275\n",
            "CBOW | Epoch 5 | Batch 41500 | Loss: 0.1365\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(5):\n",
        "    losses = []\n",
        "\n",
        "    for x, y, labels in gen_batches_cbow_neg(indexed_wiki, window_size=WINDOW_SIZE, batch_size=512):\n",
        "        x, y, labels = x.to(DEVICE), y.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = cbow_model(x, y)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if len(losses) % 500 == 0:\n",
        "            print(f\"CBOW | Epoch {epoch+1} | Batch {len(losses)} | Loss: {np.mean(losses[-500:]):.4f}\")\n",
        "\n",
        "    print(f\"CBOW Epoch {epoch+1}. Средний Loss: {np.mean(losses):.4f}\")\n",
        "\n",
        "cbow_weights = cbow_model.embeddings.weight.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eRb0eZ2KwB_i",
      "metadata": {
        "id": "eRb0eZ2KwB_i"
      },
      "outputs": [],
      "source": [
        "torch.save(cbow_model.state_dict(), '/content/drive/My Drive/cbow_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_YODyBdBn-3R",
      "metadata": {
        "id": "_YODyBdBn-3R"
      },
      "outputs": [],
      "source": [
        "def get_closest_words(word, weights, word2id, id2word, top_n=5):\n",
        "    if word not in word2id:\n",
        "        return \"Слова нет в словаре\"\n",
        "\n",
        "    word_id = word2id[word]\n",
        "    vec = weights[word_id]\n",
        "\n",
        "    norms = np.linalg.norm(weights, axis=1)\n",
        "    cos_sim = np.dot(weights, vec) / (norms * np.linalg.norm(vec))\n",
        "\n",
        "    closest_ids = np.argsort(cos_sim)[-(top_n+1):-1][::-1]\n",
        "\n",
        "    return [id2word[idx] for idx in closest_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ybbk96FFe9uK",
      "metadata": {
        "id": "ybbk96FFe9uK"
      },
      "source": [
        "skipgram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ASbdFthce_q-",
      "metadata": {
        "id": "ASbdFthce_q-"
      },
      "outputs": [],
      "source": [
        "def gen_batches_sg_neg(indexed_wiki, window_size=3, batch_size=512, n_neg=5):\n",
        "    X_target = []\n",
        "    y_context = []\n",
        "    labels = []\n",
        "\n",
        "    for sent in indexed_wiki:\n",
        "        for i in range(len(sent)):\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(sent), i + window_size + 1)\n",
        "            context = sent[start:i] + sent[i+1:end]\n",
        "\n",
        "            for context_word in context:\n",
        "                X_target.append(sent[i])\n",
        "                y_context.append(context_word)\n",
        "                labels.append(1)\n",
        "\n",
        "                for _ in range(n_neg):\n",
        "                    X_target.append(sent[i])\n",
        "                    y_context.append(np.random.randint(0, len(word2id)))\n",
        "                    labels.append(0)\n",
        "\n",
        "                if len(X_target) >= batch_size:\n",
        "                    yield (torch.LongTensor(X_target),\n",
        "                           torch.LongTensor(y_context),\n",
        "                           torch.FloatTensor(labels))\n",
        "                    X_target, y_context, labels = [], [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4COLmAKwfYX_",
      "metadata": {
        "id": "4COLmAKwfYX_"
      },
      "outputs": [],
      "source": [
        "class SkipGramModelNeg(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=256):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.out_embeddings = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "    def forward(self, target, context):\n",
        "        target_emb = self.embeddings(target)\n",
        "        context_emb = self.out_embeddings(context)\n",
        "\n",
        "        scores = (target_emb * context_emb).sum(dim=1)\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KuMmQ4sdlCQU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuMmQ4sdlCQU",
        "outputId": "0230832a-cbb7-492a-df92-12e0b021f2f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skip-gram | Epoch 1 | Batch 500 | Loss: 6.2129\n",
            "Skip-gram | Epoch 1 | Batch 1000 | Loss: 5.7592\n",
            "Skip-gram | Epoch 1 | Batch 1500 | Loss: 5.4359\n",
            "Skip-gram | Epoch 1 | Batch 2000 | Loss: 5.1119\n",
            "Skip-gram | Epoch 1 | Batch 2500 | Loss: 4.8331\n",
            "Skip-gram | Epoch 1 | Batch 3000 | Loss: 4.7254\n",
            "Skip-gram | Epoch 1 | Batch 3500 | Loss: 4.7766\n",
            "Skip-gram | Epoch 1 | Batch 4000 | Loss: 4.5209\n",
            "Skip-gram | Epoch 1 | Batch 4500 | Loss: 4.2831\n",
            "Skip-gram | Epoch 1 | Batch 5000 | Loss: 4.1786\n",
            "Skip-gram | Epoch 1 | Batch 5500 | Loss: 4.1382\n",
            "Skip-gram | Epoch 1 | Batch 6000 | Loss: 3.9299\n",
            "Skip-gram | Epoch 1 | Batch 6500 | Loss: 3.8407\n",
            "Skip-gram | Epoch 1 | Batch 7000 | Loss: 3.5969\n",
            "Skip-gram | Epoch 1 | Batch 7500 | Loss: 3.5200\n",
            "Skip-gram | Epoch 1 | Batch 8000 | Loss: 3.4420\n",
            "Skip-gram | Epoch 1 | Batch 8500 | Loss: 3.3999\n",
            "Skip-gram | Epoch 1 | Batch 9000 | Loss: 3.2607\n",
            "Skip-gram | Epoch 1 | Batch 9500 | Loss: 3.2308\n",
            "Skip-gram | Epoch 1 | Batch 10000 | Loss: 3.1135\n",
            "Skip-gram | Epoch 1 | Batch 10500 | Loss: 3.1235\n",
            "Skip-gram | Epoch 1 | Batch 11000 | Loss: 3.0492\n",
            "Skip-gram | Epoch 1 | Batch 11500 | Loss: 3.0387\n",
            "Skip-gram | Epoch 1 | Batch 12000 | Loss: 2.8651\n",
            "Skip-gram | Epoch 1 | Batch 12500 | Loss: 2.6566\n",
            "Skip-gram | Epoch 1 | Batch 13000 | Loss: 2.7888\n",
            "Skip-gram | Epoch 1 | Batch 13500 | Loss: 2.5743\n",
            "Skip-gram | Epoch 1 | Batch 14000 | Loss: 2.5481\n",
            "Skip-gram | Epoch 1 | Batch 14500 | Loss: 2.4387\n",
            "Skip-gram | Epoch 1 | Batch 15000 | Loss: 2.4758\n",
            "Skip-gram | Epoch 1 | Batch 15500 | Loss: 2.1311\n",
            "Skip-gram | Epoch 1 | Batch 16000 | Loss: 2.1849\n",
            "Skip-gram | Epoch 1 | Batch 16500 | Loss: 2.3061\n",
            "Skip-gram | Epoch 1 | Batch 17000 | Loss: 2.1068\n",
            "Skip-gram | Epoch 1 | Batch 17500 | Loss: 1.8691\n",
            "Skip-gram | Epoch 1 | Batch 18000 | Loss: 2.0216\n",
            "Skip-gram | Epoch 1 | Batch 18500 | Loss: 2.1357\n",
            "Skip-gram | Epoch 1 | Batch 19000 | Loss: 1.9752\n",
            "Skip-gram | Epoch 1 | Batch 19500 | Loss: 1.8973\n",
            "Skip-gram | Epoch 1 | Batch 20000 | Loss: 1.7744\n",
            "Skip-gram | Epoch 1 | Batch 20500 | Loss: 1.7153\n",
            "Skip-gram | Epoch 1 | Batch 21000 | Loss: 1.8199\n",
            "Skip-gram | Epoch 1 | Batch 21500 | Loss: 1.7088\n",
            "Skip-gram | Epoch 1 | Batch 22000 | Loss: 1.8195\n",
            "Skip-gram | Epoch 1 | Batch 22500 | Loss: 1.6720\n",
            "Skip-gram | Epoch 1 | Batch 23000 | Loss: 1.6790\n",
            "Skip-gram | Epoch 1 | Batch 23500 | Loss: 1.6677\n",
            "Skip-gram | Epoch 1 | Batch 24000 | Loss: 1.6694\n",
            "Skip-gram | Epoch 1 | Batch 24500 | Loss: 1.5628\n",
            "Skip-gram | Epoch 1 | Batch 25000 | Loss: 1.5331\n",
            "Skip-gram | Epoch 1 | Batch 25500 | Loss: 1.5866\n",
            "Skip-gram | Epoch 1 | Batch 26000 | Loss: 1.5397\n",
            "Skip-gram | Epoch 1 | Batch 26500 | Loss: 1.3302\n",
            "Skip-gram | Epoch 1 | Batch 27000 | Loss: 1.3064\n",
            "Skip-gram | Epoch 1 | Batch 27500 | Loss: 1.4455\n",
            "Skip-gram | Epoch 1 | Batch 28000 | Loss: 1.4404\n",
            "Skip-gram | Epoch 1 | Batch 28500 | Loss: 1.5620\n",
            "Skip-gram | Epoch 1 | Batch 29000 | Loss: 1.3896\n",
            "Skip-gram | Epoch 1 | Batch 29500 | Loss: 1.4502\n",
            "Skip-gram | Epoch 1 | Batch 30000 | Loss: 1.2817\n",
            "Skip-gram | Epoch 1 | Batch 30500 | Loss: 1.3808\n",
            "Skip-gram | Epoch 1 | Batch 31000 | Loss: 1.2446\n",
            "Skip-gram | Epoch 1 | Batch 31500 | Loss: 1.6259\n",
            "Skip-gram | Epoch 1 | Batch 32000 | Loss: 1.3569\n",
            "Skip-gram | Epoch 1 | Batch 32500 | Loss: 1.1854\n",
            "Skip-gram | Epoch 1 | Batch 33000 | Loss: 1.3303\n",
            "Skip-gram | Epoch 1 | Batch 33500 | Loss: 1.2208\n",
            "Skip-gram | Epoch 1 | Batch 34000 | Loss: 1.2140\n",
            "Skip-gram | Epoch 1 | Batch 34500 | Loss: 1.0949\n",
            "Skip-gram | Epoch 1 | Batch 35000 | Loss: 0.7549\n",
            "Skip-gram | Epoch 1 | Batch 35500 | Loss: 0.5922\n",
            "Skip-gram | Epoch 1 | Batch 36000 | Loss: 0.6877\n",
            "Skip-gram | Epoch 1 | Batch 36500 | Loss: 1.1962\n",
            "Skip-gram | Epoch 1 | Batch 37000 | Loss: 1.1008\n",
            "Skip-gram | Epoch 1 | Batch 37500 | Loss: 1.2648\n",
            "Skip-gram | Epoch 1 | Batch 38000 | Loss: 1.0154\n",
            "Skip-gram | Epoch 1 | Batch 38500 | Loss: 1.1378\n",
            "Skip-gram | Epoch 1 | Batch 39000 | Loss: 1.1238\n",
            "Skip-gram | Epoch 1 | Batch 39500 | Loss: 1.1406\n",
            "Skip-gram | Epoch 1 | Batch 40000 | Loss: 1.0291\n",
            "Skip-gram | Epoch 1 | Batch 40500 | Loss: 1.0845\n",
            "Skip-gram | Epoch 1 | Batch 41000 | Loss: 1.0841\n",
            "Skip-gram | Epoch 1 | Batch 41500 | Loss: 1.0452\n",
            "Skip-gram | Epoch 1 | Batch 42000 | Loss: 1.1283\n",
            "Skip-gram | Epoch 1 | Batch 42500 | Loss: 0.9303\n",
            "Skip-gram | Epoch 1 | Batch 43000 | Loss: 1.0659\n",
            "Skip-gram | Epoch 1 | Batch 43500 | Loss: 1.0971\n",
            "Skip-gram | Epoch 1 | Batch 44000 | Loss: 1.0562\n",
            "Skip-gram | Epoch 1 | Batch 44500 | Loss: 1.0361\n",
            "Skip-gram | Epoch 1 | Batch 45000 | Loss: 1.1007\n",
            "Skip-gram | Epoch 1 | Batch 45500 | Loss: 1.1264\n",
            "Skip-gram | Epoch 1 | Batch 46000 | Loss: 0.9738\n",
            "Skip-gram | Epoch 1 | Batch 46500 | Loss: 0.9773\n",
            "Skip-gram | Epoch 1 | Batch 47000 | Loss: 0.9080\n",
            "Skip-gram | Epoch 1 | Batch 47500 | Loss: 0.9583\n",
            "Skip-gram | Epoch 1 | Batch 48000 | Loss: 0.9464\n",
            "Skip-gram | Epoch 1 | Batch 48500 | Loss: 0.9566\n",
            "Skip-gram | Epoch 1 | Batch 49000 | Loss: 0.9358\n",
            "Skip-gram | Epoch 1 | Batch 49500 | Loss: 0.9392\n",
            "Skip-gram | Epoch 1 | Batch 50000 | Loss: 0.9951\n",
            "Skip-gram | Epoch 1 | Batch 50500 | Loss: 0.9057\n",
            "Skip-gram | Epoch 1 | Batch 51000 | Loss: 0.8845\n",
            "Skip-gram | Epoch 1 | Batch 51500 | Loss: 0.9289\n",
            "Skip-gram | Epoch 1 | Batch 52000 | Loss: 0.8859\n",
            "Skip-gram | Epoch 1 | Batch 52500 | Loss: 0.8735\n",
            "Skip-gram | Epoch 1 | Batch 53000 | Loss: 0.9435\n",
            "Skip-gram | Epoch 1 | Batch 53500 | Loss: 0.8788\n",
            "Skip-gram | Epoch 1 | Batch 54000 | Loss: 0.8933\n",
            "Skip-gram | Epoch 1 | Batch 54500 | Loss: 0.8455\n",
            "Skip-gram | Epoch 1 | Batch 55000 | Loss: 0.8532\n",
            "Skip-gram | Epoch 1 | Batch 55500 | Loss: 0.9071\n",
            "Skip-gram | Epoch 1 | Batch 56000 | Loss: 0.6955\n",
            "Skip-gram | Epoch 1 | Batch 56500 | Loss: 0.7234\n",
            "Skip-gram | Epoch 1 | Batch 57000 | Loss: 0.9151\n",
            "Skip-gram | Epoch 1 | Batch 57500 | Loss: 0.8772\n",
            "Skip-gram | Epoch 1 | Batch 58000 | Loss: 0.8111\n",
            "Skip-gram | Epoch 1 | Batch 58500 | Loss: 0.9044\n",
            "Skip-gram | Epoch 1 | Batch 59000 | Loss: 0.7955\n",
            "Skip-gram | Epoch 1 | Batch 59500 | Loss: 0.8114\n",
            "Skip-gram | Epoch 1 | Batch 60000 | Loss: 0.7176\n",
            "Skip-gram | Epoch 1 | Batch 60500 | Loss: 0.7942\n",
            "Skip-gram | Epoch 1 | Batch 61000 | Loss: 0.7499\n",
            "Skip-gram | Epoch 1 | Batch 61500 | Loss: 0.7536\n",
            "Skip-gram | Epoch 1 | Batch 62000 | Loss: 0.8339\n",
            "Skip-gram | Epoch 1 | Batch 62500 | Loss: 0.8273\n",
            "Skip-gram | Epoch 1 | Batch 63000 | Loss: 0.7829\n",
            "Skip-gram | Epoch 1 | Batch 63500 | Loss: 0.7617\n",
            "Skip-gram | Epoch 1 | Batch 64000 | Loss: 0.7968\n",
            "Skip-gram | Epoch 1 | Batch 64500 | Loss: 0.8135\n",
            "Skip-gram | Epoch 1 | Batch 65000 | Loss: 0.8272\n",
            "Skip-gram | Epoch 1 | Batch 65500 | Loss: 0.7818\n",
            "Skip-gram | Epoch 1 | Batch 66000 | Loss: 0.7502\n",
            "Skip-gram | Epoch 1 | Batch 66500 | Loss: 0.7334\n",
            "Skip-gram | Epoch 1 | Batch 67000 | Loss: 0.7407\n",
            "Skip-gram | Epoch 1 | Batch 67500 | Loss: 0.7776\n",
            "Skip-gram | Epoch 1 | Batch 68000 | Loss: 0.7079\n",
            "Skip-gram | Epoch 1 | Batch 68500 | Loss: 0.7574\n",
            "Skip-gram | Epoch 1 | Batch 69000 | Loss: 0.8126\n",
            "Skip-gram | Epoch 1 | Batch 69500 | Loss: 0.7351\n",
            "Skip-gram | Epoch 1 | Batch 70000 | Loss: 0.6806\n",
            "Skip-gram | Epoch 1 | Batch 70500 | Loss: 0.7203\n",
            "Skip-gram | Epoch 1 | Batch 71000 | Loss: 0.7381\n",
            "Skip-gram | Epoch 1 | Batch 71500 | Loss: 0.7091\n",
            "Skip-gram | Epoch 1 | Batch 72000 | Loss: 0.7220\n",
            "Skip-gram | Epoch 1 | Batch 72500 | Loss: 0.6584\n",
            "Skip-gram | Epoch 1 | Batch 73000 | Loss: 0.8127\n",
            "Skip-gram | Epoch 1 | Batch 73500 | Loss: 0.6822\n",
            "Skip-gram | Epoch 1 | Batch 74000 | Loss: 0.6267\n",
            "Skip-gram | Epoch 1 | Batch 74500 | Loss: 0.5104\n",
            "Skip-gram | Epoch 1 | Batch 75000 | Loss: 0.5544\n",
            "Skip-gram | Epoch 1 | Batch 75500 | Loss: 0.5791\n",
            "Skip-gram | Epoch 1 | Batch 76000 | Loss: 0.6983\n",
            "Skip-gram | Epoch 1 | Batch 76500 | Loss: 0.6612\n",
            "Skip-gram | Epoch 1 | Batch 77000 | Loss: 0.6185\n",
            "Skip-gram | Epoch 1 | Batch 77500 | Loss: 0.6999\n",
            "Skip-gram | Epoch 1 | Batch 78000 | Loss: 0.6920\n",
            "Skip-gram | Epoch 1 | Batch 78500 | Loss: 0.6327\n",
            "Skip-gram | Epoch 1 | Batch 79000 | Loss: 0.6303\n",
            "Skip-gram | Epoch 1 | Batch 79500 | Loss: 0.7128\n",
            "Skip-gram | Epoch 1 | Batch 80000 | Loss: 0.5964\n",
            "Skip-gram | Epoch 1 | Batch 80500 | Loss: 0.6499\n",
            "Skip-gram | Epoch 1 | Batch 81000 | Loss: 0.5962\n",
            "Skip-gram | Epoch 1 | Batch 81500 | Loss: 0.5904\n",
            "Skip-gram | Epoch 1 | Batch 82000 | Loss: 0.5801\n",
            "Skip-gram | Epoch 1 | Batch 82500 | Loss: 0.5418\n",
            "Skip-gram | Epoch 1 | Batch 83000 | Loss: 0.5860\n",
            "Skip-gram | Epoch 1 | Batch 83500 | Loss: 0.6657\n",
            "Skip-gram | Epoch 1 | Batch 84000 | Loss: 0.5822\n",
            "Skip-gram | Epoch 1 | Batch 84500 | Loss: 0.6158\n",
            "Skip-gram | Epoch 1 | Batch 85000 | Loss: 0.5355\n",
            "Skip-gram | Epoch 1 | Batch 85500 | Loss: 0.5542\n",
            "Skip-gram | Epoch 1 | Batch 86000 | Loss: 0.6864\n",
            "Skip-gram | Epoch 1 | Batch 86500 | Loss: 0.6326\n",
            "Skip-gram | Epoch 1 | Batch 87000 | Loss: 0.5978\n",
            "Skip-gram | Epoch 1 | Batch 87500 | Loss: 0.5532\n",
            "Skip-gram | Epoch 1 | Batch 88000 | Loss: 0.5507\n",
            "Skip-gram | Epoch 1 | Batch 88500 | Loss: 0.5816\n",
            "Skip-gram | Epoch 1 | Batch 89000 | Loss: 0.5623\n",
            "Skip-gram | Epoch 1 | Batch 89500 | Loss: 0.6107\n",
            "Skip-gram | Epoch 1 | Batch 90000 | Loss: 0.5920\n",
            "Skip-gram | Epoch 1 | Batch 90500 | Loss: 0.5929\n",
            "Skip-gram | Epoch 1 | Batch 91000 | Loss: 0.6698\n",
            "Skip-gram | Epoch 1 | Batch 91500 | Loss: 0.5467\n",
            "Skip-gram | Epoch 1 | Batch 92000 | Loss: 0.5742\n",
            "Skip-gram | Epoch 1 | Batch 92500 | Loss: 0.6019\n",
            "Skip-gram | Epoch 1 | Batch 93000 | Loss: 0.6425\n",
            "Skip-gram | Epoch 1 | Batch 93500 | Loss: 0.5435\n",
            "Skip-gram | Epoch 1 | Batch 94000 | Loss: 0.6264\n",
            "Skip-gram | Epoch 1 | Batch 94500 | Loss: 0.4596\n",
            "Skip-gram | Epoch 1 | Batch 95000 | Loss: 0.5588\n",
            "Skip-gram | Epoch 1 | Batch 95500 | Loss: 0.4997\n",
            "Skip-gram | Epoch 1 | Batch 96000 | Loss: 0.6750\n",
            "Skip-gram | Epoch 1 | Batch 96500 | Loss: 0.5782\n",
            "Skip-gram | Epoch 1 | Batch 97000 | Loss: 0.5840\n",
            "Skip-gram | Epoch 1 | Batch 97500 | Loss: 0.6245\n",
            "Skip-gram | Epoch 1 | Batch 98000 | Loss: 0.5638\n",
            "Skip-gram | Epoch 1 | Batch 98500 | Loss: 0.6285\n",
            "Skip-gram | Epoch 1 | Batch 99000 | Loss: 0.5800\n",
            "Skip-gram | Epoch 1 | Batch 99500 | Loss: 0.5776\n",
            "Skip-gram | Epoch 1 | Batch 100000 | Loss: 0.5488\n",
            "Skip-gram | Epoch 1 | Batch 100500 | Loss: 0.5607\n",
            "Skip-gram | Epoch 1 | Batch 101000 | Loss: 0.5644\n",
            "Skip-gram | Epoch 1 | Batch 101500 | Loss: 0.5540\n",
            "Skip-gram | Epoch 1 | Batch 102000 | Loss: 0.6301\n",
            "Skip-gram | Epoch 1 | Batch 102500 | Loss: 0.6362\n",
            "Skip-gram | Epoch 1 | Batch 103000 | Loss: 0.5704\n",
            "Skip-gram | Epoch 1 | Batch 103500 | Loss: 0.5098\n",
            "Skip-gram | Epoch 1 | Batch 104000 | Loss: 0.5433\n",
            "Skip-gram | Epoch 1 | Batch 104500 | Loss: 0.5372\n",
            "Skip-gram | Epoch 1 | Batch 105000 | Loss: 0.5302\n",
            "Skip-gram | Epoch 1 | Batch 105500 | Loss: 0.5337\n",
            "Skip-gram | Epoch 1 | Batch 106000 | Loss: 0.5153\n",
            "Skip-gram | Epoch 1 | Batch 106500 | Loss: 0.6435\n",
            "Skip-gram | Epoch 1 | Batch 107000 | Loss: 0.5489\n",
            "Skip-gram | Epoch 1 | Batch 107500 | Loss: 0.5244\n",
            "Skip-gram | Epoch 1 | Batch 108000 | Loss: 0.5412\n",
            "Skip-gram | Epoch 1 | Batch 108500 | Loss: 0.5242\n",
            "Skip-gram | Epoch 1 | Batch 109000 | Loss: 0.5295\n",
            "Skip-gram | Epoch 1 | Batch 109500 | Loss: 0.5511\n",
            "Skip-gram | Epoch 1 | Batch 110000 | Loss: 0.5337\n",
            "Skip-gram | Epoch 1 | Batch 110500 | Loss: 0.5397\n",
            "Skip-gram | Epoch 1 | Batch 111000 | Loss: 0.5928\n",
            "Skip-gram | Epoch 1 | Batch 111500 | Loss: 0.6067\n",
            "Skip-gram | Epoch 1 | Batch 112000 | Loss: 0.5440\n",
            "Skip-gram | Epoch 1 | Batch 112500 | Loss: 0.5916\n",
            "Skip-gram | Epoch 1 | Batch 113000 | Loss: 0.5137\n",
            "Skip-gram | Epoch 1 | Batch 113500 | Loss: 0.5086\n",
            "Skip-gram | Epoch 1 | Batch 114000 | Loss: 0.5320\n",
            "Skip-gram | Epoch 1 | Batch 114500 | Loss: 0.5256\n",
            "Skip-gram | Epoch 1 | Batch 115000 | Loss: 0.5179\n",
            "Skip-gram | Epoch 1 | Batch 115500 | Loss: 0.4796\n",
            "Skip-gram | Epoch 1 | Batch 116000 | Loss: 0.5262\n",
            "Skip-gram | Epoch 1 | Batch 116500 | Loss: 0.4759\n",
            "Skip-gram | Epoch 1 | Batch 117000 | Loss: 0.4594\n",
            "Skip-gram | Epoch 1 | Batch 117500 | Loss: 0.4619\n",
            "Skip-gram | Epoch 1 | Batch 118000 | Loss: 0.5348\n",
            "Skip-gram | Epoch 1 | Batch 118500 | Loss: 0.4866\n",
            "Skip-gram | Epoch 1 | Batch 119000 | Loss: 0.4994\n",
            "Skip-gram | Epoch 1 | Batch 119500 | Loss: 0.4918\n",
            "Skip-gram | Epoch 1 | Batch 120000 | Loss: 0.5119\n",
            "Skip-gram | Epoch 1 | Batch 120500 | Loss: 0.4980\n",
            "Skip-gram | Epoch 1 | Batch 121000 | Loss: 0.5132\n",
            "Skip-gram | Epoch 1 | Batch 121500 | Loss: 0.5077\n",
            "Skip-gram | Epoch 1 | Batch 122000 | Loss: 0.4588\n",
            "Skip-gram | Epoch 1 | Batch 122500 | Loss: 0.3982\n",
            "Skip-gram | Epoch 1 | Batch 123000 | Loss: 0.2259\n",
            "Skip-gram | Epoch 1 | Batch 123500 | Loss: 0.2340\n",
            "Skip-gram | Epoch 1 | Batch 124000 | Loss: 0.2554\n",
            "Skip-gram | Epoch 1 | Batch 124500 | Loss: 0.2591\n",
            "Skip-gram | Epoch 1 | Batch 125000 | Loss: 0.2102\n",
            "Skip-gram | Epoch 1 | Batch 125500 | Loss: 0.1808\n",
            "Skip-gram | Epoch 1 | Batch 126000 | Loss: 0.3092\n",
            "Skip-gram | Epoch 1 | Batch 126500 | Loss: 0.3104\n",
            "Skip-gram | Epoch 1 | Batch 127000 | Loss: 0.4894\n",
            "Skip-gram | Epoch 1 | Batch 127500 | Loss: 0.4838\n",
            "Skip-gram | Epoch 1 | Batch 128000 | Loss: 0.4542\n",
            "Skip-gram | Epoch 1 | Batch 128500 | Loss: 0.5423\n",
            "Skip-gram | Epoch 1 | Batch 129000 | Loss: 0.5000\n",
            "Skip-gram | Epoch 1 | Batch 129500 | Loss: 0.4711\n",
            "Skip-gram | Epoch 1 | Batch 130000 | Loss: 0.4591\n",
            "Skip-gram | Epoch 1 | Batch 130500 | Loss: 0.4411\n",
            "Skip-gram | Epoch 1 | Batch 131000 | Loss: 0.5358\n",
            "Skip-gram | Epoch 1 | Batch 131500 | Loss: 0.4547\n",
            "Skip-gram | Epoch 1 | Batch 132000 | Loss: 0.4906\n",
            "Skip-gram | Epoch 1 | Batch 132500 | Loss: 0.4994\n",
            "Skip-gram | Epoch 1 | Batch 133000 | Loss: 0.3845\n",
            "Skip-gram | Epoch 1 | Batch 133500 | Loss: 0.4379\n",
            "Skip-gram | Epoch 1 | Batch 134000 | Loss: 0.4070\n",
            "Skip-gram | Epoch 1 | Batch 134500 | Loss: 0.3856\n",
            "Skip-gram | Epoch 1 | Batch 135000 | Loss: 0.4258\n",
            "Skip-gram | Epoch 1 | Batch 135500 | Loss: 0.4482\n",
            "Skip-gram | Epoch 1 | Batch 136000 | Loss: 0.4452\n",
            "Skip-gram | Epoch 1 | Batch 136500 | Loss: 0.4920\n",
            "Skip-gram | Epoch 1 | Batch 137000 | Loss: 0.4602\n",
            "Skip-gram | Epoch 1 | Batch 137500 | Loss: 0.4948\n",
            "Skip-gram | Epoch 1 | Batch 138000 | Loss: 0.4978\n",
            "Skip-gram | Epoch 1 | Batch 138500 | Loss: 0.4408\n",
            "Skip-gram | Epoch 1 | Batch 139000 | Loss: 0.4356\n",
            "Skip-gram | Epoch 1 | Batch 139500 | Loss: 0.4566\n",
            "Skip-gram | Epoch 1 | Batch 140000 | Loss: 0.4245\n",
            "Skip-gram | Epoch 1 | Batch 140500 | Loss: 0.4644\n",
            "Skip-gram | Epoch 1 | Batch 141000 | Loss: 0.4539\n",
            "Skip-gram | Epoch 1 | Batch 141500 | Loss: 0.4358\n",
            "Skip-gram | Epoch 1 | Batch 142000 | Loss: 0.4689\n",
            "Skip-gram | Epoch 1 | Batch 142500 | Loss: 0.4422\n",
            "Skip-gram | Epoch 1 | Batch 143000 | Loss: 0.4280\n",
            "Skip-gram | Epoch 1 | Batch 143500 | Loss: 0.4890\n",
            "Skip-gram | Epoch 1 | Batch 144000 | Loss: 0.4786\n",
            "Skip-gram | Epoch 1 | Batch 144500 | Loss: 0.4202\n",
            "Skip-gram | Epoch 1 | Batch 145000 | Loss: 0.4614\n",
            "Skip-gram | Epoch 1 | Batch 145500 | Loss: 0.4429\n",
            "Skip-gram | Epoch 1 | Batch 146000 | Loss: 0.4412\n",
            "Skip-gram | Epoch 1 | Batch 146500 | Loss: 0.4382\n",
            "Skip-gram | Epoch 1 | Batch 147000 | Loss: 0.4630\n",
            "Skip-gram | Epoch 1 | Batch 147500 | Loss: 0.4572\n",
            "Skip-gram | Epoch 1 | Batch 148000 | Loss: 0.4107\n",
            "Skip-gram | Epoch 1 | Batch 148500 | Loss: 0.4674\n",
            "Skip-gram | Epoch 1 | Batch 149000 | Loss: 0.5038\n",
            "Skip-gram | Epoch 1 | Batch 149500 | Loss: 0.4143\n",
            "Skip-gram | Epoch 1 | Batch 150000 | Loss: 0.4179\n",
            "Skip-gram | Epoch 1 | Batch 150500 | Loss: 0.4624\n",
            "Skip-gram | Epoch 1 | Batch 151000 | Loss: 0.3985\n",
            "Skip-gram | Epoch 1 | Batch 151500 | Loss: 0.5024\n",
            "Skip-gram | Epoch 1 | Batch 152000 | Loss: 0.4856\n",
            "Skip-gram | Epoch 1 | Batch 152500 | Loss: 0.4272\n",
            "Skip-gram | Epoch 1 | Batch 153000 | Loss: 0.4589\n",
            "Skip-gram | Epoch 1 | Batch 153500 | Loss: 0.4287\n",
            "Skip-gram | Epoch 1 | Batch 154000 | Loss: 0.4602\n",
            "Skip-gram | Epoch 1 | Batch 154500 | Loss: 0.4388\n",
            "Skip-gram | Epoch 1 | Batch 155000 | Loss: 0.4045\n",
            "Skip-gram | Epoch 1 | Batch 155500 | Loss: 0.4098\n",
            "Skip-gram | Epoch 1 | Batch 156000 | Loss: 0.4041\n",
            "Skip-gram | Epoch 1 | Batch 156500 | Loss: 0.4392\n",
            "Skip-gram | Epoch 1 | Batch 157000 | Loss: 0.4725\n",
            "Skip-gram | Epoch 1 | Batch 157500 | Loss: 0.4210\n",
            "Skip-gram | Epoch 1 | Batch 158000 | Loss: 0.3856\n",
            "Skip-gram | Epoch 1 | Batch 158500 | Loss: 0.4505\n",
            "Skip-gram | Epoch 1 | Batch 159000 | Loss: 0.4081\n",
            "Skip-gram | Epoch 1 | Batch 159500 | Loss: 0.4143\n",
            "Skip-gram | Epoch 1 | Batch 160000 | Loss: 0.4337\n",
            "Skip-gram | Epoch 1 | Batch 160500 | Loss: 0.4079\n",
            "Skip-gram | Epoch 1 | Batch 161000 | Loss: 0.3964\n",
            "Skip-gram | Epoch 1 | Batch 161500 | Loss: 0.4373\n",
            "Skip-gram | Epoch 1 | Batch 162000 | Loss: 0.4303\n",
            "Skip-gram | Epoch 1 | Batch 162500 | Loss: 0.4367\n",
            "Skip-gram | Epoch 1 | Batch 163000 | Loss: 0.4150\n",
            "Skip-gram | Epoch 1 | Batch 163500 | Loss: 0.4150\n",
            "Skip-gram | Epoch 1 | Batch 164000 | Loss: 0.4222\n",
            "Skip-gram | Epoch 1 | Batch 164500 | Loss: 0.4149\n",
            "Skip-gram | Epoch 1 | Batch 165000 | Loss: 0.4077\n",
            "Skip-gram | Epoch 1 | Batch 165500 | Loss: 0.4149\n",
            "Skip-gram | Epoch 1 | Batch 166000 | Loss: 0.4009\n",
            "Skip-gram | Epoch 1 | Batch 166500 | Loss: 0.4044\n",
            "Skip-gram | Epoch 1 | Batch 167000 | Loss: 0.4048\n",
            "Skip-gram | Epoch 1 | Batch 167500 | Loss: 0.3890\n",
            "Skip-gram | Epoch 1 | Batch 168000 | Loss: 0.4272\n",
            "Skip-gram | Epoch 1 | Batch 168500 | Loss: 0.3880\n",
            "Skip-gram | Epoch 1 | Batch 169000 | Loss: 0.4134\n",
            "Skip-gram | Epoch 1 | Batch 169500 | Loss: 0.4009\n",
            "Skip-gram | Epoch 1 | Batch 170000 | Loss: 0.3865\n",
            "Skip-gram | Epoch 1 | Batch 170500 | Loss: 0.3840\n",
            "Skip-gram | Epoch 1 | Batch 171000 | Loss: 0.4122\n",
            "Skip-gram | Epoch 1 | Batch 171500 | Loss: 0.3982\n",
            "Skip-gram | Epoch 1 | Batch 172000 | Loss: 0.4281\n",
            "Skip-gram | Epoch 1 | Batch 172500 | Loss: 0.4215\n",
            "Skip-gram | Epoch 1 | Batch 173000 | Loss: 0.3917\n",
            "Skip-gram | Epoch 1 | Batch 173500 | Loss: 0.4041\n",
            "Skip-gram | Epoch 1 | Batch 174000 | Loss: 0.4030\n",
            "Skip-gram | Epoch 1 | Batch 174500 | Loss: 0.4013\n",
            "Skip-gram | Epoch 1 | Batch 175000 | Loss: 0.3889\n",
            "Skip-gram | Epoch 1 | Batch 175500 | Loss: 0.4078\n",
            "Skip-gram | Epoch 1 | Batch 176000 | Loss: 0.3976\n",
            "Skip-gram | Epoch 1 | Batch 176500 | Loss: 0.4709\n",
            "Skip-gram | Epoch 1 | Batch 177000 | Loss: 0.4027\n",
            "Skip-gram | Epoch 1 | Batch 177500 | Loss: 0.3604\n",
            "Skip-gram | Epoch 1 | Batch 178000 | Loss: 0.3727\n",
            "Skip-gram | Epoch 1 | Batch 178500 | Loss: 0.4036\n",
            "Skip-gram | Epoch 1 | Batch 179000 | Loss: 0.4192\n",
            "Skip-gram | Epoch 1 | Batch 179500 | Loss: 0.4064\n",
            "Skip-gram | Epoch 1 | Batch 180000 | Loss: 0.3686\n",
            "Skip-gram | Epoch 1 | Batch 180500 | Loss: 0.3633\n",
            "Skip-gram | Epoch 1 | Batch 181000 | Loss: 0.4101\n",
            "Skip-gram | Epoch 1 | Batch 181500 | Loss: 0.3743\n",
            "Skip-gram | Epoch 1 | Batch 182000 | Loss: 0.3683\n",
            "Skip-gram | Epoch 1 | Batch 182500 | Loss: 0.3760\n",
            "Skip-gram | Epoch 1 | Batch 183000 | Loss: 0.3540\n",
            "Skip-gram | Epoch 1 | Batch 183500 | Loss: 0.4171\n",
            "Skip-gram | Epoch 1 | Batch 184000 | Loss: 0.4035\n",
            "Skip-gram | Epoch 1 | Batch 184500 | Loss: 0.3550\n",
            "Skip-gram | Epoch 1 | Batch 185000 | Loss: 0.3606\n",
            "Skip-gram | Epoch 1 | Batch 185500 | Loss: 0.4238\n",
            "Skip-gram | Epoch 1 | Batch 186000 | Loss: 0.3889\n",
            "Skip-gram | Epoch 1 | Batch 186500 | Loss: 0.3557\n",
            "Skip-gram | Epoch 1 | Batch 187000 | Loss: 0.3833\n",
            "Skip-gram | Epoch 1 | Batch 187500 | Loss: 0.4035\n",
            "Skip-gram | Epoch 1 | Batch 188000 | Loss: 0.3880\n",
            "Skip-gram | Epoch 1 | Batch 188500 | Loss: 0.4047\n",
            "Skip-gram | Epoch 1 | Batch 189000 | Loss: 0.3791\n",
            "Skip-gram | Epoch 1 | Batch 189500 | Loss: 0.4204\n",
            "Skip-gram | Epoch 1 | Batch 190000 | Loss: 0.3801\n",
            "Skip-gram | Epoch 1 | Batch 190500 | Loss: 0.3990\n",
            "Skip-gram | Epoch 1 | Batch 191000 | Loss: 0.3724\n",
            "Skip-gram | Epoch 1 | Batch 191500 | Loss: 0.3635\n",
            "Skip-gram | Epoch 1 | Batch 192000 | Loss: 0.3966\n",
            "Skip-gram | Epoch 1 | Batch 192500 | Loss: 0.3594\n",
            "Skip-gram | Epoch 1 | Batch 193000 | Loss: 0.3928\n",
            "Skip-gram | Epoch 1 | Batch 193500 | Loss: 0.3580\n",
            "Skip-gram | Epoch 1 | Batch 194000 | Loss: 0.3638\n",
            "Skip-gram | Epoch 1 | Batch 194500 | Loss: 0.3693\n",
            "Skip-gram | Epoch 1 | Batch 195000 | Loss: 0.3884\n",
            "Skip-gram | Epoch 1 | Batch 195500 | Loss: 0.3672\n",
            "Skip-gram | Epoch 1 | Batch 196000 | Loss: 0.3740\n",
            "Skip-gram | Epoch 1 | Batch 196500 | Loss: 0.3710\n",
            "Skip-gram | Epoch 1 | Batch 197000 | Loss: 0.3566\n",
            "Skip-gram | Epoch 1 | Batch 197500 | Loss: 0.3520\n",
            "Skip-gram | Epoch 1 | Batch 198000 | Loss: 0.3682\n",
            "Skip-gram | Epoch 1 | Batch 198500 | Loss: 0.3435\n",
            "Skip-gram | Epoch 1 | Batch 199000 | Loss: 0.3399\n",
            "Skip-gram | Epoch 1 | Batch 199500 | Loss: 0.3405\n",
            "Skip-gram | Epoch 1 | Batch 200000 | Loss: 0.3636\n",
            "Skip-gram | Epoch 1 | Batch 200500 | Loss: 0.3213\n",
            "Skip-gram | Epoch 1 | Batch 201000 | Loss: 0.3933\n",
            "Skip-gram | Epoch 1 | Batch 201500 | Loss: 0.3596\n",
            "Skip-gram | Epoch 1 | Batch 202000 | Loss: 0.3700\n",
            "Skip-gram | Epoch 1 | Batch 202500 | Loss: 0.3848\n",
            "Skip-gram | Epoch 1 | Batch 203000 | Loss: 0.3722\n",
            "Skip-gram | Epoch 1 | Batch 203500 | Loss: 0.3704\n",
            "Skip-gram | Epoch 1 | Batch 204000 | Loss: 0.3923\n",
            "Skip-gram | Epoch 1 | Batch 204500 | Loss: 0.3826\n",
            "Skip-gram | Epoch 1 | Batch 205000 | Loss: 0.3440\n",
            "Skip-gram | Epoch 1 | Batch 205500 | Loss: 0.3778\n",
            "Skip-gram | Epoch 1 | Batch 206000 | Loss: 0.3973\n",
            "Skip-gram | Epoch 1 | Batch 206500 | Loss: 0.4156\n",
            "Skip-gram | Epoch 1 | Batch 207000 | Loss: 0.3675\n",
            "Skip-gram | Epoch 1 | Batch 207500 | Loss: 0.3683\n",
            "Skip-gram | Epoch 1 | Batch 208000 | Loss: 0.4050\n",
            "Skip-gram | Epoch 1 | Batch 208500 | Loss: 0.3601\n",
            "Skip-gram | Epoch 1 | Batch 209000 | Loss: 0.3844\n",
            "Skip-gram | Epoch 1 | Batch 209500 | Loss: 0.3840\n",
            "Skip-gram | Epoch 1 | Batch 210000 | Loss: 0.3668\n",
            "Skip-gram | Epoch 1 | Batch 210500 | Loss: 0.3539\n",
            "Skip-gram | Epoch 1 | Batch 211000 | Loss: 0.3398\n",
            "Skip-gram | Epoch 1 | Batch 211500 | Loss: 0.2763\n",
            "Skip-gram | Epoch 1 | Batch 212000 | Loss: 0.2664\n",
            "Skip-gram | Epoch 1 | Batch 212500 | Loss: 0.3018\n",
            "Skip-gram | Epoch 1 | Batch 213000 | Loss: 0.3432\n",
            "Skip-gram | Epoch 1 | Batch 213500 | Loss: 0.3936\n",
            "Skip-gram | Epoch 1 | Batch 214000 | Loss: 0.3798\n",
            "Skip-gram | Epoch 1 | Batch 214500 | Loss: 0.3565\n",
            "Skip-gram | Epoch 1 | Batch 215000 | Loss: 0.3365\n",
            "Skip-gram | Epoch 1 | Batch 215500 | Loss: 0.3202\n",
            "Skip-gram | Epoch 1 | Batch 216000 | Loss: 0.3711\n",
            "Skip-gram | Epoch 1 | Batch 216500 | Loss: 0.3527\n",
            "Skip-gram | Epoch 1 | Batch 217000 | Loss: 0.3517\n",
            "Skip-gram | Epoch 1 | Batch 217500 | Loss: 0.3272\n",
            "Skip-gram | Epoch 1 | Batch 218000 | Loss: 0.4055\n",
            "Skip-gram | Epoch 1 | Batch 218500 | Loss: 0.3447\n",
            "Skip-gram | Epoch 1 | Batch 219000 | Loss: 0.3521\n",
            "Skip-gram | Epoch 1 | Batch 219500 | Loss: 0.3753\n",
            "Skip-gram | Epoch 1 | Batch 220000 | Loss: 0.3509\n",
            "Skip-gram | Epoch 1 | Batch 220500 | Loss: 0.3647\n",
            "Skip-gram | Epoch 1 | Batch 221000 | Loss: 0.3646\n",
            "Skip-gram | Epoch 1 | Batch 221500 | Loss: 0.3612\n",
            "Skip-gram | Epoch 1 | Batch 222000 | Loss: 0.3534\n",
            "Skip-gram | Epoch 1 | Batch 222500 | Loss: 0.3193\n",
            "Skip-gram | Epoch 1 | Batch 223000 | Loss: 0.3574\n",
            "Skip-gram | Epoch 1 | Batch 223500 | Loss: 0.3528\n",
            "Skip-gram | Epoch 1 | Batch 224000 | Loss: 0.3672\n",
            "Skip-gram | Epoch 1 | Batch 224500 | Loss: 0.3398\n",
            "Skip-gram | Epoch 1 | Batch 225000 | Loss: 0.3475\n",
            "Skip-gram | Epoch 1 | Batch 225500 | Loss: 0.3505\n",
            "Skip-gram | Epoch 1 | Batch 226000 | Loss: 0.3415\n",
            "Skip-gram | Epoch 1 | Batch 226500 | Loss: 0.3306\n",
            "Skip-gram | Epoch 1 | Batch 227000 | Loss: 0.3732\n",
            "Skip-gram | Epoch 1 | Batch 227500 | Loss: 0.3471\n",
            "Skip-gram | Epoch 1 | Batch 228000 | Loss: 0.3658\n",
            "Skip-gram | Epoch 1 | Batch 228500 | Loss: 0.3553\n",
            "Skip-gram | Epoch 1 | Batch 229000 | Loss: 0.3190\n",
            "Skip-gram | Epoch 1 | Batch 229500 | Loss: 0.3453\n",
            "Skip-gram | Epoch 1 | Batch 230000 | Loss: 0.3323\n",
            "Skip-gram | Epoch 1 | Batch 230500 | Loss: 0.3500\n",
            "Skip-gram | Epoch 1 | Batch 231000 | Loss: 0.3533\n",
            "Skip-gram | Epoch 1 | Batch 231500 | Loss: 0.3222\n",
            "Skip-gram | Epoch 1 | Batch 232000 | Loss: 0.3001\n",
            "Skip-gram | Epoch 1 | Batch 232500 | Loss: 0.2795\n",
            "Skip-gram | Epoch 1 | Batch 233000 | Loss: 0.3875\n",
            "Skip-gram | Epoch 1 | Batch 233500 | Loss: 0.3414\n",
            "Skip-gram | Epoch 1 | Batch 234000 | Loss: 0.3476\n",
            "Skip-gram | Epoch 1 | Batch 234500 | Loss: 0.3418\n",
            "Skip-gram | Epoch 1 | Batch 235000 | Loss: 0.3495\n",
            "Skip-gram | Epoch 1 | Batch 235500 | Loss: 0.3647\n",
            "Skip-gram | Epoch 1 | Batch 236000 | Loss: 0.3288\n",
            "Skip-gram | Epoch 1 | Batch 236500 | Loss: 0.3619\n",
            "Skip-gram | Epoch 1 | Batch 237000 | Loss: 0.3430\n",
            "Skip-gram | Epoch 1 | Batch 237500 | Loss: 0.3365\n",
            "Skip-gram | Epoch 1 | Batch 238000 | Loss: 0.3502\n",
            "Skip-gram | Epoch 1 | Batch 238500 | Loss: 0.3108\n",
            "Skip-gram | Epoch 1 | Batch 239000 | Loss: 0.3255\n",
            "Skip-gram | Epoch 1 | Batch 239500 | Loss: 0.3383\n",
            "Skip-gram | Epoch 1 | Batch 240000 | Loss: 0.3509\n",
            "Skip-gram | Epoch 1 | Batch 240500 | Loss: 0.3509\n",
            "Skip-gram | Epoch 1 | Batch 241000 | Loss: 0.3580\n",
            "Skip-gram | Epoch 1 | Batch 241500 | Loss: 0.3640\n",
            "Skip-gram | Epoch 1 | Batch 242000 | Loss: 0.3016\n",
            "Skip-gram | Epoch 1 | Batch 242500 | Loss: 0.3355\n",
            "Skip-gram | Epoch 1 | Batch 243000 | Loss: 0.3236\n",
            "Skip-gram | Epoch 1 | Batch 243500 | Loss: 0.3423\n",
            "Skip-gram | Epoch 1 | Batch 244000 | Loss: 0.3566\n",
            "Skip-gram | Epoch 1 | Batch 244500 | Loss: 0.3625\n",
            "Skip-gram | Epoch 1 | Batch 245000 | Loss: 0.3803\n",
            "Skip-gram | Epoch 1 | Batch 245500 | Loss: 0.3457\n",
            "Skip-gram | Epoch 1 | Batch 246000 | Loss: 0.3545\n",
            "Skip-gram | Epoch 1 | Batch 246500 | Loss: 0.3354\n",
            "Skip-gram | Epoch 1 | Batch 247000 | Loss: 0.3403\n",
            "Skip-gram | Epoch 1 | Batch 247500 | Loss: 0.3328\n",
            "Skip-gram | Epoch 1 | Batch 248000 | Loss: 0.3067\n",
            "Skip-gram | Epoch 1 | Batch 248500 | Loss: 0.3407\n",
            "Skip-gram | Epoch 1 | Batch 249000 | Loss: 0.3510\n",
            "Skip-gram | Epoch 1 | Batch 249500 | Loss: 0.3372\n",
            "Skip-gram | Epoch 1 | Batch 250000 | Loss: 0.3358\n",
            "Skip-gram | Epoch 1 | Batch 250500 | Loss: 0.3583\n",
            "Skip-gram | Epoch 1 | Batch 251000 | Loss: 0.3553\n",
            "Skip-gram | Epoch 1 | Batch 251500 | Loss: 0.3595\n",
            "Skip-gram | Epoch 1 | Batch 252000 | Loss: 0.3444\n",
            "Skip-gram | Epoch 1 | Batch 252500 | Loss: 0.3273\n",
            "Skip-gram | Epoch 1 | Batch 253000 | Loss: 0.3125\n",
            "Skip-gram | Epoch 1 | Batch 253500 | Loss: 0.3246\n",
            "Skip-gram | Epoch 1 | Batch 254000 | Loss: 0.3501\n",
            "Skip-gram | Epoch 1 | Batch 254500 | Loss: 0.3596\n",
            "Skip-gram | Epoch 1 | Batch 255000 | Loss: 0.2989\n",
            "Skip-gram | Epoch 1 | Batch 255500 | Loss: 0.3357\n",
            "Skip-gram | Epoch 1 | Batch 256000 | Loss: 0.3404\n",
            "Skip-gram | Epoch 1 | Batch 256500 | Loss: 0.3214\n",
            "Skip-gram | Epoch 1 | Batch 257000 | Loss: 0.3187\n",
            "Skip-gram | Epoch 1 | Batch 257500 | Loss: 0.3265\n",
            "Skip-gram | Epoch 1 | Batch 258000 | Loss: 0.3168\n",
            "Skip-gram | Epoch 1 | Batch 258500 | Loss: 0.2591\n",
            "Skip-gram | Epoch 1 | Batch 259000 | Loss: 0.2839\n",
            "Skip-gram | Epoch 1 | Batch 259500 | Loss: 0.3416\n",
            "Skip-gram | Epoch 1 | Batch 260000 | Loss: 0.3540\n",
            "Skip-gram | Epoch 1 | Batch 260500 | Loss: 0.3423\n",
            "Skip-gram | Epoch 1 | Batch 261000 | Loss: 0.3507\n",
            "Skip-gram | Epoch 1 | Batch 261500 | Loss: 0.3151\n",
            "Skip-gram | Epoch 1 | Batch 262000 | Loss: 0.3453\n",
            "Skip-gram | Epoch 1 | Batch 262500 | Loss: 0.3424\n",
            "Skip-gram | Epoch 1 | Batch 263000 | Loss: 0.3418\n",
            "Skip-gram | Epoch 1 | Batch 263500 | Loss: 0.3678\n",
            "Skip-gram | Epoch 1 | Batch 264000 | Loss: 0.3374\n",
            "Skip-gram | Epoch 1 | Batch 264500 | Loss: 0.3398\n",
            "Skip-gram | Epoch 1 | Batch 265000 | Loss: 0.3256\n",
            "Skip-gram | Epoch 1 | Batch 265500 | Loss: 0.3573\n",
            "Skip-gram | Epoch 1 | Batch 266000 | Loss: 0.3263\n",
            "Skip-gram | Epoch 1 | Batch 266500 | Loss: 0.3329\n",
            "Skip-gram | Epoch 1 | Batch 267000 | Loss: 0.3099\n",
            "Skip-gram | Epoch 1 | Batch 267500 | Loss: 0.3336\n",
            "Skip-gram | Epoch 1 | Batch 268000 | Loss: 0.3677\n",
            "Skip-gram | Epoch 1 | Batch 268500 | Loss: 0.3273\n",
            "Skip-gram | Epoch 1 | Batch 269000 | Loss: 0.3445\n",
            "Skip-gram | Epoch 1 | Batch 269500 | Loss: 0.2901\n",
            "Skip-gram | Epoch 1 | Batch 270000 | Loss: 0.3284\n",
            "Skip-gram | Epoch 1 | Batch 270500 | Loss: 0.3658\n",
            "Skip-gram | Epoch 1 | Batch 271000 | Loss: 0.3442\n",
            "Skip-gram | Epoch 1 | Batch 271500 | Loss: 0.3365\n",
            "Skip-gram | Epoch 1 | Batch 272000 | Loss: 0.3140\n",
            "Skip-gram | Epoch 1 | Batch 272500 | Loss: 0.3202\n",
            "Skip-gram | Epoch 1 | Batch 273000 | Loss: 0.3100\n",
            "Skip-gram | Epoch 1 | Batch 273500 | Loss: 0.1954\n",
            "Skip-gram | Epoch 1 | Batch 274000 | Loss: 0.3434\n",
            "Skip-gram | Epoch 1 | Batch 274500 | Loss: 0.3383\n",
            "Skip-gram | Epoch 1 | Batch 275000 | Loss: 0.3331\n",
            "Skip-gram | Epoch 1 | Batch 275500 | Loss: 0.3158\n",
            "Skip-gram | Epoch 1 | Batch 276000 | Loss: 0.3021\n",
            "Skip-gram | Epoch 1 | Batch 276500 | Loss: 0.2847\n",
            "Skip-gram | Epoch 1 | Batch 277000 | Loss: 0.3651\n",
            "Skip-gram | Epoch 1 | Batch 277500 | Loss: 0.3701\n",
            "Skip-gram | Epoch 1 | Batch 278000 | Loss: 0.3191\n",
            "Skip-gram | Epoch 1 | Batch 278500 | Loss: 0.3214\n",
            "Skip-gram | Epoch 1 | Batch 279000 | Loss: 0.3395\n",
            "Skip-gram | Epoch 1 | Batch 279500 | Loss: 0.3260\n",
            "Skip-gram | Epoch 1 | Batch 280000 | Loss: 0.3132\n",
            "Skip-gram | Epoch 1 | Batch 280500 | Loss: 0.3081\n",
            "Skip-gram | Epoch 1 | Batch 281000 | Loss: 0.3174\n",
            "Skip-gram | Epoch 1 | Batch 281500 | Loss: 0.3328\n",
            "Skip-gram | Epoch 1 | Batch 282000 | Loss: 0.3615\n",
            "Skip-gram | Epoch 1 | Batch 282500 | Loss: 0.2988\n",
            "Skip-gram | Epoch 1 | Batch 283000 | Loss: 0.3263\n",
            "Skip-gram | Epoch 1 | Batch 283500 | Loss: 0.1991\n",
            "Skip-gram | Epoch 1 | Batch 284000 | Loss: 0.2196\n",
            "Skip-gram | Epoch 1 | Batch 284500 | Loss: 0.3406\n",
            "Skip-gram | Epoch 1 | Batch 285000 | Loss: 0.3219\n",
            "Skip-gram | Epoch 1 | Batch 285500 | Loss: 0.3169\n",
            "Skip-gram | Epoch 1 | Batch 286000 | Loss: 0.3239\n",
            "Skip-gram | Epoch 1 | Batch 286500 | Loss: 0.3211\n",
            "Skip-gram | Epoch 1 | Batch 287000 | Loss: 0.3286\n",
            "Skip-gram | Epoch 1 | Batch 287500 | Loss: 0.3303\n",
            "Skip-gram | Epoch 1 | Batch 288000 | Loss: 0.3471\n",
            "Skip-gram | Epoch 1 | Batch 288500 | Loss: 0.2441\n",
            "Skip-gram | Epoch 1 | Batch 289000 | Loss: 0.3048\n",
            "Skip-gram | Epoch 1 | Batch 289500 | Loss: 0.3442\n",
            "Skip-gram | Epoch 1 | Batch 290000 | Loss: 0.3395\n",
            "Skip-gram | Epoch 1 | Batch 290500 | Loss: 0.2942\n",
            "Skip-gram | Epoch 1 | Batch 291000 | Loss: 0.3145\n",
            "Skip-gram | Epoch 1 | Batch 291500 | Loss: 0.2883\n",
            "Skip-gram | Epoch 1 | Batch 292000 | Loss: 0.3121\n",
            "Skip-gram | Epoch 1 | Batch 292500 | Loss: 0.3134\n",
            "Skip-gram | Epoch 1 | Batch 293000 | Loss: 0.3413\n",
            "Skip-gram | Epoch 1 | Batch 293500 | Loss: 0.3174\n",
            "Skip-gram | Epoch 1 | Batch 294000 | Loss: 0.3318\n",
            "Skip-gram | Epoch 1 | Batch 294500 | Loss: 0.3196\n",
            "Skip-gram | Epoch 1 | Batch 295000 | Loss: 0.3255\n",
            "Skip-gram | Epoch 1 | Batch 295500 | Loss: 0.3309\n",
            "Skip-gram | Epoch 1 | Batch 296000 | Loss: 0.3280\n",
            "Skip-gram | Epoch 1 | Batch 296500 | Loss: 0.3282\n",
            "Skip-gram | Epoch 1 | Batch 297000 | Loss: 0.3185\n",
            "Skip-gram | Epoch 1 | Batch 297500 | Loss: 0.3421\n",
            "Skip-gram | Epoch 1 | Batch 298000 | Loss: 0.2647\n",
            "Skip-gram | Epoch 1 | Batch 298500 | Loss: 0.3514\n",
            "Skip-gram | Epoch 1 | Batch 299000 | Loss: 0.3331\n",
            "Skip-gram | Epoch 1 | Batch 299500 | Loss: 0.3232\n",
            "Skip-gram | Epoch 1 | Batch 300000 | Loss: 0.3126\n",
            "Skip-gram | Epoch 1 | Batch 300500 | Loss: 0.3302\n",
            "Skip-gram | Epoch 1 | Batch 301000 | Loss: 0.3002\n",
            "Skip-gram | Epoch 1 | Batch 301500 | Loss: 0.3264\n",
            "Skip-gram | Epoch 1 | Batch 302000 | Loss: 0.3207\n",
            "Skip-gram | Epoch 1 | Batch 302500 | Loss: 0.3328\n",
            "Skip-gram | Epoch 1 | Batch 303000 | Loss: 0.3124\n",
            "Skip-gram | Epoch 1 | Batch 303500 | Loss: 0.2956\n",
            "Skip-gram | Epoch 1 | Batch 304000 | Loss: 0.3271\n",
            "Skip-gram | Epoch 1 | Batch 304500 | Loss: 0.3531\n",
            "Skip-gram | Epoch 1 | Batch 305000 | Loss: 0.3287\n",
            "Skip-gram | Epoch 1 | Batch 305500 | Loss: 0.3047\n",
            "Skip-gram | Epoch 1 | Batch 306000 | Loss: 0.3250\n",
            "Skip-gram | Epoch 1 | Batch 306500 | Loss: 0.3269\n",
            "Skip-gram | Epoch 1 | Batch 307000 | Loss: 0.3252\n",
            "Skip-gram | Epoch 1 | Batch 307500 | Loss: 0.3330\n",
            "Skip-gram | Epoch 1 | Batch 308000 | Loss: 0.2909\n",
            "Skip-gram | Epoch 1 | Batch 308500 | Loss: 0.3463\n",
            "Skip-gram | Epoch 1 | Batch 309000 | Loss: 0.3183\n",
            "Skip-gram | Epoch 1 | Batch 309500 | Loss: 0.3228\n",
            "Skip-gram | Epoch 1 | Batch 310000 | Loss: 0.3071\n",
            "Skip-gram | Epoch 1 | Batch 310500 | Loss: 0.3308\n",
            "Skip-gram | Epoch 1 | Batch 311000 | Loss: 0.3452\n",
            "Skip-gram | Epoch 1 | Batch 311500 | Loss: 0.3263\n",
            "Skip-gram | Epoch 1 | Batch 312000 | Loss: 0.3261\n",
            "Skip-gram | Epoch 1 | Batch 312500 | Loss: 0.3192\n",
            "Skip-gram | Epoch 1 | Batch 313000 | Loss: 0.3339\n",
            "Skip-gram | Epoch 1 | Batch 313500 | Loss: 0.3294\n",
            "Skip-gram | Epoch 1 | Batch 314000 | Loss: 0.3327\n",
            "Skip-gram | Epoch 1 | Batch 314500 | Loss: 0.3016\n",
            "Skip-gram | Epoch 1 | Batch 315000 | Loss: 0.3110\n",
            "Skip-gram | Epoch 1 | Batch 315500 | Loss: 0.3339\n",
            "Skip-gram | Epoch 1 | Batch 316000 | Loss: 0.3172\n",
            "Skip-gram | Epoch 1 | Batch 316500 | Loss: 0.3414\n",
            "Skip-gram | Epoch 1 | Batch 317000 | Loss: 0.2768\n",
            "Skip-gram | Epoch 1 | Batch 317500 | Loss: 0.2720\n",
            "Skip-gram | Epoch 1 | Batch 318000 | Loss: 0.3218\n",
            "Skip-gram | Epoch 1 | Batch 318500 | Loss: 0.2985\n",
            "Skip-gram | Epoch 1 | Batch 319000 | Loss: 0.3107\n",
            "Skip-gram | Epoch 1 | Batch 319500 | Loss: 0.3171\n",
            "Skip-gram | Epoch 1 | Batch 320000 | Loss: 0.3192\n",
            "Skip-gram | Epoch 1 | Batch 320500 | Loss: 0.3592\n",
            "Skip-gram | Epoch 1 | Batch 321000 | Loss: 0.3200\n",
            "Skip-gram | Epoch 1 | Batch 321500 | Loss: 0.3250\n",
            "Skip-gram | Epoch 1 | Batch 322000 | Loss: 0.3229\n",
            "Skip-gram | Epoch 1 | Batch 322500 | Loss: 0.3375\n",
            "Skip-gram | Epoch 1 | Batch 323000 | Loss: 0.2745\n",
            "Skip-gram | Epoch 1 | Batch 323500 | Loss: 0.3316\n",
            "Skip-gram Epoch 1. Средний Loss: 0.6898\n",
            "Skip-gram | Epoch 2 | Batch 500 | Loss: 0.3393\n",
            "Skip-gram | Epoch 2 | Batch 1000 | Loss: 0.2900\n",
            "Skip-gram | Epoch 2 | Batch 1500 | Loss: 0.2937\n",
            "Skip-gram | Epoch 2 | Batch 2000 | Loss: 0.3096\n",
            "Skip-gram | Epoch 2 | Batch 2500 | Loss: 0.2943\n",
            "Skip-gram | Epoch 2 | Batch 3000 | Loss: 0.2895\n",
            "Skip-gram | Epoch 2 | Batch 3500 | Loss: 0.3202\n",
            "Skip-gram | Epoch 2 | Batch 4000 | Loss: 0.3050\n",
            "Skip-gram | Epoch 2 | Batch 4500 | Loss: 0.3091\n",
            "Skip-gram | Epoch 2 | Batch 5000 | Loss: 0.3158\n",
            "Skip-gram | Epoch 2 | Batch 5500 | Loss: 0.3472\n",
            "Skip-gram | Epoch 2 | Batch 6000 | Loss: 0.2979\n",
            "Skip-gram | Epoch 2 | Batch 6500 | Loss: 0.2994\n",
            "Skip-gram | Epoch 2 | Batch 7000 | Loss: 0.2935\n",
            "Skip-gram | Epoch 2 | Batch 7500 | Loss: 0.2923\n",
            "Skip-gram | Epoch 2 | Batch 8000 | Loss: 0.2843\n",
            "Skip-gram | Epoch 2 | Batch 8500 | Loss: 0.3097\n",
            "Skip-gram | Epoch 2 | Batch 9000 | Loss: 0.3014\n",
            "Skip-gram | Epoch 2 | Batch 9500 | Loss: 0.3122\n",
            "Skip-gram | Epoch 2 | Batch 10000 | Loss: 0.3008\n",
            "Skip-gram | Epoch 2 | Batch 10500 | Loss: 0.3168\n",
            "Skip-gram | Epoch 2 | Batch 11000 | Loss: 0.3114\n",
            "Skip-gram | Epoch 2 | Batch 11500 | Loss: 0.3269\n",
            "Skip-gram | Epoch 2 | Batch 12000 | Loss: 0.3310\n",
            "Skip-gram | Epoch 2 | Batch 12500 | Loss: 0.3015\n",
            "Skip-gram | Epoch 2 | Batch 13000 | Loss: 0.3248\n",
            "Skip-gram | Epoch 2 | Batch 13500 | Loss: 0.3142\n",
            "Skip-gram | Epoch 2 | Batch 14000 | Loss: 0.3202\n",
            "Skip-gram | Epoch 2 | Batch 14500 | Loss: 0.2979\n",
            "Skip-gram | Epoch 2 | Batch 15000 | Loss: 0.3095\n",
            "Skip-gram | Epoch 2 | Batch 15500 | Loss: 0.2760\n",
            "Skip-gram | Epoch 2 | Batch 16000 | Loss: 0.3004\n",
            "Skip-gram | Epoch 2 | Batch 16500 | Loss: 0.3293\n",
            "Skip-gram | Epoch 2 | Batch 17000 | Loss: 0.2949\n",
            "Skip-gram | Epoch 2 | Batch 17500 | Loss: 0.2705\n",
            "Skip-gram | Epoch 2 | Batch 18000 | Loss: 0.3109\n",
            "Skip-gram | Epoch 2 | Batch 18500 | Loss: 0.3391\n",
            "Skip-gram | Epoch 2 | Batch 19000 | Loss: 0.3063\n",
            "Skip-gram | Epoch 2 | Batch 19500 | Loss: 0.3017\n",
            "Skip-gram | Epoch 2 | Batch 20000 | Loss: 0.2877\n",
            "Skip-gram | Epoch 2 | Batch 20500 | Loss: 0.2940\n",
            "Skip-gram | Epoch 2 | Batch 21000 | Loss: 0.3140\n",
            "Skip-gram | Epoch 2 | Batch 21500 | Loss: 0.2996\n",
            "Skip-gram | Epoch 2 | Batch 22000 | Loss: 0.3296\n",
            "Skip-gram | Epoch 2 | Batch 22500 | Loss: 0.3053\n",
            "Skip-gram | Epoch 2 | Batch 23000 | Loss: 0.3141\n",
            "Skip-gram | Epoch 2 | Batch 23500 | Loss: 0.3131\n",
            "Skip-gram | Epoch 2 | Batch 24000 | Loss: 0.3078\n",
            "Skip-gram | Epoch 2 | Batch 24500 | Loss: 0.2923\n",
            "Skip-gram | Epoch 2 | Batch 25000 | Loss: 0.3049\n",
            "Skip-gram | Epoch 2 | Batch 25500 | Loss: 0.3143\n",
            "Skip-gram | Epoch 2 | Batch 26000 | Loss: 0.3098\n",
            "Skip-gram | Epoch 2 | Batch 26500 | Loss: 0.2762\n",
            "Skip-gram | Epoch 2 | Batch 27000 | Loss: 0.2825\n",
            "Skip-gram | Epoch 2 | Batch 27500 | Loss: 0.3116\n",
            "Skip-gram | Epoch 2 | Batch 28000 | Loss: 0.3127\n",
            "Skip-gram | Epoch 2 | Batch 28500 | Loss: 0.3235\n",
            "Skip-gram | Epoch 2 | Batch 29000 | Loss: 0.3145\n",
            "Skip-gram | Epoch 2 | Batch 29500 | Loss: 0.3306\n",
            "Skip-gram | Epoch 2 | Batch 30000 | Loss: 0.2856\n",
            "Skip-gram | Epoch 2 | Batch 30500 | Loss: 0.3269\n",
            "Skip-gram | Epoch 2 | Batch 31000 | Loss: 0.2925\n",
            "Skip-gram | Epoch 2 | Batch 31500 | Loss: 0.3756\n",
            "Skip-gram | Epoch 2 | Batch 32000 | Loss: 0.3172\n",
            "Skip-gram | Epoch 2 | Batch 32500 | Loss: 0.2849\n",
            "Skip-gram | Epoch 2 | Batch 33000 | Loss: 0.3328\n",
            "Skip-gram | Epoch 2 | Batch 33500 | Loss: 0.2972\n",
            "Skip-gram | Epoch 2 | Batch 34000 | Loss: 0.3000\n",
            "Skip-gram | Epoch 2 | Batch 34500 | Loss: 0.2665\n",
            "Skip-gram | Epoch 2 | Batch 35000 | Loss: 0.1802\n",
            "Skip-gram | Epoch 2 | Batch 35500 | Loss: 0.1497\n",
            "Skip-gram | Epoch 2 | Batch 36000 | Loss: 0.1598\n",
            "Skip-gram | Epoch 2 | Batch 36500 | Loss: 0.3096\n",
            "Skip-gram | Epoch 2 | Batch 37000 | Loss: 0.2927\n",
            "Skip-gram | Epoch 2 | Batch 37500 | Loss: 0.3348\n",
            "Skip-gram | Epoch 2 | Batch 38000 | Loss: 0.2842\n",
            "Skip-gram | Epoch 2 | Batch 38500 | Loss: 0.3038\n",
            "Skip-gram | Epoch 2 | Batch 39000 | Loss: 0.3270\n",
            "Skip-gram | Epoch 2 | Batch 39500 | Loss: 0.3093\n",
            "Skip-gram | Epoch 2 | Batch 40000 | Loss: 0.2813\n",
            "Skip-gram | Epoch 2 | Batch 40500 | Loss: 0.3053\n",
            "Skip-gram | Epoch 2 | Batch 41000 | Loss: 0.3054\n",
            "Skip-gram | Epoch 2 | Batch 41500 | Loss: 0.2871\n",
            "Skip-gram | Epoch 2 | Batch 42000 | Loss: 0.3306\n",
            "Skip-gram | Epoch 2 | Batch 42500 | Loss: 0.2825\n",
            "Skip-gram | Epoch 2 | Batch 43000 | Loss: 0.3064\n",
            "Skip-gram | Epoch 2 | Batch 43500 | Loss: 0.3256\n",
            "Skip-gram | Epoch 2 | Batch 44000 | Loss: 0.3220\n",
            "Skip-gram | Epoch 2 | Batch 44500 | Loss: 0.3094\n",
            "Skip-gram | Epoch 2 | Batch 45000 | Loss: 0.3312\n",
            "Skip-gram | Epoch 2 | Batch 45500 | Loss: 0.3585\n",
            "Skip-gram | Epoch 2 | Batch 46000 | Loss: 0.2897\n",
            "Skip-gram | Epoch 2 | Batch 46500 | Loss: 0.3270\n",
            "Skip-gram | Epoch 2 | Batch 47000 | Loss: 0.2949\n",
            "Skip-gram | Epoch 2 | Batch 47500 | Loss: 0.3136\n",
            "Skip-gram | Epoch 2 | Batch 48000 | Loss: 0.3113\n",
            "Skip-gram | Epoch 2 | Batch 48500 | Loss: 0.3080\n",
            "Skip-gram | Epoch 2 | Batch 49000 | Loss: 0.2908\n",
            "Skip-gram | Epoch 2 | Batch 49500 | Loss: 0.2919\n",
            "Skip-gram | Epoch 2 | Batch 50000 | Loss: 0.3123\n",
            "Skip-gram | Epoch 2 | Batch 50500 | Loss: 0.3142\n",
            "Skip-gram | Epoch 2 | Batch 51000 | Loss: 0.3031\n",
            "Skip-gram | Epoch 2 | Batch 51500 | Loss: 0.3127\n",
            "Skip-gram | Epoch 2 | Batch 52000 | Loss: 0.3073\n",
            "Skip-gram | Epoch 2 | Batch 52500 | Loss: 0.2981\n",
            "Skip-gram | Epoch 2 | Batch 53000 | Loss: 0.3158\n",
            "Skip-gram | Epoch 2 | Batch 53500 | Loss: 0.3077\n",
            "Skip-gram | Epoch 2 | Batch 54000 | Loss: 0.3097\n",
            "Skip-gram | Epoch 2 | Batch 54500 | Loss: 0.3060\n",
            "Skip-gram | Epoch 2 | Batch 55000 | Loss: 0.3182\n",
            "Skip-gram | Epoch 2 | Batch 55500 | Loss: 0.3221\n",
            "Skip-gram | Epoch 2 | Batch 56000 | Loss: 0.2187\n",
            "Skip-gram | Epoch 2 | Batch 56500 | Loss: 0.2509\n",
            "Skip-gram | Epoch 2 | Batch 57000 | Loss: 0.3166\n",
            "Skip-gram | Epoch 2 | Batch 57500 | Loss: 0.3170\n",
            "Skip-gram | Epoch 2 | Batch 58000 | Loss: 0.2969\n",
            "Skip-gram | Epoch 2 | Batch 58500 | Loss: 0.3267\n",
            "Skip-gram | Epoch 2 | Batch 59000 | Loss: 0.3069\n",
            "Skip-gram | Epoch 2 | Batch 59500 | Loss: 0.3054\n",
            "Skip-gram | Epoch 2 | Batch 60000 | Loss: 0.2754\n",
            "Skip-gram | Epoch 2 | Batch 60500 | Loss: 0.2974\n",
            "Skip-gram | Epoch 2 | Batch 61000 | Loss: 0.2853\n",
            "Skip-gram | Epoch 2 | Batch 61500 | Loss: 0.2933\n",
            "Skip-gram | Epoch 2 | Batch 62000 | Loss: 0.3246\n",
            "Skip-gram | Epoch 2 | Batch 62500 | Loss: 0.3228\n",
            "Skip-gram | Epoch 2 | Batch 63000 | Loss: 0.3120\n",
            "Skip-gram | Epoch 2 | Batch 63500 | Loss: 0.3028\n",
            "Skip-gram | Epoch 2 | Batch 64000 | Loss: 0.3136\n",
            "Skip-gram | Epoch 2 | Batch 64500 | Loss: 0.3098\n",
            "Skip-gram | Epoch 2 | Batch 65000 | Loss: 0.3253\n",
            "Skip-gram | Epoch 2 | Batch 65500 | Loss: 0.3121\n",
            "Skip-gram | Epoch 2 | Batch 66000 | Loss: 0.3034\n",
            "Skip-gram | Epoch 2 | Batch 66500 | Loss: 0.3057\n",
            "Skip-gram | Epoch 2 | Batch 67000 | Loss: 0.3059\n",
            "Skip-gram | Epoch 2 | Batch 67500 | Loss: 0.3101\n",
            "Skip-gram | Epoch 2 | Batch 68000 | Loss: 0.2888\n",
            "Skip-gram | Epoch 2 | Batch 68500 | Loss: 0.3081\n",
            "Skip-gram | Epoch 2 | Batch 69000 | Loss: 0.3381\n",
            "Skip-gram | Epoch 2 | Batch 69500 | Loss: 0.3156\n",
            "Skip-gram | Epoch 2 | Batch 70000 | Loss: 0.2974\n",
            "Skip-gram | Epoch 2 | Batch 70500 | Loss: 0.2966\n",
            "Skip-gram | Epoch 2 | Batch 71000 | Loss: 0.3118\n",
            "Skip-gram | Epoch 2 | Batch 71500 | Loss: 0.3092\n",
            "Skip-gram | Epoch 2 | Batch 72000 | Loss: 0.3140\n",
            "Skip-gram | Epoch 2 | Batch 72500 | Loss: 0.2891\n",
            "Skip-gram | Epoch 2 | Batch 73000 | Loss: 0.3339\n",
            "Skip-gram | Epoch 2 | Batch 73500 | Loss: 0.2951\n",
            "Skip-gram | Epoch 2 | Batch 74000 | Loss: 0.2748\n",
            "Skip-gram | Epoch 2 | Batch 74500 | Loss: 0.2384\n",
            "Skip-gram | Epoch 2 | Batch 75000 | Loss: 0.2488\n",
            "Skip-gram | Epoch 2 | Batch 75500 | Loss: 0.2601\n",
            "Skip-gram | Epoch 2 | Batch 76000 | Loss: 0.3122\n",
            "Skip-gram | Epoch 2 | Batch 76500 | Loss: 0.3011\n",
            "Skip-gram | Epoch 2 | Batch 77000 | Loss: 0.2763\n",
            "Skip-gram | Epoch 2 | Batch 77500 | Loss: 0.3196\n",
            "Skip-gram | Epoch 2 | Batch 78000 | Loss: 0.3218\n",
            "Skip-gram | Epoch 2 | Batch 78500 | Loss: 0.2910\n",
            "Skip-gram | Epoch 2 | Batch 79000 | Loss: 0.2830\n",
            "Skip-gram | Epoch 2 | Batch 79500 | Loss: 0.3171\n",
            "Skip-gram | Epoch 2 | Batch 80000 | Loss: 0.2788\n",
            "Skip-gram | Epoch 2 | Batch 80500 | Loss: 0.2986\n",
            "Skip-gram | Epoch 2 | Batch 81000 | Loss: 0.2834\n",
            "Skip-gram | Epoch 2 | Batch 81500 | Loss: 0.2777\n",
            "Skip-gram | Epoch 2 | Batch 82000 | Loss: 0.2751\n",
            "Skip-gram | Epoch 2 | Batch 82500 | Loss: 0.2672\n",
            "Skip-gram | Epoch 2 | Batch 83000 | Loss: 0.2805\n",
            "Skip-gram | Epoch 2 | Batch 83500 | Loss: 0.3184\n",
            "Skip-gram | Epoch 2 | Batch 84000 | Loss: 0.2850\n",
            "Skip-gram | Epoch 2 | Batch 84500 | Loss: 0.2987\n",
            "Skip-gram | Epoch 2 | Batch 85000 | Loss: 0.2565\n",
            "Skip-gram | Epoch 2 | Batch 85500 | Loss: 0.2683\n",
            "Skip-gram | Epoch 2 | Batch 86000 | Loss: 0.3292\n",
            "Skip-gram | Epoch 2 | Batch 86500 | Loss: 0.3159\n",
            "Skip-gram | Epoch 2 | Batch 87000 | Loss: 0.2983\n",
            "Skip-gram | Epoch 2 | Batch 87500 | Loss: 0.2811\n",
            "Skip-gram | Epoch 2 | Batch 88000 | Loss: 0.2837\n",
            "Skip-gram | Epoch 2 | Batch 88500 | Loss: 0.3067\n",
            "Skip-gram | Epoch 2 | Batch 89000 | Loss: 0.2869\n",
            "Skip-gram | Epoch 2 | Batch 89500 | Loss: 0.2899\n",
            "Skip-gram | Epoch 2 | Batch 90000 | Loss: 0.2997\n",
            "Skip-gram | Epoch 2 | Batch 90500 | Loss: 0.3038\n",
            "Skip-gram | Epoch 2 | Batch 91000 | Loss: 0.3356\n",
            "Skip-gram | Epoch 2 | Batch 91500 | Loss: 0.2912\n",
            "Skip-gram | Epoch 2 | Batch 92000 | Loss: 0.2986\n",
            "Skip-gram | Epoch 2 | Batch 92500 | Loss: 0.3088\n",
            "Skip-gram | Epoch 2 | Batch 93000 | Loss: 0.3186\n",
            "Skip-gram | Epoch 2 | Batch 93500 | Loss: 0.2838\n",
            "Skip-gram | Epoch 2 | Batch 94000 | Loss: 0.3168\n",
            "Skip-gram | Epoch 2 | Batch 94500 | Loss: 0.2519\n",
            "Skip-gram | Epoch 2 | Batch 95000 | Loss: 0.2988\n",
            "Skip-gram | Epoch 2 | Batch 95500 | Loss: 0.2639\n",
            "Skip-gram | Epoch 2 | Batch 96000 | Loss: 0.3418\n",
            "Skip-gram | Epoch 2 | Batch 96500 | Loss: 0.3018\n",
            "Skip-gram | Epoch 2 | Batch 97000 | Loss: 0.3016\n",
            "Skip-gram | Epoch 2 | Batch 97500 | Loss: 0.3277\n",
            "Skip-gram | Epoch 2 | Batch 98000 | Loss: 0.2914\n",
            "Skip-gram | Epoch 2 | Batch 98500 | Loss: 0.3346\n",
            "Skip-gram | Epoch 2 | Batch 99000 | Loss: 0.3141\n",
            "Skip-gram | Epoch 2 | Batch 99500 | Loss: 0.3111\n",
            "Skip-gram | Epoch 2 | Batch 100000 | Loss: 0.3015\n",
            "Skip-gram | Epoch 2 | Batch 100500 | Loss: 0.3070\n",
            "Skip-gram | Epoch 2 | Batch 101000 | Loss: 0.3016\n",
            "Skip-gram | Epoch 2 | Batch 101500 | Loss: 0.3043\n",
            "Skip-gram | Epoch 2 | Batch 102000 | Loss: 0.3334\n",
            "Skip-gram | Epoch 2 | Batch 102500 | Loss: 0.3381\n",
            "Skip-gram | Epoch 2 | Batch 103000 | Loss: 0.3108\n",
            "Skip-gram | Epoch 2 | Batch 103500 | Loss: 0.2877\n",
            "Skip-gram | Epoch 2 | Batch 104000 | Loss: 0.3036\n",
            "Skip-gram | Epoch 2 | Batch 104500 | Loss: 0.3047\n",
            "Skip-gram | Epoch 2 | Batch 105000 | Loss: 0.3011\n",
            "Skip-gram | Epoch 2 | Batch 105500 | Loss: 0.3005\n",
            "Skip-gram | Epoch 2 | Batch 106000 | Loss: 0.2987\n",
            "Skip-gram | Epoch 2 | Batch 106500 | Loss: 0.3478\n",
            "Skip-gram | Epoch 2 | Batch 107000 | Loss: 0.3063\n",
            "Skip-gram | Epoch 2 | Batch 107500 | Loss: 0.2929\n",
            "Skip-gram | Epoch 2 | Batch 108000 | Loss: 0.3084\n",
            "Skip-gram | Epoch 2 | Batch 108500 | Loss: 0.2970\n",
            "Skip-gram | Epoch 2 | Batch 109000 | Loss: 0.3034\n",
            "Skip-gram | Epoch 2 | Batch 109500 | Loss: 0.3110\n",
            "Skip-gram | Epoch 2 | Batch 110000 | Loss: 0.3091\n",
            "Skip-gram | Epoch 2 | Batch 110500 | Loss: 0.3091\n",
            "Skip-gram | Epoch 2 | Batch 111000 | Loss: 0.3401\n",
            "Skip-gram | Epoch 2 | Batch 111500 | Loss: 0.3457\n",
            "Skip-gram | Epoch 2 | Batch 112000 | Loss: 0.3224\n",
            "Skip-gram | Epoch 2 | Batch 112500 | Loss: 0.3253\n",
            "Skip-gram | Epoch 2 | Batch 113000 | Loss: 0.2906\n",
            "Skip-gram | Epoch 2 | Batch 113500 | Loss: 0.3116\n",
            "Skip-gram | Epoch 2 | Batch 114000 | Loss: 0.3085\n",
            "Skip-gram | Epoch 2 | Batch 114500 | Loss: 0.3085\n",
            "Skip-gram | Epoch 2 | Batch 115000 | Loss: 0.3051\n",
            "Skip-gram | Epoch 2 | Batch 115500 | Loss: 0.2925\n",
            "Skip-gram | Epoch 2 | Batch 116000 | Loss: 0.3155\n",
            "Skip-gram | Epoch 2 | Batch 116500 | Loss: 0.2942\n",
            "Skip-gram | Epoch 2 | Batch 117000 | Loss: 0.2839\n",
            "Skip-gram | Epoch 2 | Batch 117500 | Loss: 0.2861\n",
            "Skip-gram | Epoch 2 | Batch 118000 | Loss: 0.3224\n",
            "Skip-gram | Epoch 2 | Batch 118500 | Loss: 0.2962\n",
            "Skip-gram | Epoch 2 | Batch 119000 | Loss: 0.2986\n",
            "Skip-gram | Epoch 2 | Batch 119500 | Loss: 0.3040\n",
            "Skip-gram | Epoch 2 | Batch 120000 | Loss: 0.3170\n",
            "Skip-gram | Epoch 2 | Batch 120500 | Loss: 0.3148\n",
            "Skip-gram | Epoch 2 | Batch 121000 | Loss: 0.3096\n",
            "Skip-gram | Epoch 2 | Batch 121500 | Loss: 0.3129\n",
            "Skip-gram | Epoch 2 | Batch 122000 | Loss: 0.2901\n",
            "Skip-gram | Epoch 2 | Batch 122500 | Loss: 0.2510\n",
            "Skip-gram | Epoch 2 | Batch 123000 | Loss: 0.1240\n",
            "Skip-gram | Epoch 2 | Batch 123500 | Loss: 0.1467\n",
            "Skip-gram | Epoch 2 | Batch 124000 | Loss: 0.1561\n",
            "Skip-gram | Epoch 2 | Batch 124500 | Loss: 0.1577\n",
            "Skip-gram | Epoch 2 | Batch 125000 | Loss: 0.1159\n",
            "Skip-gram | Epoch 2 | Batch 125500 | Loss: 0.1022\n",
            "Skip-gram | Epoch 2 | Batch 126000 | Loss: 0.2101\n",
            "Skip-gram | Epoch 2 | Batch 126500 | Loss: 0.2097\n",
            "Skip-gram | Epoch 2 | Batch 127000 | Loss: 0.3252\n",
            "Skip-gram | Epoch 2 | Batch 127500 | Loss: 0.3133\n",
            "Skip-gram | Epoch 2 | Batch 128000 | Loss: 0.3022\n",
            "Skip-gram | Epoch 2 | Batch 128500 | Loss: 0.3451\n",
            "Skip-gram | Epoch 2 | Batch 129000 | Loss: 0.3224\n",
            "Skip-gram | Epoch 2 | Batch 129500 | Loss: 0.2996\n",
            "Skip-gram | Epoch 2 | Batch 130000 | Loss: 0.2955\n",
            "Skip-gram | Epoch 2 | Batch 130500 | Loss: 0.2883\n",
            "Skip-gram | Epoch 2 | Batch 131000 | Loss: 0.3241\n",
            "Skip-gram | Epoch 2 | Batch 131500 | Loss: 0.2989\n",
            "Skip-gram | Epoch 2 | Batch 132000 | Loss: 0.3072\n",
            "Skip-gram | Epoch 2 | Batch 132500 | Loss: 0.2999\n",
            "Skip-gram | Epoch 2 | Batch 133000 | Loss: 0.2581\n",
            "Skip-gram | Epoch 2 | Batch 133500 | Loss: 0.2795\n",
            "Skip-gram | Epoch 2 | Batch 134000 | Loss: 0.2775\n",
            "Skip-gram | Epoch 2 | Batch 134500 | Loss: 0.2653\n",
            "Skip-gram | Epoch 2 | Batch 135000 | Loss: 0.2889\n",
            "Skip-gram | Epoch 2 | Batch 135500 | Loss: 0.2993\n",
            "Skip-gram | Epoch 2 | Batch 136000 | Loss: 0.2960\n",
            "Skip-gram | Epoch 2 | Batch 136500 | Loss: 0.3192\n",
            "Skip-gram | Epoch 2 | Batch 137000 | Loss: 0.3107\n",
            "Skip-gram | Epoch 2 | Batch 137500 | Loss: 0.3187\n",
            "Skip-gram | Epoch 2 | Batch 138000 | Loss: 0.3237\n",
            "Skip-gram | Epoch 2 | Batch 138500 | Loss: 0.2947\n",
            "Skip-gram | Epoch 2 | Batch 139000 | Loss: 0.2985\n",
            "Skip-gram | Epoch 2 | Batch 139500 | Loss: 0.3114\n",
            "Skip-gram | Epoch 2 | Batch 140000 | Loss: 0.2888\n",
            "Skip-gram | Epoch 2 | Batch 140500 | Loss: 0.3085\n",
            "Skip-gram | Epoch 2 | Batch 141000 | Loss: 0.3028\n",
            "Skip-gram | Epoch 2 | Batch 141500 | Loss: 0.2999\n",
            "Skip-gram | Epoch 2 | Batch 142000 | Loss: 0.3032\n",
            "Skip-gram | Epoch 2 | Batch 142500 | Loss: 0.2959\n",
            "Skip-gram | Epoch 2 | Batch 143000 | Loss: 0.2951\n",
            "Skip-gram | Epoch 2 | Batch 143500 | Loss: 0.3194\n",
            "Skip-gram | Epoch 2 | Batch 144000 | Loss: 0.3203\n",
            "Skip-gram | Epoch 2 | Batch 144500 | Loss: 0.2935\n",
            "Skip-gram | Epoch 2 | Batch 145000 | Loss: 0.3111\n",
            "Skip-gram | Epoch 2 | Batch 145500 | Loss: 0.2998\n",
            "Skip-gram | Epoch 2 | Batch 146000 | Loss: 0.3108\n",
            "Skip-gram | Epoch 2 | Batch 146500 | Loss: 0.3117\n",
            "Skip-gram | Epoch 2 | Batch 147000 | Loss: 0.3183\n",
            "Skip-gram | Epoch 2 | Batch 147500 | Loss: 0.3130\n",
            "Skip-gram | Epoch 2 | Batch 148000 | Loss: 0.2883\n",
            "Skip-gram | Epoch 2 | Batch 148500 | Loss: 0.3095\n",
            "Skip-gram | Epoch 2 | Batch 149000 | Loss: 0.3385\n",
            "Skip-gram | Epoch 2 | Batch 149500 | Loss: 0.2961\n",
            "Skip-gram | Epoch 2 | Batch 150000 | Loss: 0.2961\n",
            "Skip-gram | Epoch 2 | Batch 150500 | Loss: 0.3156\n",
            "Skip-gram | Epoch 2 | Batch 151000 | Loss: 0.2852\n",
            "Skip-gram | Epoch 2 | Batch 151500 | Loss: 0.3507\n",
            "Skip-gram | Epoch 2 | Batch 152000 | Loss: 0.3287\n",
            "Skip-gram | Epoch 2 | Batch 152500 | Loss: 0.3017\n",
            "Skip-gram | Epoch 2 | Batch 153000 | Loss: 0.3093\n",
            "Skip-gram | Epoch 2 | Batch 153500 | Loss: 0.2988\n",
            "Skip-gram | Epoch 2 | Batch 154000 | Loss: 0.3251\n",
            "Skip-gram | Epoch 2 | Batch 154500 | Loss: 0.3072\n",
            "Skip-gram | Epoch 2 | Batch 155000 | Loss: 0.2947\n",
            "Skip-gram | Epoch 2 | Batch 155500 | Loss: 0.2959\n",
            "Skip-gram | Epoch 2 | Batch 156000 | Loss: 0.2909\n",
            "Skip-gram | Epoch 2 | Batch 156500 | Loss: 0.3154\n",
            "Skip-gram | Epoch 2 | Batch 157000 | Loss: 0.3224\n",
            "Skip-gram | Epoch 2 | Batch 157500 | Loss: 0.3047\n",
            "Skip-gram | Epoch 2 | Batch 158000 | Loss: 0.2825\n",
            "Skip-gram | Epoch 2 | Batch 158500 | Loss: 0.3222\n",
            "Skip-gram | Epoch 2 | Batch 159000 | Loss: 0.2969\n",
            "Skip-gram | Epoch 2 | Batch 159500 | Loss: 0.2990\n",
            "Skip-gram | Epoch 2 | Batch 160000 | Loss: 0.3163\n",
            "Skip-gram | Epoch 2 | Batch 160500 | Loss: 0.3046\n",
            "Skip-gram | Epoch 2 | Batch 161000 | Loss: 0.2940\n",
            "Skip-gram | Epoch 2 | Batch 161500 | Loss: 0.3128\n",
            "Skip-gram | Epoch 2 | Batch 162000 | Loss: 0.3104\n",
            "Skip-gram | Epoch 2 | Batch 162500 | Loss: 0.3133\n",
            "Skip-gram | Epoch 2 | Batch 163000 | Loss: 0.3106\n",
            "Skip-gram | Epoch 2 | Batch 163500 | Loss: 0.3040\n",
            "Skip-gram | Epoch 2 | Batch 164000 | Loss: 0.3100\n",
            "Skip-gram | Epoch 2 | Batch 164500 | Loss: 0.3124\n",
            "Skip-gram | Epoch 2 | Batch 165000 | Loss: 0.3093\n",
            "Skip-gram | Epoch 2 | Batch 165500 | Loss: 0.3085\n",
            "Skip-gram | Epoch 2 | Batch 166000 | Loss: 0.3023\n",
            "Skip-gram | Epoch 2 | Batch 166500 | Loss: 0.3084\n",
            "Skip-gram | Epoch 2 | Batch 167000 | Loss: 0.2942\n",
            "Skip-gram | Epoch 2 | Batch 167500 | Loss: 0.2940\n",
            "Skip-gram | Epoch 2 | Batch 168000 | Loss: 0.3161\n",
            "Skip-gram | Epoch 2 | Batch 168500 | Loss: 0.2975\n",
            "Skip-gram | Epoch 2 | Batch 169000 | Loss: 0.3084\n",
            "Skip-gram | Epoch 2 | Batch 169500 | Loss: 0.3035\n",
            "Skip-gram | Epoch 2 | Batch 170000 | Loss: 0.3036\n",
            "Skip-gram | Epoch 2 | Batch 170500 | Loss: 0.2902\n",
            "Skip-gram | Epoch 2 | Batch 171000 | Loss: 0.3134\n",
            "Skip-gram | Epoch 2 | Batch 171500 | Loss: 0.3036\n",
            "Skip-gram | Epoch 2 | Batch 172000 | Loss: 0.3150\n",
            "Skip-gram | Epoch 2 | Batch 172500 | Loss: 0.3209\n",
            "Skip-gram | Epoch 2 | Batch 173000 | Loss: 0.3053\n",
            "Skip-gram | Epoch 2 | Batch 173500 | Loss: 0.3100\n",
            "Skip-gram | Epoch 2 | Batch 174000 | Loss: 0.3037\n",
            "Skip-gram | Epoch 2 | Batch 174500 | Loss: 0.3090\n",
            "Skip-gram | Epoch 2 | Batch 175000 | Loss: 0.3022\n",
            "Skip-gram | Epoch 2 | Batch 175500 | Loss: 0.2919\n",
            "Skip-gram | Epoch 2 | Batch 176000 | Loss: 0.2865\n",
            "Skip-gram | Epoch 2 | Batch 176500 | Loss: 0.3354\n",
            "Skip-gram | Epoch 2 | Batch 177000 | Loss: 0.3087\n",
            "Skip-gram | Epoch 2 | Batch 177500 | Loss: 0.2916\n",
            "Skip-gram | Epoch 2 | Batch 178000 | Loss: 0.2891\n",
            "Skip-gram | Epoch 2 | Batch 178500 | Loss: 0.3146\n",
            "Skip-gram | Epoch 2 | Batch 179000 | Loss: 0.3211\n",
            "Skip-gram | Epoch 2 | Batch 179500 | Loss: 0.3146\n",
            "Skip-gram | Epoch 2 | Batch 180000 | Loss: 0.2951\n",
            "Skip-gram | Epoch 2 | Batch 180500 | Loss: 0.2892\n",
            "Skip-gram | Epoch 2 | Batch 181000 | Loss: 0.3148\n",
            "Skip-gram | Epoch 2 | Batch 181500 | Loss: 0.2918\n",
            "Skip-gram | Epoch 2 | Batch 182000 | Loss: 0.2872\n",
            "Skip-gram | Epoch 2 | Batch 182500 | Loss: 0.2944\n",
            "Skip-gram | Epoch 2 | Batch 183000 | Loss: 0.2814\n",
            "Skip-gram | Epoch 2 | Batch 183500 | Loss: 0.3313\n",
            "Skip-gram | Epoch 2 | Batch 184000 | Loss: 0.3103\n",
            "Skip-gram | Epoch 2 | Batch 184500 | Loss: 0.2890\n",
            "Skip-gram | Epoch 2 | Batch 185000 | Loss: 0.2913\n",
            "Skip-gram | Epoch 2 | Batch 185500 | Loss: 0.3280\n",
            "Skip-gram | Epoch 2 | Batch 186000 | Loss: 0.3108\n",
            "Skip-gram | Epoch 2 | Batch 186500 | Loss: 0.2907\n",
            "Skip-gram | Epoch 2 | Batch 187000 | Loss: 0.3077\n",
            "Skip-gram | Epoch 2 | Batch 187500 | Loss: 0.3269\n",
            "Skip-gram | Epoch 2 | Batch 188000 | Loss: 0.3103\n",
            "Skip-gram | Epoch 2 | Batch 188500 | Loss: 0.3185\n",
            "Skip-gram | Epoch 2 | Batch 189000 | Loss: 0.2928\n",
            "Skip-gram | Epoch 2 | Batch 189500 | Loss: 0.3316\n",
            "Skip-gram | Epoch 2 | Batch 190000 | Loss: 0.3144\n",
            "Skip-gram | Epoch 2 | Batch 190500 | Loss: 0.3239\n",
            "Skip-gram | Epoch 2 | Batch 191000 | Loss: 0.3018\n",
            "Skip-gram | Epoch 2 | Batch 191500 | Loss: 0.2926\n",
            "Skip-gram | Epoch 2 | Batch 192000 | Loss: 0.3188\n",
            "Skip-gram | Epoch 2 | Batch 192500 | Loss: 0.2968\n",
            "Skip-gram | Epoch 2 | Batch 193000 | Loss: 0.3149\n",
            "Skip-gram | Epoch 2 | Batch 193500 | Loss: 0.2960\n",
            "Skip-gram | Epoch 2 | Batch 194000 | Loss: 0.2980\n",
            "Skip-gram | Epoch 2 | Batch 194500 | Loss: 0.2969\n",
            "Skip-gram | Epoch 2 | Batch 195000 | Loss: 0.3163\n",
            "Skip-gram | Epoch 2 | Batch 195500 | Loss: 0.3005\n",
            "Skip-gram | Epoch 2 | Batch 196000 | Loss: 0.3073\n",
            "Skip-gram | Epoch 2 | Batch 196500 | Loss: 0.3069\n",
            "Skip-gram | Epoch 2 | Batch 197000 | Loss: 0.2972\n",
            "Skip-gram | Epoch 2 | Batch 197500 | Loss: 0.2951\n",
            "Skip-gram | Epoch 2 | Batch 198000 | Loss: 0.3025\n",
            "Skip-gram | Epoch 2 | Batch 198500 | Loss: 0.2883\n",
            "Skip-gram | Epoch 2 | Batch 199000 | Loss: 0.2865\n",
            "Skip-gram | Epoch 2 | Batch 199500 | Loss: 0.2761\n",
            "Skip-gram | Epoch 2 | Batch 200000 | Loss: 0.3091\n",
            "Skip-gram | Epoch 2 | Batch 200500 | Loss: 0.2717\n",
            "Skip-gram | Epoch 2 | Batch 201000 | Loss: 0.3227\n",
            "Skip-gram | Epoch 2 | Batch 201500 | Loss: 0.3031\n",
            "Skip-gram | Epoch 2 | Batch 202000 | Loss: 0.3099\n",
            "Skip-gram | Epoch 2 | Batch 202500 | Loss: 0.3214\n",
            "Skip-gram | Epoch 2 | Batch 203000 | Loss: 0.3127\n",
            "Skip-gram | Epoch 2 | Batch 203500 | Loss: 0.3107\n",
            "Skip-gram | Epoch 2 | Batch 204000 | Loss: 0.3223\n",
            "Skip-gram | Epoch 2 | Batch 204500 | Loss: 0.3214\n",
            "Skip-gram | Epoch 2 | Batch 205000 | Loss: 0.2928\n",
            "Skip-gram | Epoch 2 | Batch 205500 | Loss: 0.3115\n",
            "Skip-gram | Epoch 2 | Batch 206000 | Loss: 0.3279\n",
            "Skip-gram | Epoch 2 | Batch 206500 | Loss: 0.3353\n",
            "Skip-gram | Epoch 2 | Batch 207000 | Loss: 0.3102\n",
            "Skip-gram | Epoch 2 | Batch 207500 | Loss: 0.3077\n",
            "Skip-gram | Epoch 2 | Batch 208000 | Loss: 0.3263\n",
            "Skip-gram | Epoch 2 | Batch 208500 | Loss: 0.3109\n",
            "Skip-gram | Epoch 2 | Batch 209000 | Loss: 0.3070\n",
            "Skip-gram | Epoch 2 | Batch 209500 | Loss: 0.3156\n",
            "Skip-gram | Epoch 2 | Batch 210000 | Loss: 0.3110\n",
            "Skip-gram | Epoch 2 | Batch 210500 | Loss: 0.3018\n",
            "Skip-gram | Epoch 2 | Batch 211000 | Loss: 0.2669\n",
            "Skip-gram | Epoch 2 | Batch 211500 | Loss: 0.2375\n",
            "Skip-gram | Epoch 2 | Batch 212000 | Loss: 0.2085\n",
            "Skip-gram | Epoch 2 | Batch 212500 | Loss: 0.2437\n",
            "Skip-gram | Epoch 2 | Batch 213000 | Loss: 0.2822\n",
            "Skip-gram | Epoch 2 | Batch 213500 | Loss: 0.3441\n",
            "Skip-gram | Epoch 2 | Batch 214000 | Loss: 0.3222\n",
            "Skip-gram | Epoch 2 | Batch 214500 | Loss: 0.3118\n",
            "Skip-gram | Epoch 2 | Batch 215000 | Loss: 0.2963\n",
            "Skip-gram | Epoch 2 | Batch 215500 | Loss: 0.2827\n",
            "Skip-gram | Epoch 2 | Batch 216000 | Loss: 0.3243\n",
            "Skip-gram | Epoch 2 | Batch 216500 | Loss: 0.3000\n",
            "Skip-gram | Epoch 2 | Batch 217000 | Loss: 0.3094\n",
            "Skip-gram | Epoch 2 | Batch 217500 | Loss: 0.2848\n",
            "Skip-gram | Epoch 2 | Batch 218000 | Loss: 0.3405\n",
            "Skip-gram | Epoch 2 | Batch 218500 | Loss: 0.3053\n",
            "Skip-gram | Epoch 2 | Batch 219000 | Loss: 0.3045\n",
            "Skip-gram | Epoch 2 | Batch 219500 | Loss: 0.3197\n",
            "Skip-gram | Epoch 2 | Batch 220000 | Loss: 0.3055\n",
            "Skip-gram | Epoch 2 | Batch 220500 | Loss: 0.3190\n",
            "Skip-gram | Epoch 2 | Batch 221000 | Loss: 0.3111\n",
            "Skip-gram | Epoch 2 | Batch 221500 | Loss: 0.3046\n",
            "Skip-gram | Epoch 2 | Batch 222000 | Loss: 0.3117\n",
            "Skip-gram | Epoch 2 | Batch 222500 | Loss: 0.2853\n",
            "Skip-gram | Epoch 2 | Batch 223000 | Loss: 0.3186\n",
            "Skip-gram | Epoch 2 | Batch 223500 | Loss: 0.3076\n",
            "Skip-gram | Epoch 2 | Batch 224000 | Loss: 0.3163\n",
            "Skip-gram | Epoch 2 | Batch 224500 | Loss: 0.3034\n",
            "Skip-gram | Epoch 2 | Batch 225000 | Loss: 0.3119\n",
            "Skip-gram | Epoch 2 | Batch 225500 | Loss: 0.3016\n",
            "Skip-gram | Epoch 2 | Batch 226000 | Loss: 0.2964\n",
            "Skip-gram | Epoch 2 | Batch 226500 | Loss: 0.2976\n",
            "Skip-gram | Epoch 2 | Batch 227000 | Loss: 0.3259\n",
            "Skip-gram | Epoch 2 | Batch 227500 | Loss: 0.3093\n",
            "Skip-gram | Epoch 2 | Batch 228000 | Loss: 0.3153\n",
            "Skip-gram | Epoch 2 | Batch 228500 | Loss: 0.3101\n",
            "Skip-gram | Epoch 2 | Batch 229000 | Loss: 0.2872\n",
            "Skip-gram | Epoch 2 | Batch 229500 | Loss: 0.3076\n",
            "Skip-gram | Epoch 2 | Batch 230000 | Loss: 0.3015\n",
            "Skip-gram | Epoch 2 | Batch 230500 | Loss: 0.3136\n",
            "Skip-gram | Epoch 2 | Batch 231000 | Loss: 0.3134\n",
            "Skip-gram | Epoch 2 | Batch 231500 | Loss: 0.2941\n",
            "Skip-gram | Epoch 2 | Batch 232000 | Loss: 0.2722\n",
            "Skip-gram | Epoch 2 | Batch 232500 | Loss: 0.2487\n",
            "Skip-gram | Epoch 2 | Batch 233000 | Loss: 0.3367\n",
            "Skip-gram | Epoch 2 | Batch 233500 | Loss: 0.2990\n",
            "Skip-gram | Epoch 2 | Batch 234000 | Loss: 0.3096\n",
            "Skip-gram | Epoch 2 | Batch 234500 | Loss: 0.3047\n",
            "Skip-gram | Epoch 2 | Batch 235000 | Loss: 0.3104\n",
            "Skip-gram | Epoch 2 | Batch 235500 | Loss: 0.3240\n",
            "Skip-gram | Epoch 2 | Batch 236000 | Loss: 0.2970\n",
            "Skip-gram | Epoch 2 | Batch 236500 | Loss: 0.3291\n",
            "Skip-gram | Epoch 2 | Batch 237000 | Loss: 0.3077\n",
            "Skip-gram | Epoch 2 | Batch 237500 | Loss: 0.3061\n",
            "Skip-gram | Epoch 2 | Batch 238000 | Loss: 0.3092\n",
            "Skip-gram | Epoch 2 | Batch 238500 | Loss: 0.2873\n",
            "Skip-gram | Epoch 2 | Batch 239000 | Loss: 0.2994\n",
            "Skip-gram | Epoch 2 | Batch 239500 | Loss: 0.3025\n",
            "Skip-gram | Epoch 2 | Batch 240000 | Loss: 0.3160\n",
            "Skip-gram | Epoch 2 | Batch 240500 | Loss: 0.3111\n",
            "Skip-gram | Epoch 2 | Batch 241000 | Loss: 0.3293\n",
            "Skip-gram | Epoch 2 | Batch 241500 | Loss: 0.3211\n",
            "Skip-gram | Epoch 2 | Batch 242000 | Loss: 0.2732\n",
            "Skip-gram | Epoch 2 | Batch 242500 | Loss: 0.2953\n",
            "Skip-gram | Epoch 2 | Batch 243000 | Loss: 0.2924\n",
            "Skip-gram | Epoch 2 | Batch 243500 | Loss: 0.3047\n",
            "Skip-gram | Epoch 2 | Batch 244000 | Loss: 0.3286\n",
            "Skip-gram | Epoch 2 | Batch 244500 | Loss: 0.3305\n",
            "Skip-gram | Epoch 2 | Batch 245000 | Loss: 0.3459\n",
            "Skip-gram | Epoch 2 | Batch 245500 | Loss: 0.3210\n",
            "Skip-gram | Epoch 2 | Batch 246000 | Loss: 0.3187\n",
            "Skip-gram | Epoch 2 | Batch 246500 | Loss: 0.3063\n",
            "Skip-gram | Epoch 2 | Batch 247000 | Loss: 0.3083\n",
            "Skip-gram | Epoch 2 | Batch 247500 | Loss: 0.2999\n",
            "Skip-gram | Epoch 2 | Batch 248000 | Loss: 0.2863\n",
            "Skip-gram | Epoch 2 | Batch 248500 | Loss: 0.3094\n",
            "Skip-gram | Epoch 2 | Batch 249000 | Loss: 0.3211\n",
            "Skip-gram | Epoch 2 | Batch 249500 | Loss: 0.3096\n",
            "Skip-gram | Epoch 2 | Batch 250000 | Loss: 0.3092\n",
            "Skip-gram | Epoch 2 | Batch 250500 | Loss: 0.3295\n",
            "Skip-gram | Epoch 2 | Batch 251000 | Loss: 0.3227\n",
            "Skip-gram | Epoch 2 | Batch 251500 | Loss: 0.2985\n",
            "Skip-gram | Epoch 2 | Batch 252000 | Loss: 0.3195\n",
            "Skip-gram | Epoch 2 | Batch 252500 | Loss: 0.3044\n",
            "Skip-gram | Epoch 2 | Batch 253000 | Loss: 0.2768\n",
            "Skip-gram | Epoch 2 | Batch 253500 | Loss: 0.2986\n",
            "Skip-gram | Epoch 2 | Batch 254000 | Loss: 0.3053\n",
            "Skip-gram | Epoch 2 | Batch 254500 | Loss: 0.3313\n",
            "Skip-gram | Epoch 2 | Batch 255000 | Loss: 0.2826\n",
            "Skip-gram | Epoch 2 | Batch 255500 | Loss: 0.3183\n",
            "Skip-gram | Epoch 2 | Batch 256000 | Loss: 0.3152\n",
            "Skip-gram | Epoch 2 | Batch 256500 | Loss: 0.3053\n",
            "Skip-gram | Epoch 2 | Batch 257000 | Loss: 0.3022\n",
            "Skip-gram | Epoch 2 | Batch 257500 | Loss: 0.3071\n",
            "Skip-gram | Epoch 2 | Batch 258000 | Loss: 0.3005\n",
            "Skip-gram | Epoch 2 | Batch 258500 | Loss: 0.2488\n",
            "Skip-gram | Epoch 2 | Batch 259000 | Loss: 0.2735\n",
            "Skip-gram | Epoch 2 | Batch 259500 | Loss: 0.3225\n",
            "Skip-gram | Epoch 2 | Batch 260000 | Loss: 0.3326\n",
            "Skip-gram | Epoch 2 | Batch 260500 | Loss: 0.3167\n",
            "Skip-gram | Epoch 2 | Batch 261000 | Loss: 0.3319\n",
            "Skip-gram | Epoch 2 | Batch 261500 | Loss: 0.2957\n",
            "Skip-gram | Epoch 2 | Batch 262000 | Loss: 0.3221\n",
            "Skip-gram | Epoch 2 | Batch 262500 | Loss: 0.3232\n",
            "Skip-gram | Epoch 2 | Batch 263000 | Loss: 0.3184\n",
            "Skip-gram | Epoch 2 | Batch 263500 | Loss: 0.3317\n",
            "Skip-gram | Epoch 2 | Batch 264000 | Loss: 0.3217\n",
            "Skip-gram | Epoch 2 | Batch 264500 | Loss: 0.3146\n",
            "Skip-gram | Epoch 2 | Batch 265000 | Loss: 0.3080\n",
            "Skip-gram | Epoch 2 | Batch 265500 | Loss: 0.3284\n",
            "Skip-gram | Epoch 2 | Batch 266000 | Loss: 0.3093\n",
            "Skip-gram | Epoch 2 | Batch 266500 | Loss: 0.3095\n",
            "Skip-gram | Epoch 2 | Batch 267000 | Loss: 0.2936\n",
            "Skip-gram | Epoch 2 | Batch 267500 | Loss: 0.3073\n",
            "Skip-gram | Epoch 2 | Batch 268000 | Loss: 0.3333\n",
            "Skip-gram | Epoch 2 | Batch 268500 | Loss: 0.3056\n",
            "Skip-gram | Epoch 2 | Batch 269000 | Loss: 0.3178\n",
            "Skip-gram | Epoch 2 | Batch 269500 | Loss: 0.2774\n",
            "Skip-gram | Epoch 2 | Batch 270000 | Loss: 0.3072\n",
            "Skip-gram | Epoch 2 | Batch 270500 | Loss: 0.3419\n",
            "Skip-gram | Epoch 2 | Batch 271000 | Loss: 0.3239\n",
            "Skip-gram | Epoch 2 | Batch 271500 | Loss: 0.3156\n",
            "Skip-gram | Epoch 2 | Batch 272000 | Loss: 0.3008\n",
            "Skip-gram | Epoch 2 | Batch 272500 | Loss: 0.3057\n",
            "Skip-gram | Epoch 2 | Batch 273000 | Loss: 0.2937\n",
            "Skip-gram | Epoch 2 | Batch 273500 | Loss: 0.1841\n",
            "Skip-gram | Epoch 2 | Batch 274000 | Loss: 0.3182\n",
            "Skip-gram | Epoch 2 | Batch 274500 | Loss: 0.3222\n",
            "Skip-gram | Epoch 2 | Batch 275000 | Loss: 0.3177\n",
            "Skip-gram | Epoch 2 | Batch 275500 | Loss: 0.3106\n",
            "Skip-gram | Epoch 2 | Batch 276000 | Loss: 0.2743\n",
            "Skip-gram | Epoch 2 | Batch 276500 | Loss: 0.2727\n",
            "Skip-gram | Epoch 2 | Batch 277000 | Loss: 0.3479\n",
            "Skip-gram | Epoch 2 | Batch 277500 | Loss: 0.3392\n",
            "Skip-gram | Epoch 2 | Batch 278000 | Loss: 0.3041\n",
            "Skip-gram | Epoch 2 | Batch 278500 | Loss: 0.3132\n",
            "Skip-gram | Epoch 2 | Batch 279000 | Loss: 0.3190\n",
            "Skip-gram | Epoch 2 | Batch 279500 | Loss: 0.3079\n",
            "Skip-gram | Epoch 2 | Batch 280000 | Loss: 0.3001\n",
            "Skip-gram | Epoch 2 | Batch 280500 | Loss: 0.2982\n",
            "Skip-gram | Epoch 2 | Batch 281000 | Loss: 0.3119\n",
            "Skip-gram | Epoch 2 | Batch 281500 | Loss: 0.3153\n",
            "Skip-gram | Epoch 2 | Batch 282000 | Loss: 0.3344\n",
            "Skip-gram | Epoch 2 | Batch 282500 | Loss: 0.2921\n",
            "Skip-gram | Epoch 2 | Batch 283000 | Loss: 0.3116\n",
            "Skip-gram | Epoch 2 | Batch 283500 | Loss: 0.1950\n",
            "Skip-gram | Epoch 2 | Batch 284000 | Loss: 0.2100\n",
            "Skip-gram | Epoch 2 | Batch 284500 | Loss: 0.3343\n",
            "Skip-gram | Epoch 2 | Batch 285000 | Loss: 0.3043\n",
            "Skip-gram | Epoch 2 | Batch 285500 | Loss: 0.3122\n",
            "Skip-gram | Epoch 2 | Batch 286000 | Loss: 0.3171\n",
            "Skip-gram | Epoch 2 | Batch 286500 | Loss: 0.3212\n",
            "Skip-gram | Epoch 2 | Batch 287000 | Loss: 0.3216\n",
            "Skip-gram | Epoch 2 | Batch 287500 | Loss: 0.3139\n",
            "Skip-gram | Epoch 2 | Batch 288000 | Loss: 0.3265\n",
            "Skip-gram | Epoch 2 | Batch 288500 | Loss: 0.2325\n",
            "Skip-gram | Epoch 2 | Batch 289000 | Loss: 0.2988\n",
            "Skip-gram | Epoch 2 | Batch 289500 | Loss: 0.3299\n",
            "Skip-gram | Epoch 2 | Batch 290000 | Loss: 0.3243\n",
            "Skip-gram | Epoch 2 | Batch 290500 | Loss: 0.2823\n",
            "Skip-gram | Epoch 2 | Batch 291000 | Loss: 0.2978\n",
            "Skip-gram | Epoch 2 | Batch 291500 | Loss: 0.2789\n",
            "Skip-gram | Epoch 2 | Batch 292000 | Loss: 0.3037\n",
            "Skip-gram | Epoch 2 | Batch 292500 | Loss: 0.3043\n",
            "Skip-gram | Epoch 2 | Batch 293000 | Loss: 0.3290\n",
            "Skip-gram | Epoch 2 | Batch 293500 | Loss: 0.3124\n",
            "Skip-gram | Epoch 2 | Batch 294000 | Loss: 0.3219\n",
            "Skip-gram | Epoch 2 | Batch 294500 | Loss: 0.3129\n",
            "Skip-gram | Epoch 2 | Batch 295000 | Loss: 0.3137\n",
            "Skip-gram | Epoch 2 | Batch 295500 | Loss: 0.3194\n",
            "Skip-gram | Epoch 2 | Batch 296000 | Loss: 0.3184\n",
            "Skip-gram | Epoch 2 | Batch 296500 | Loss: 0.3158\n",
            "Skip-gram | Epoch 2 | Batch 297000 | Loss: 0.3094\n",
            "Skip-gram | Epoch 2 | Batch 297500 | Loss: 0.3281\n",
            "Skip-gram | Epoch 2 | Batch 298000 | Loss: 0.2566\n",
            "Skip-gram | Epoch 2 | Batch 298500 | Loss: 0.3370\n",
            "Skip-gram | Epoch 2 | Batch 299000 | Loss: 0.3277\n",
            "Skip-gram | Epoch 2 | Batch 299500 | Loss: 0.3128\n",
            "Skip-gram | Epoch 2 | Batch 300000 | Loss: 0.3101\n",
            "Skip-gram | Epoch 2 | Batch 300500 | Loss: 0.3168\n",
            "Skip-gram | Epoch 2 | Batch 301000 | Loss: 0.2941\n",
            "Skip-gram | Epoch 2 | Batch 301500 | Loss: 0.3161\n",
            "Skip-gram | Epoch 2 | Batch 302000 | Loss: 0.3157\n",
            "Skip-gram | Epoch 2 | Batch 302500 | Loss: 0.3180\n",
            "Skip-gram | Epoch 2 | Batch 303000 | Loss: 0.3042\n",
            "Skip-gram | Epoch 2 | Batch 303500 | Loss: 0.2961\n",
            "Skip-gram | Epoch 2 | Batch 304000 | Loss: 0.3179\n",
            "Skip-gram | Epoch 2 | Batch 304500 | Loss: 0.3457\n",
            "Skip-gram | Epoch 2 | Batch 305000 | Loss: 0.3236\n",
            "Skip-gram | Epoch 2 | Batch 305500 | Loss: 0.3021\n",
            "Skip-gram | Epoch 2 | Batch 306000 | Loss: 0.3166\n",
            "Skip-gram | Epoch 2 | Batch 306500 | Loss: 0.3138\n",
            "Skip-gram | Epoch 2 | Batch 307000 | Loss: 0.2914\n",
            "Skip-gram | Epoch 2 | Batch 307500 | Loss: 0.3267\n",
            "Skip-gram | Epoch 2 | Batch 308000 | Loss: 0.2891\n",
            "Skip-gram | Epoch 2 | Batch 308500 | Loss: 0.3391\n",
            "Skip-gram | Epoch 2 | Batch 309000 | Loss: 0.3115\n",
            "Skip-gram | Epoch 2 | Batch 309500 | Loss: 0.3236\n",
            "Skip-gram | Epoch 2 | Batch 310000 | Loss: 0.2969\n",
            "Skip-gram | Epoch 2 | Batch 310500 | Loss: 0.3244\n",
            "Skip-gram | Epoch 2 | Batch 311000 | Loss: 0.3350\n",
            "Skip-gram | Epoch 2 | Batch 311500 | Loss: 0.3188\n",
            "Skip-gram | Epoch 2 | Batch 312000 | Loss: 0.3237\n",
            "Skip-gram | Epoch 2 | Batch 312500 | Loss: 0.3125\n",
            "Skip-gram | Epoch 2 | Batch 313000 | Loss: 0.3294\n",
            "Skip-gram | Epoch 2 | Batch 313500 | Loss: 0.3217\n",
            "Skip-gram | Epoch 2 | Batch 314000 | Loss: 0.3288\n",
            "Skip-gram | Epoch 2 | Batch 314500 | Loss: 0.2994\n",
            "Skip-gram | Epoch 2 | Batch 315000 | Loss: 0.3020\n",
            "Skip-gram | Epoch 2 | Batch 315500 | Loss: 0.3192\n",
            "Skip-gram | Epoch 2 | Batch 316000 | Loss: 0.3178\n",
            "Skip-gram | Epoch 2 | Batch 316500 | Loss: 0.3377\n",
            "Skip-gram | Epoch 2 | Batch 317000 | Loss: 0.2765\n",
            "Skip-gram | Epoch 2 | Batch 317500 | Loss: 0.2735\n",
            "Skip-gram | Epoch 2 | Batch 318000 | Loss: 0.3173\n",
            "Skip-gram | Epoch 2 | Batch 318500 | Loss: 0.2938\n",
            "Skip-gram | Epoch 2 | Batch 319000 | Loss: 0.3085\n",
            "Skip-gram | Epoch 2 | Batch 319500 | Loss: 0.3098\n",
            "Skip-gram | Epoch 2 | Batch 320000 | Loss: 0.3165\n",
            "Skip-gram | Epoch 2 | Batch 320500 | Loss: 0.3505\n",
            "Skip-gram | Epoch 2 | Batch 321000 | Loss: 0.3198\n",
            "Skip-gram | Epoch 2 | Batch 321500 | Loss: 0.3186\n",
            "Skip-gram | Epoch 2 | Batch 322000 | Loss: 0.3221\n",
            "Skip-gram | Epoch 2 | Batch 322500 | Loss: 0.3359\n",
            "Skip-gram | Epoch 2 | Batch 323000 | Loss: 0.2732\n",
            "Skip-gram | Epoch 2 | Batch 323500 | Loss: 0.3332\n",
            "Skip-gram Epoch 2. Средний Loss: 0.3033\n",
            "Skip-gram | Epoch 3 | Batch 500 | Loss: 0.3316\n",
            "Skip-gram | Epoch 3 | Batch 1000 | Loss: 0.2937\n",
            "Skip-gram | Epoch 3 | Batch 1500 | Loss: 0.3000\n",
            "Skip-gram | Epoch 3 | Batch 2000 | Loss: 0.3097\n",
            "Skip-gram | Epoch 3 | Batch 2500 | Loss: 0.2927\n",
            "Skip-gram | Epoch 3 | Batch 3000 | Loss: 0.2971\n",
            "Skip-gram | Epoch 3 | Batch 3500 | Loss: 0.3217\n",
            "Skip-gram | Epoch 3 | Batch 4000 | Loss: 0.3118\n",
            "Skip-gram | Epoch 3 | Batch 4500 | Loss: 0.3127\n",
            "Skip-gram | Epoch 3 | Batch 5000 | Loss: 0.3158\n",
            "Skip-gram | Epoch 3 | Batch 5500 | Loss: 0.3506\n",
            "Skip-gram | Epoch 3 | Batch 6000 | Loss: 0.3039\n",
            "Skip-gram | Epoch 3 | Batch 6500 | Loss: 0.3029\n",
            "Skip-gram | Epoch 3 | Batch 7000 | Loss: 0.2966\n",
            "Skip-gram | Epoch 3 | Batch 7500 | Loss: 0.2972\n",
            "Skip-gram | Epoch 3 | Batch 8000 | Loss: 0.2887\n",
            "Skip-gram | Epoch 3 | Batch 8500 | Loss: 0.3140\n",
            "Skip-gram | Epoch 3 | Batch 9000 | Loss: 0.3047\n",
            "Skip-gram | Epoch 3 | Batch 9500 | Loss: 0.3199\n",
            "Skip-gram | Epoch 3 | Batch 10000 | Loss: 0.3031\n",
            "Skip-gram | Epoch 3 | Batch 10500 | Loss: 0.3202\n",
            "Skip-gram | Epoch 3 | Batch 11000 | Loss: 0.3210\n",
            "Skip-gram | Epoch 3 | Batch 11500 | Loss: 0.3310\n",
            "Skip-gram | Epoch 3 | Batch 12000 | Loss: 0.3349\n",
            "Skip-gram | Epoch 3 | Batch 12500 | Loss: 0.3043\n",
            "Skip-gram | Epoch 3 | Batch 13000 | Loss: 0.3266\n",
            "Skip-gram | Epoch 3 | Batch 13500 | Loss: 0.3219\n",
            "Skip-gram | Epoch 3 | Batch 14000 | Loss: 0.3279\n",
            "Skip-gram | Epoch 3 | Batch 14500 | Loss: 0.3097\n",
            "Skip-gram | Epoch 3 | Batch 15000 | Loss: 0.3079\n",
            "Skip-gram | Epoch 3 | Batch 15500 | Loss: 0.2812\n",
            "Skip-gram | Epoch 3 | Batch 16000 | Loss: 0.3012\n",
            "Skip-gram | Epoch 3 | Batch 16500 | Loss: 0.3351\n",
            "Skip-gram | Epoch 3 | Batch 17000 | Loss: 0.3007\n",
            "Skip-gram | Epoch 3 | Batch 17500 | Loss: 0.2679\n",
            "Skip-gram | Epoch 3 | Batch 18000 | Loss: 0.3125\n",
            "Skip-gram | Epoch 3 | Batch 18500 | Loss: 0.3419\n",
            "Skip-gram | Epoch 3 | Batch 19000 | Loss: 0.3190\n",
            "Skip-gram | Epoch 3 | Batch 19500 | Loss: 0.3117\n",
            "Skip-gram | Epoch 3 | Batch 20000 | Loss: 0.2985\n",
            "Skip-gram | Epoch 3 | Batch 20500 | Loss: 0.2967\n",
            "Skip-gram | Epoch 3 | Batch 21000 | Loss: 0.3210\n",
            "Skip-gram | Epoch 3 | Batch 21500 | Loss: 0.3102\n",
            "Skip-gram | Epoch 3 | Batch 22000 | Loss: 0.3365\n",
            "Skip-gram | Epoch 3 | Batch 22500 | Loss: 0.3135\n",
            "Skip-gram | Epoch 3 | Batch 23000 | Loss: 0.3191\n",
            "Skip-gram | Epoch 3 | Batch 23500 | Loss: 0.3245\n",
            "Skip-gram | Epoch 3 | Batch 24000 | Loss: 0.3143\n",
            "Skip-gram | Epoch 3 | Batch 24500 | Loss: 0.3068\n",
            "Skip-gram | Epoch 3 | Batch 25000 | Loss: 0.3131\n",
            "Skip-gram | Epoch 3 | Batch 25500 | Loss: 0.3217\n",
            "Skip-gram | Epoch 3 | Batch 26000 | Loss: 0.3208\n",
            "Skip-gram | Epoch 3 | Batch 26500 | Loss: 0.2840\n",
            "Skip-gram | Epoch 3 | Batch 27000 | Loss: 0.2878\n",
            "Skip-gram | Epoch 3 | Batch 27500 | Loss: 0.3131\n",
            "Skip-gram | Epoch 3 | Batch 28000 | Loss: 0.3208\n",
            "Skip-gram | Epoch 3 | Batch 28500 | Loss: 0.3285\n",
            "Skip-gram | Epoch 3 | Batch 29000 | Loss: 0.3257\n",
            "Skip-gram | Epoch 3 | Batch 29500 | Loss: 0.3361\n",
            "Skip-gram | Epoch 3 | Batch 30000 | Loss: 0.2947\n",
            "Skip-gram | Epoch 3 | Batch 30500 | Loss: 0.3328\n",
            "Skip-gram | Epoch 3 | Batch 31000 | Loss: 0.3077\n",
            "Skip-gram | Epoch 3 | Batch 31500 | Loss: 0.3599\n",
            "Skip-gram | Epoch 3 | Batch 32000 | Loss: 0.2952\n",
            "Skip-gram | Epoch 3 | Batch 32500 | Loss: 0.2772\n",
            "Skip-gram | Epoch 3 | Batch 33000 | Loss: 0.3391\n",
            "Skip-gram | Epoch 3 | Batch 33500 | Loss: 0.3077\n",
            "Skip-gram | Epoch 3 | Batch 34000 | Loss: 0.3096\n",
            "Skip-gram | Epoch 3 | Batch 34500 | Loss: 0.2708\n",
            "Skip-gram | Epoch 3 | Batch 35000 | Loss: 0.1826\n",
            "Skip-gram | Epoch 3 | Batch 35500 | Loss: 0.1462\n",
            "Skip-gram | Epoch 3 | Batch 36000 | Loss: 0.1612\n",
            "Skip-gram | Epoch 3 | Batch 36500 | Loss: 0.3279\n",
            "Skip-gram | Epoch 3 | Batch 37000 | Loss: 0.3038\n",
            "Skip-gram | Epoch 3 | Batch 37500 | Loss: 0.3514\n",
            "Skip-gram | Epoch 3 | Batch 38000 | Loss: 0.2976\n",
            "Skip-gram | Epoch 3 | Batch 38500 | Loss: 0.3194\n",
            "Skip-gram | Epoch 3 | Batch 39000 | Loss: 0.3328\n",
            "Skip-gram | Epoch 3 | Batch 39500 | Loss: 0.3291\n",
            "Skip-gram | Epoch 3 | Batch 40000 | Loss: 0.2890\n",
            "Skip-gram | Epoch 3 | Batch 40500 | Loss: 0.3176\n",
            "Skip-gram | Epoch 3 | Batch 41000 | Loss: 0.3149\n",
            "Skip-gram | Epoch 3 | Batch 41500 | Loss: 0.3021\n",
            "Skip-gram | Epoch 3 | Batch 42000 | Loss: 0.3405\n",
            "Skip-gram | Epoch 3 | Batch 42500 | Loss: 0.2941\n",
            "Skip-gram | Epoch 3 | Batch 43000 | Loss: 0.3161\n",
            "Skip-gram | Epoch 3 | Batch 43500 | Loss: 0.3363\n",
            "Skip-gram | Epoch 3 | Batch 44000 | Loss: 0.3363\n",
            "Skip-gram | Epoch 3 | Batch 44500 | Loss: 0.3259\n",
            "Skip-gram | Epoch 3 | Batch 45000 | Loss: 0.3438\n",
            "Skip-gram | Epoch 3 | Batch 45500 | Loss: 0.3677\n",
            "Skip-gram | Epoch 3 | Batch 46000 | Loss: 0.2951\n",
            "Skip-gram | Epoch 3 | Batch 46500 | Loss: 0.3270\n",
            "Skip-gram | Epoch 3 | Batch 47000 | Loss: 0.3014\n",
            "Skip-gram | Epoch 3 | Batch 47500 | Loss: 0.3243\n",
            "Skip-gram | Epoch 3 | Batch 48000 | Loss: 0.3265\n",
            "Skip-gram | Epoch 3 | Batch 48500 | Loss: 0.3237\n",
            "Skip-gram | Epoch 3 | Batch 49000 | Loss: 0.3010\n",
            "Skip-gram | Epoch 3 | Batch 49500 | Loss: 0.3044\n",
            "Skip-gram | Epoch 3 | Batch 50000 | Loss: 0.3256\n",
            "Skip-gram | Epoch 3 | Batch 50500 | Loss: 0.3247\n",
            "Skip-gram | Epoch 3 | Batch 51000 | Loss: 0.3159\n",
            "Skip-gram | Epoch 3 | Batch 51500 | Loss: 0.3301\n",
            "Skip-gram | Epoch 3 | Batch 52000 | Loss: 0.3235\n",
            "Skip-gram | Epoch 3 | Batch 52500 | Loss: 0.3089\n",
            "Skip-gram | Epoch 3 | Batch 53000 | Loss: 0.3304\n",
            "Skip-gram | Epoch 3 | Batch 53500 | Loss: 0.3273\n",
            "Skip-gram | Epoch 3 | Batch 54000 | Loss: 0.3160\n",
            "Skip-gram | Epoch 3 | Batch 54500 | Loss: 0.3242\n",
            "Skip-gram | Epoch 3 | Batch 55000 | Loss: 0.3221\n",
            "Skip-gram | Epoch 3 | Batch 55500 | Loss: 0.3337\n",
            "Skip-gram | Epoch 3 | Batch 56000 | Loss: 0.2205\n",
            "Skip-gram | Epoch 3 | Batch 56500 | Loss: 0.2587\n",
            "Skip-gram | Epoch 3 | Batch 57000 | Loss: 0.3278\n",
            "Skip-gram | Epoch 3 | Batch 57500 | Loss: 0.3282\n",
            "Skip-gram | Epoch 3 | Batch 58000 | Loss: 0.3120\n",
            "Skip-gram | Epoch 3 | Batch 58500 | Loss: 0.3344\n",
            "Skip-gram | Epoch 3 | Batch 59000 | Loss: 0.3237\n",
            "Skip-gram | Epoch 3 | Batch 59500 | Loss: 0.3178\n",
            "Skip-gram | Epoch 3 | Batch 60000 | Loss: 0.2879\n",
            "Skip-gram | Epoch 3 | Batch 60500 | Loss: 0.3122\n",
            "Skip-gram | Epoch 3 | Batch 61000 | Loss: 0.2969\n",
            "Skip-gram | Epoch 3 | Batch 61500 | Loss: 0.3035\n",
            "Skip-gram | Epoch 3 | Batch 62000 | Loss: 0.3386\n",
            "Skip-gram | Epoch 3 | Batch 62500 | Loss: 0.3409\n",
            "Skip-gram | Epoch 3 | Batch 63000 | Loss: 0.3271\n",
            "Skip-gram | Epoch 3 | Batch 63500 | Loss: 0.3136\n",
            "Skip-gram | Epoch 3 | Batch 64000 | Loss: 0.3332\n",
            "Skip-gram | Epoch 3 | Batch 64500 | Loss: 0.3278\n",
            "Skip-gram | Epoch 3 | Batch 65000 | Loss: 0.3437\n",
            "Skip-gram | Epoch 3 | Batch 65500 | Loss: 0.3205\n",
            "Skip-gram | Epoch 3 | Batch 66000 | Loss: 0.3153\n",
            "Skip-gram | Epoch 3 | Batch 66500 | Loss: 0.3159\n",
            "Skip-gram | Epoch 3 | Batch 67000 | Loss: 0.3213\n",
            "Skip-gram | Epoch 3 | Batch 67500 | Loss: 0.3237\n",
            "Skip-gram | Epoch 3 | Batch 68000 | Loss: 0.3046\n",
            "Skip-gram | Epoch 3 | Batch 68500 | Loss: 0.3227\n",
            "Skip-gram | Epoch 3 | Batch 69000 | Loss: 0.3551\n",
            "Skip-gram | Epoch 3 | Batch 69500 | Loss: 0.3272\n",
            "Skip-gram | Epoch 3 | Batch 70000 | Loss: 0.3103\n",
            "Skip-gram | Epoch 3 | Batch 70500 | Loss: 0.3101\n",
            "Skip-gram | Epoch 3 | Batch 71000 | Loss: 0.3296\n",
            "Skip-gram | Epoch 3 | Batch 71500 | Loss: 0.3274\n",
            "Skip-gram | Epoch 3 | Batch 72000 | Loss: 0.3278\n",
            "Skip-gram | Epoch 3 | Batch 72500 | Loss: 0.3072\n",
            "Skip-gram | Epoch 3 | Batch 73000 | Loss: 0.3494\n",
            "Skip-gram | Epoch 3 | Batch 73500 | Loss: 0.3103\n",
            "Skip-gram | Epoch 3 | Batch 74000 | Loss: 0.2894\n",
            "Skip-gram | Epoch 3 | Batch 74500 | Loss: 0.2441\n",
            "Skip-gram | Epoch 3 | Batch 75000 | Loss: 0.2590\n",
            "Skip-gram | Epoch 3 | Batch 75500 | Loss: 0.2691\n",
            "Skip-gram | Epoch 3 | Batch 76000 | Loss: 0.3249\n",
            "Skip-gram | Epoch 3 | Batch 76500 | Loss: 0.3218\n",
            "Skip-gram | Epoch 3 | Batch 77000 | Loss: 0.2879\n",
            "Skip-gram | Epoch 3 | Batch 77500 | Loss: 0.3301\n",
            "Skip-gram | Epoch 3 | Batch 78000 | Loss: 0.3332\n",
            "Skip-gram | Epoch 3 | Batch 78500 | Loss: 0.3061\n",
            "Skip-gram | Epoch 3 | Batch 79000 | Loss: 0.2993\n",
            "Skip-gram | Epoch 3 | Batch 79500 | Loss: 0.3339\n",
            "Skip-gram | Epoch 3 | Batch 80000 | Loss: 0.3022\n",
            "Skip-gram | Epoch 3 | Batch 80500 | Loss: 0.3105\n",
            "Skip-gram | Epoch 3 | Batch 81000 | Loss: 0.2989\n",
            "Skip-gram | Epoch 3 | Batch 81500 | Loss: 0.2887\n",
            "Skip-gram | Epoch 3 | Batch 82000 | Loss: 0.2905\n",
            "Skip-gram | Epoch 3 | Batch 82500 | Loss: 0.2783\n",
            "Skip-gram | Epoch 3 | Batch 83000 | Loss: 0.2894\n",
            "Skip-gram | Epoch 3 | Batch 83500 | Loss: 0.3347\n",
            "Skip-gram | Epoch 3 | Batch 84000 | Loss: 0.3033\n",
            "Skip-gram | Epoch 3 | Batch 84500 | Loss: 0.3164\n",
            "Skip-gram | Epoch 3 | Batch 85000 | Loss: 0.2634\n",
            "Skip-gram | Epoch 3 | Batch 85500 | Loss: 0.2820\n",
            "Skip-gram | Epoch 3 | Batch 86000 | Loss: 0.3543\n",
            "Skip-gram | Epoch 3 | Batch 86500 | Loss: 0.3363\n",
            "Skip-gram | Epoch 3 | Batch 87000 | Loss: 0.3149\n",
            "Skip-gram | Epoch 3 | Batch 87500 | Loss: 0.3036\n",
            "Skip-gram | Epoch 3 | Batch 88000 | Loss: 0.3008\n",
            "Skip-gram | Epoch 3 | Batch 88500 | Loss: 0.3202\n",
            "Skip-gram | Epoch 3 | Batch 89000 | Loss: 0.2986\n",
            "Skip-gram | Epoch 3 | Batch 89500 | Loss: 0.3037\n",
            "Skip-gram | Epoch 3 | Batch 90000 | Loss: 0.3117\n",
            "Skip-gram | Epoch 3 | Batch 90500 | Loss: 0.3136\n",
            "Skip-gram | Epoch 3 | Batch 91000 | Loss: 0.3522\n",
            "Skip-gram | Epoch 3 | Batch 91500 | Loss: 0.3062\n",
            "Skip-gram | Epoch 3 | Batch 92000 | Loss: 0.3169\n",
            "Skip-gram | Epoch 3 | Batch 92500 | Loss: 0.3225\n",
            "Skip-gram | Epoch 3 | Batch 93000 | Loss: 0.3331\n",
            "Skip-gram | Epoch 3 | Batch 93500 | Loss: 0.2949\n",
            "Skip-gram | Epoch 3 | Batch 94000 | Loss: 0.3298\n",
            "Skip-gram | Epoch 3 | Batch 94500 | Loss: 0.2646\n",
            "Skip-gram | Epoch 3 | Batch 95000 | Loss: 0.3158\n",
            "Skip-gram | Epoch 3 | Batch 95500 | Loss: 0.2751\n",
            "Skip-gram | Epoch 3 | Batch 96000 | Loss: 0.3540\n",
            "Skip-gram | Epoch 3 | Batch 96500 | Loss: 0.3172\n",
            "Skip-gram | Epoch 3 | Batch 97000 | Loss: 0.3221\n",
            "Skip-gram | Epoch 3 | Batch 97500 | Loss: 0.3456\n",
            "Skip-gram | Epoch 3 | Batch 98000 | Loss: 0.3062\n",
            "Skip-gram | Epoch 3 | Batch 98500 | Loss: 0.3487\n",
            "Skip-gram | Epoch 3 | Batch 99000 | Loss: 0.3322\n",
            "Skip-gram | Epoch 3 | Batch 99500 | Loss: 0.3264\n",
            "Skip-gram | Epoch 3 | Batch 100000 | Loss: 0.3142\n",
            "Skip-gram | Epoch 3 | Batch 100500 | Loss: 0.3241\n",
            "Skip-gram | Epoch 3 | Batch 101000 | Loss: 0.3086\n",
            "Skip-gram | Epoch 3 | Batch 101500 | Loss: 0.3205\n",
            "Skip-gram | Epoch 3 | Batch 102000 | Loss: 0.3480\n",
            "Skip-gram | Epoch 3 | Batch 102500 | Loss: 0.3598\n",
            "Skip-gram | Epoch 3 | Batch 103000 | Loss: 0.3262\n",
            "Skip-gram | Epoch 3 | Batch 103500 | Loss: 0.3008\n",
            "Skip-gram | Epoch 3 | Batch 104000 | Loss: 0.3210\n",
            "Skip-gram | Epoch 3 | Batch 104500 | Loss: 0.3222\n",
            "Skip-gram | Epoch 3 | Batch 105000 | Loss: 0.3174\n",
            "Skip-gram | Epoch 3 | Batch 105500 | Loss: 0.3169\n",
            "Skip-gram | Epoch 3 | Batch 106000 | Loss: 0.3141\n",
            "Skip-gram | Epoch 3 | Batch 106500 | Loss: 0.3633\n",
            "Skip-gram | Epoch 3 | Batch 107000 | Loss: 0.3204\n",
            "Skip-gram | Epoch 3 | Batch 107500 | Loss: 0.3080\n",
            "Skip-gram | Epoch 3 | Batch 108000 | Loss: 0.3217\n",
            "Skip-gram | Epoch 3 | Batch 108500 | Loss: 0.3113\n",
            "Skip-gram | Epoch 3 | Batch 109000 | Loss: 0.3167\n",
            "Skip-gram | Epoch 3 | Batch 109500 | Loss: 0.3280\n",
            "Skip-gram | Epoch 3 | Batch 110000 | Loss: 0.3297\n",
            "Skip-gram | Epoch 3 | Batch 110500 | Loss: 0.3242\n",
            "Skip-gram | Epoch 3 | Batch 111000 | Loss: 0.3563\n",
            "Skip-gram | Epoch 3 | Batch 111500 | Loss: 0.3621\n",
            "Skip-gram | Epoch 3 | Batch 112000 | Loss: 0.3374\n",
            "Skip-gram | Epoch 3 | Batch 112500 | Loss: 0.3427\n",
            "Skip-gram | Epoch 3 | Batch 113000 | Loss: 0.3028\n",
            "Skip-gram | Epoch 3 | Batch 113500 | Loss: 0.3225\n",
            "Skip-gram | Epoch 3 | Batch 114000 | Loss: 0.3197\n",
            "Skip-gram | Epoch 3 | Batch 114500 | Loss: 0.3268\n",
            "Skip-gram | Epoch 3 | Batch 115000 | Loss: 0.3168\n",
            "Skip-gram | Epoch 3 | Batch 115500 | Loss: 0.3076\n",
            "Skip-gram | Epoch 3 | Batch 116000 | Loss: 0.3299\n",
            "Skip-gram | Epoch 3 | Batch 116500 | Loss: 0.3073\n",
            "Skip-gram | Epoch 3 | Batch 117000 | Loss: 0.2941\n",
            "Skip-gram | Epoch 3 | Batch 117500 | Loss: 0.2974\n",
            "Skip-gram | Epoch 3 | Batch 118000 | Loss: 0.3359\n",
            "Skip-gram | Epoch 3 | Batch 118500 | Loss: 0.3087\n",
            "Skip-gram | Epoch 3 | Batch 119000 | Loss: 0.3056\n",
            "Skip-gram | Epoch 3 | Batch 119500 | Loss: 0.3200\n",
            "Skip-gram | Epoch 3 | Batch 120000 | Loss: 0.3357\n",
            "Skip-gram | Epoch 3 | Batch 120500 | Loss: 0.3340\n",
            "Skip-gram | Epoch 3 | Batch 121000 | Loss: 0.3235\n",
            "Skip-gram | Epoch 3 | Batch 121500 | Loss: 0.3286\n",
            "Skip-gram | Epoch 3 | Batch 122000 | Loss: 0.3072\n",
            "Skip-gram | Epoch 3 | Batch 122500 | Loss: 0.2641\n",
            "Skip-gram | Epoch 3 | Batch 123000 | Loss: 0.1260\n",
            "Skip-gram | Epoch 3 | Batch 123500 | Loss: 0.1532\n",
            "Skip-gram | Epoch 3 | Batch 124000 | Loss: 0.1617\n",
            "Skip-gram | Epoch 3 | Batch 124500 | Loss: 0.1624\n",
            "Skip-gram | Epoch 3 | Batch 125000 | Loss: 0.1185\n",
            "Skip-gram | Epoch 3 | Batch 125500 | Loss: 0.1028\n",
            "Skip-gram | Epoch 3 | Batch 126000 | Loss: 0.2190\n",
            "Skip-gram | Epoch 3 | Batch 126500 | Loss: 0.2202\n",
            "Skip-gram | Epoch 3 | Batch 127000 | Loss: 0.3488\n",
            "Skip-gram | Epoch 3 | Batch 127500 | Loss: 0.3393\n",
            "Skip-gram | Epoch 3 | Batch 128000 | Loss: 0.3211\n",
            "Skip-gram | Epoch 3 | Batch 128500 | Loss: 0.3662\n",
            "Skip-gram | Epoch 3 | Batch 129000 | Loss: 0.3461\n",
            "Skip-gram | Epoch 3 | Batch 129500 | Loss: 0.3179\n",
            "Skip-gram | Epoch 3 | Batch 130000 | Loss: 0.3039\n",
            "Skip-gram | Epoch 3 | Batch 130500 | Loss: 0.3007\n",
            "Skip-gram | Epoch 3 | Batch 131000 | Loss: 0.3419\n",
            "Skip-gram | Epoch 3 | Batch 131500 | Loss: 0.3138\n",
            "Skip-gram | Epoch 3 | Batch 132000 | Loss: 0.3304\n",
            "Skip-gram | Epoch 3 | Batch 132500 | Loss: 0.3104\n",
            "Skip-gram | Epoch 3 | Batch 133000 | Loss: 0.2714\n",
            "Skip-gram | Epoch 3 | Batch 133500 | Loss: 0.2914\n",
            "Skip-gram | Epoch 3 | Batch 134000 | Loss: 0.2892\n",
            "Skip-gram | Epoch 3 | Batch 134500 | Loss: 0.2844\n",
            "Skip-gram | Epoch 3 | Batch 135000 | Loss: 0.3051\n",
            "Skip-gram | Epoch 3 | Batch 135500 | Loss: 0.3172\n",
            "Skip-gram | Epoch 3 | Batch 136000 | Loss: 0.3148\n",
            "Skip-gram | Epoch 3 | Batch 136500 | Loss: 0.3329\n",
            "Skip-gram | Epoch 3 | Batch 137000 | Loss: 0.3287\n",
            "Skip-gram | Epoch 3 | Batch 137500 | Loss: 0.3323\n",
            "Skip-gram | Epoch 3 | Batch 138000 | Loss: 0.3396\n",
            "Skip-gram | Epoch 3 | Batch 138500 | Loss: 0.3029\n",
            "Skip-gram | Epoch 3 | Batch 139000 | Loss: 0.3185\n",
            "Skip-gram | Epoch 3 | Batch 139500 | Loss: 0.3253\n",
            "Skip-gram | Epoch 3 | Batch 140000 | Loss: 0.2991\n",
            "Skip-gram | Epoch 3 | Batch 140500 | Loss: 0.3268\n",
            "Skip-gram | Epoch 3 | Batch 141000 | Loss: 0.3155\n",
            "Skip-gram | Epoch 3 | Batch 141500 | Loss: 0.3152\n",
            "Skip-gram | Epoch 3 | Batch 142000 | Loss: 0.3153\n",
            "Skip-gram | Epoch 3 | Batch 142500 | Loss: 0.3064\n",
            "Skip-gram | Epoch 3 | Batch 143000 | Loss: 0.3102\n",
            "Skip-gram | Epoch 3 | Batch 143500 | Loss: 0.3324\n",
            "Skip-gram | Epoch 3 | Batch 144000 | Loss: 0.3384\n",
            "Skip-gram | Epoch 3 | Batch 144500 | Loss: 0.3070\n",
            "Skip-gram | Epoch 3 | Batch 145000 | Loss: 0.3229\n",
            "Skip-gram | Epoch 3 | Batch 145500 | Loss: 0.3145\n",
            "Skip-gram | Epoch 3 | Batch 146000 | Loss: 0.3268\n",
            "Skip-gram | Epoch 3 | Batch 146500 | Loss: 0.3227\n",
            "Skip-gram | Epoch 3 | Batch 147000 | Loss: 0.3305\n",
            "Skip-gram | Epoch 3 | Batch 147500 | Loss: 0.3244\n",
            "Skip-gram | Epoch 3 | Batch 148000 | Loss: 0.3033\n",
            "Skip-gram | Epoch 3 | Batch 148500 | Loss: 0.3257\n",
            "Skip-gram | Epoch 3 | Batch 149000 | Loss: 0.3619\n",
            "Skip-gram | Epoch 3 | Batch 149500 | Loss: 0.3133\n",
            "Skip-gram | Epoch 3 | Batch 150000 | Loss: 0.3056\n",
            "Skip-gram | Epoch 3 | Batch 150500 | Loss: 0.3243\n",
            "Skip-gram | Epoch 3 | Batch 151000 | Loss: 0.2991\n",
            "Skip-gram | Epoch 3 | Batch 151500 | Loss: 0.3646\n",
            "Skip-gram | Epoch 3 | Batch 152000 | Loss: 0.3458\n",
            "Skip-gram | Epoch 3 | Batch 152500 | Loss: 0.3174\n",
            "Skip-gram | Epoch 3 | Batch 153000 | Loss: 0.3226\n",
            "Skip-gram | Epoch 3 | Batch 153500 | Loss: 0.3084\n",
            "Skip-gram | Epoch 3 | Batch 154000 | Loss: 0.3399\n",
            "Skip-gram | Epoch 3 | Batch 154500 | Loss: 0.3205\n",
            "Skip-gram | Epoch 3 | Batch 155000 | Loss: 0.3070\n",
            "Skip-gram | Epoch 3 | Batch 155500 | Loss: 0.3130\n",
            "Skip-gram | Epoch 3 | Batch 156000 | Loss: 0.3083\n",
            "Skip-gram | Epoch 3 | Batch 156500 | Loss: 0.3269\n",
            "Skip-gram | Epoch 3 | Batch 157000 | Loss: 0.3377\n",
            "Skip-gram | Epoch 3 | Batch 157500 | Loss: 0.3152\n",
            "Skip-gram | Epoch 3 | Batch 158000 | Loss: 0.2923\n",
            "Skip-gram | Epoch 3 | Batch 158500 | Loss: 0.3397\n",
            "Skip-gram | Epoch 3 | Batch 159000 | Loss: 0.3096\n",
            "Skip-gram | Epoch 3 | Batch 159500 | Loss: 0.3127\n",
            "Skip-gram | Epoch 3 | Batch 160000 | Loss: 0.3342\n",
            "Skip-gram | Epoch 3 | Batch 160500 | Loss: 0.3269\n",
            "Skip-gram | Epoch 3 | Batch 161000 | Loss: 0.3036\n",
            "Skip-gram | Epoch 3 | Batch 161500 | Loss: 0.3241\n",
            "Skip-gram | Epoch 3 | Batch 162000 | Loss: 0.3205\n",
            "Skip-gram | Epoch 3 | Batch 162500 | Loss: 0.3248\n",
            "Skip-gram | Epoch 3 | Batch 163000 | Loss: 0.3231\n",
            "Skip-gram | Epoch 3 | Batch 163500 | Loss: 0.3143\n",
            "Skip-gram | Epoch 3 | Batch 164000 | Loss: 0.3166\n",
            "Skip-gram | Epoch 3 | Batch 164500 | Loss: 0.3245\n",
            "Skip-gram | Epoch 3 | Batch 165000 | Loss: 0.3260\n",
            "Skip-gram | Epoch 3 | Batch 165500 | Loss: 0.3292\n",
            "Skip-gram | Epoch 3 | Batch 166000 | Loss: 0.3184\n",
            "Skip-gram | Epoch 3 | Batch 166500 | Loss: 0.3190\n",
            "Skip-gram | Epoch 3 | Batch 167000 | Loss: 0.3058\n",
            "Skip-gram | Epoch 3 | Batch 167500 | Loss: 0.3000\n",
            "Skip-gram | Epoch 3 | Batch 168000 | Loss: 0.3264\n",
            "Skip-gram | Epoch 3 | Batch 168500 | Loss: 0.3117\n",
            "Skip-gram | Epoch 3 | Batch 169000 | Loss: 0.3213\n",
            "Skip-gram | Epoch 3 | Batch 169500 | Loss: 0.3120\n",
            "Skip-gram | Epoch 3 | Batch 170000 | Loss: 0.3201\n",
            "Skip-gram | Epoch 3 | Batch 170500 | Loss: 0.3067\n",
            "Skip-gram | Epoch 3 | Batch 171000 | Loss: 0.3237\n",
            "Skip-gram | Epoch 3 | Batch 171500 | Loss: 0.3184\n",
            "Skip-gram | Epoch 3 | Batch 172000 | Loss: 0.3338\n",
            "Skip-gram | Epoch 3 | Batch 172500 | Loss: 0.3380\n",
            "Skip-gram | Epoch 3 | Batch 173000 | Loss: 0.3261\n",
            "Skip-gram | Epoch 3 | Batch 173500 | Loss: 0.3280\n",
            "Skip-gram | Epoch 3 | Batch 174000 | Loss: 0.3118\n",
            "Skip-gram | Epoch 3 | Batch 174500 | Loss: 0.3218\n",
            "Skip-gram | Epoch 3 | Batch 175000 | Loss: 0.3146\n",
            "Skip-gram | Epoch 3 | Batch 175500 | Loss: 0.3044\n",
            "Skip-gram | Epoch 3 | Batch 176000 | Loss: 0.2944\n",
            "Skip-gram | Epoch 3 | Batch 176500 | Loss: 0.3487\n",
            "Skip-gram | Epoch 3 | Batch 177000 | Loss: 0.3216\n",
            "Skip-gram | Epoch 3 | Batch 177500 | Loss: 0.3081\n",
            "Skip-gram | Epoch 3 | Batch 178000 | Loss: 0.2969\n",
            "Skip-gram | Epoch 3 | Batch 178500 | Loss: 0.3355\n",
            "Skip-gram | Epoch 3 | Batch 179000 | Loss: 0.3366\n",
            "Skip-gram | Epoch 3 | Batch 179500 | Loss: 0.3217\n",
            "Skip-gram | Epoch 3 | Batch 180000 | Loss: 0.3096\n",
            "Skip-gram | Epoch 3 | Batch 180500 | Loss: 0.2967\n",
            "Skip-gram | Epoch 3 | Batch 181000 | Loss: 0.3294\n",
            "Skip-gram | Epoch 3 | Batch 181500 | Loss: 0.3008\n",
            "Skip-gram | Epoch 3 | Batch 182000 | Loss: 0.2993\n",
            "Skip-gram | Epoch 3 | Batch 182500 | Loss: 0.3037\n",
            "Skip-gram | Epoch 3 | Batch 183000 | Loss: 0.2949\n",
            "Skip-gram | Epoch 3 | Batch 183500 | Loss: 0.3485\n",
            "Skip-gram | Epoch 3 | Batch 184000 | Loss: 0.3238\n",
            "Skip-gram | Epoch 3 | Batch 184500 | Loss: 0.2990\n",
            "Skip-gram | Epoch 3 | Batch 185000 | Loss: 0.3000\n",
            "Skip-gram | Epoch 3 | Batch 185500 | Loss: 0.3391\n",
            "Skip-gram | Epoch 3 | Batch 186000 | Loss: 0.3241\n",
            "Skip-gram | Epoch 3 | Batch 186500 | Loss: 0.3060\n",
            "Skip-gram | Epoch 3 | Batch 187000 | Loss: 0.3154\n",
            "Skip-gram | Epoch 3 | Batch 187500 | Loss: 0.3445\n",
            "Skip-gram | Epoch 3 | Batch 188000 | Loss: 0.3175\n",
            "Skip-gram | Epoch 3 | Batch 188500 | Loss: 0.3313\n",
            "Skip-gram | Epoch 3 | Batch 189000 | Loss: 0.3076\n",
            "Skip-gram | Epoch 3 | Batch 189500 | Loss: 0.3409\n",
            "Skip-gram | Epoch 3 | Batch 190000 | Loss: 0.3211\n",
            "Skip-gram | Epoch 3 | Batch 190500 | Loss: 0.3339\n",
            "Skip-gram | Epoch 3 | Batch 191000 | Loss: 0.3132\n",
            "Skip-gram | Epoch 3 | Batch 191500 | Loss: 0.3064\n",
            "Skip-gram | Epoch 3 | Batch 192000 | Loss: 0.3384\n",
            "Skip-gram | Epoch 3 | Batch 192500 | Loss: 0.3069\n",
            "Skip-gram | Epoch 3 | Batch 193000 | Loss: 0.3253\n",
            "Skip-gram | Epoch 3 | Batch 193500 | Loss: 0.3040\n",
            "Skip-gram | Epoch 3 | Batch 194000 | Loss: 0.3093\n",
            "Skip-gram | Epoch 3 | Batch 194500 | Loss: 0.3055\n",
            "Skip-gram | Epoch 3 | Batch 195000 | Loss: 0.3270\n",
            "Skip-gram | Epoch 3 | Batch 195500 | Loss: 0.3200\n",
            "Skip-gram | Epoch 3 | Batch 196000 | Loss: 0.3248\n",
            "Skip-gram | Epoch 3 | Batch 196500 | Loss: 0.3212\n",
            "Skip-gram | Epoch 3 | Batch 197000 | Loss: 0.3071\n",
            "Skip-gram | Epoch 3 | Batch 197500 | Loss: 0.3050\n",
            "Skip-gram | Epoch 3 | Batch 198000 | Loss: 0.3128\n",
            "Skip-gram | Epoch 3 | Batch 198500 | Loss: 0.2992\n",
            "Skip-gram | Epoch 3 | Batch 199000 | Loss: 0.3021\n",
            "Skip-gram | Epoch 3 | Batch 199500 | Loss: 0.2822\n",
            "Skip-gram | Epoch 3 | Batch 200000 | Loss: 0.3257\n",
            "Skip-gram | Epoch 3 | Batch 200500 | Loss: 0.2816\n",
            "Skip-gram | Epoch 3 | Batch 201000 | Loss: 0.3429\n",
            "Skip-gram | Epoch 3 | Batch 201500 | Loss: 0.3186\n",
            "Skip-gram | Epoch 3 | Batch 202000 | Loss: 0.3267\n",
            "Skip-gram | Epoch 3 | Batch 202500 | Loss: 0.3341\n",
            "Skip-gram | Epoch 3 | Batch 203000 | Loss: 0.3242\n",
            "Skip-gram | Epoch 3 | Batch 203500 | Loss: 0.3267\n",
            "Skip-gram | Epoch 3 | Batch 204000 | Loss: 0.3334\n",
            "Skip-gram | Epoch 3 | Batch 204500 | Loss: 0.3367\n",
            "Skip-gram | Epoch 3 | Batch 205000 | Loss: 0.3062\n",
            "Skip-gram | Epoch 3 | Batch 205500 | Loss: 0.3191\n",
            "Skip-gram | Epoch 3 | Batch 206000 | Loss: 0.3353\n",
            "Skip-gram | Epoch 3 | Batch 206500 | Loss: 0.3460\n",
            "Skip-gram | Epoch 3 | Batch 207000 | Loss: 0.3202\n",
            "Skip-gram | Epoch 3 | Batch 207500 | Loss: 0.3148\n",
            "Skip-gram | Epoch 3 | Batch 208000 | Loss: 0.3341\n",
            "Skip-gram | Epoch 3 | Batch 208500 | Loss: 0.3237\n",
            "Skip-gram | Epoch 3 | Batch 209000 | Loss: 0.3271\n",
            "Skip-gram | Epoch 3 | Batch 209500 | Loss: 0.3236\n",
            "Skip-gram | Epoch 3 | Batch 210000 | Loss: 0.3217\n",
            "Skip-gram | Epoch 3 | Batch 210500 | Loss: 0.3116\n",
            "Skip-gram | Epoch 3 | Batch 211000 | Loss: 0.2727\n",
            "Skip-gram | Epoch 3 | Batch 211500 | Loss: 0.2442\n",
            "Skip-gram | Epoch 3 | Batch 212000 | Loss: 0.2145\n",
            "Skip-gram | Epoch 3 | Batch 212500 | Loss: 0.2546\n",
            "Skip-gram | Epoch 3 | Batch 213000 | Loss: 0.2910\n",
            "Skip-gram | Epoch 3 | Batch 213500 | Loss: 0.3539\n",
            "Skip-gram | Epoch 3 | Batch 214000 | Loss: 0.3332\n",
            "Skip-gram | Epoch 3 | Batch 214500 | Loss: 0.3271\n",
            "Skip-gram | Epoch 3 | Batch 215000 | Loss: 0.3029\n",
            "Skip-gram | Epoch 3 | Batch 215500 | Loss: 0.2896\n",
            "Skip-gram | Epoch 3 | Batch 216000 | Loss: 0.3384\n",
            "Skip-gram | Epoch 3 | Batch 216500 | Loss: 0.3095\n",
            "Skip-gram | Epoch 3 | Batch 217000 | Loss: 0.3214\n",
            "Skip-gram | Epoch 3 | Batch 217500 | Loss: 0.2969\n",
            "Skip-gram | Epoch 3 | Batch 218000 | Loss: 0.3586\n",
            "Skip-gram | Epoch 3 | Batch 218500 | Loss: 0.3106\n",
            "Skip-gram | Epoch 3 | Batch 219000 | Loss: 0.3161\n",
            "Skip-gram | Epoch 3 | Batch 219500 | Loss: 0.3311\n",
            "Skip-gram | Epoch 3 | Batch 220000 | Loss: 0.3163\n",
            "Skip-gram | Epoch 3 | Batch 220500 | Loss: 0.3347\n",
            "Skip-gram | Epoch 3 | Batch 221000 | Loss: 0.3157\n",
            "Skip-gram | Epoch 3 | Batch 221500 | Loss: 0.3160\n",
            "Skip-gram | Epoch 3 | Batch 222000 | Loss: 0.3251\n",
            "Skip-gram | Epoch 3 | Batch 222500 | Loss: 0.2942\n",
            "Skip-gram | Epoch 3 | Batch 223000 | Loss: 0.3250\n",
            "Skip-gram | Epoch 3 | Batch 223500 | Loss: 0.3249\n",
            "Skip-gram | Epoch 3 | Batch 224000 | Loss: 0.3293\n",
            "Skip-gram | Epoch 3 | Batch 224500 | Loss: 0.3147\n",
            "Skip-gram | Epoch 3 | Batch 225000 | Loss: 0.3211\n",
            "Skip-gram | Epoch 3 | Batch 225500 | Loss: 0.3146\n",
            "Skip-gram | Epoch 3 | Batch 226000 | Loss: 0.3121\n",
            "Skip-gram | Epoch 3 | Batch 226500 | Loss: 0.3069\n",
            "Skip-gram | Epoch 3 | Batch 227000 | Loss: 0.3372\n",
            "Skip-gram | Epoch 3 | Batch 227500 | Loss: 0.3195\n",
            "Skip-gram | Epoch 3 | Batch 228000 | Loss: 0.3287\n",
            "Skip-gram | Epoch 3 | Batch 228500 | Loss: 0.3151\n",
            "Skip-gram | Epoch 3 | Batch 229000 | Loss: 0.2970\n",
            "Skip-gram | Epoch 3 | Batch 229500 | Loss: 0.3181\n",
            "Skip-gram | Epoch 3 | Batch 230000 | Loss: 0.3125\n",
            "Skip-gram | Epoch 3 | Batch 230500 | Loss: 0.3213\n",
            "Skip-gram | Epoch 3 | Batch 231000 | Loss: 0.3289\n",
            "Skip-gram | Epoch 3 | Batch 231500 | Loss: 0.3069\n",
            "Skip-gram | Epoch 3 | Batch 232000 | Loss: 0.2805\n",
            "Skip-gram | Epoch 3 | Batch 232500 | Loss: 0.2601\n",
            "Skip-gram | Epoch 3 | Batch 233000 | Loss: 0.3495\n",
            "Skip-gram | Epoch 3 | Batch 233500 | Loss: 0.3106\n",
            "Skip-gram | Epoch 3 | Batch 234000 | Loss: 0.3297\n",
            "Skip-gram | Epoch 3 | Batch 234500 | Loss: 0.3193\n",
            "Skip-gram | Epoch 3 | Batch 235000 | Loss: 0.3219\n",
            "Skip-gram | Epoch 3 | Batch 235500 | Loss: 0.3304\n",
            "Skip-gram | Epoch 3 | Batch 236000 | Loss: 0.3009\n",
            "Skip-gram | Epoch 3 | Batch 236500 | Loss: 0.3387\n",
            "Skip-gram | Epoch 3 | Batch 237000 | Loss: 0.3157\n",
            "Skip-gram | Epoch 3 | Batch 237500 | Loss: 0.3129\n",
            "Skip-gram | Epoch 3 | Batch 238000 | Loss: 0.3212\n",
            "Skip-gram | Epoch 3 | Batch 238500 | Loss: 0.2941\n",
            "Skip-gram | Epoch 3 | Batch 239000 | Loss: 0.3114\n",
            "Skip-gram | Epoch 3 | Batch 239500 | Loss: 0.3214\n",
            "Skip-gram | Epoch 3 | Batch 240000 | Loss: 0.3284\n",
            "Skip-gram | Epoch 3 | Batch 240500 | Loss: 0.3233\n",
            "Skip-gram | Epoch 3 | Batch 241000 | Loss: 0.3387\n",
            "Skip-gram | Epoch 3 | Batch 241500 | Loss: 0.3402\n",
            "Skip-gram | Epoch 3 | Batch 242000 | Loss: 0.2808\n",
            "Skip-gram | Epoch 3 | Batch 242500 | Loss: 0.3108\n",
            "Skip-gram | Epoch 3 | Batch 243000 | Loss: 0.3047\n",
            "Skip-gram | Epoch 3 | Batch 243500 | Loss: 0.3214\n",
            "Skip-gram | Epoch 3 | Batch 244000 | Loss: 0.3377\n",
            "Skip-gram | Epoch 3 | Batch 244500 | Loss: 0.3452\n",
            "Skip-gram | Epoch 3 | Batch 245000 | Loss: 0.3547\n",
            "Skip-gram | Epoch 3 | Batch 245500 | Loss: 0.3276\n",
            "Skip-gram | Epoch 3 | Batch 246000 | Loss: 0.3264\n",
            "Skip-gram | Epoch 3 | Batch 246500 | Loss: 0.3208\n",
            "Skip-gram | Epoch 3 | Batch 247000 | Loss: 0.3203\n",
            "Skip-gram | Epoch 3 | Batch 247500 | Loss: 0.3168\n",
            "Skip-gram | Epoch 3 | Batch 248000 | Loss: 0.2929\n",
            "Skip-gram | Epoch 3 | Batch 248500 | Loss: 0.3170\n",
            "Skip-gram | Epoch 3 | Batch 249000 | Loss: 0.3347\n",
            "Skip-gram | Epoch 3 | Batch 249500 | Loss: 0.3178\n",
            "Skip-gram | Epoch 3 | Batch 250000 | Loss: 0.3133\n",
            "Skip-gram | Epoch 3 | Batch 250500 | Loss: 0.3376\n",
            "Skip-gram | Epoch 3 | Batch 251000 | Loss: 0.3355\n",
            "Skip-gram | Epoch 3 | Batch 251500 | Loss: 0.3093\n",
            "Skip-gram | Epoch 3 | Batch 252000 | Loss: 0.3316\n",
            "Skip-gram | Epoch 3 | Batch 252500 | Loss: 0.3092\n",
            "Skip-gram | Epoch 3 | Batch 253000 | Loss: 0.2829\n",
            "Skip-gram | Epoch 3 | Batch 253500 | Loss: 0.3068\n",
            "Skip-gram | Epoch 3 | Batch 254000 | Loss: 0.3119\n",
            "Skip-gram | Epoch 3 | Batch 254500 | Loss: 0.3401\n",
            "Skip-gram | Epoch 3 | Batch 255000 | Loss: 0.2928\n",
            "Skip-gram | Epoch 3 | Batch 255500 | Loss: 0.3240\n",
            "Skip-gram | Epoch 3 | Batch 256000 | Loss: 0.3285\n",
            "Skip-gram | Epoch 3 | Batch 256500 | Loss: 0.3093\n",
            "Skip-gram | Epoch 3 | Batch 257000 | Loss: 0.3148\n",
            "Skip-gram | Epoch 3 | Batch 257500 | Loss: 0.3157\n",
            "Skip-gram | Epoch 3 | Batch 258000 | Loss: 0.3112\n",
            "Skip-gram | Epoch 3 | Batch 258500 | Loss: 0.2522\n",
            "Skip-gram | Epoch 3 | Batch 259000 | Loss: 0.2785\n",
            "Skip-gram | Epoch 3 | Batch 259500 | Loss: 0.3363\n",
            "Skip-gram | Epoch 3 | Batch 260000 | Loss: 0.3462\n",
            "Skip-gram | Epoch 3 | Batch 260500 | Loss: 0.3293\n",
            "Skip-gram | Epoch 3 | Batch 261000 | Loss: 0.3391\n",
            "Skip-gram | Epoch 3 | Batch 261500 | Loss: 0.3028\n",
            "Skip-gram | Epoch 3 | Batch 262000 | Loss: 0.3386\n",
            "Skip-gram | Epoch 3 | Batch 262500 | Loss: 0.3278\n",
            "Skip-gram | Epoch 3 | Batch 263000 | Loss: 0.3320\n",
            "Skip-gram | Epoch 3 | Batch 263500 | Loss: 0.3404\n",
            "Skip-gram | Epoch 3 | Batch 264000 | Loss: 0.3276\n",
            "Skip-gram | Epoch 3 | Batch 264500 | Loss: 0.3213\n",
            "Skip-gram | Epoch 3 | Batch 265000 | Loss: 0.3182\n",
            "Skip-gram | Epoch 3 | Batch 265500 | Loss: 0.3388\n",
            "Skip-gram | Epoch 3 | Batch 266000 | Loss: 0.3204\n",
            "Skip-gram | Epoch 3 | Batch 266500 | Loss: 0.3137\n",
            "Skip-gram | Epoch 3 | Batch 267000 | Loss: 0.3045\n",
            "Skip-gram | Epoch 3 | Batch 267500 | Loss: 0.3172\n",
            "Skip-gram | Epoch 3 | Batch 268000 | Loss: 0.3384\n",
            "Skip-gram | Epoch 3 | Batch 268500 | Loss: 0.3163\n",
            "Skip-gram | Epoch 3 | Batch 269000 | Loss: 0.3333\n",
            "Skip-gram | Epoch 3 | Batch 269500 | Loss: 0.2884\n",
            "Skip-gram | Epoch 3 | Batch 270000 | Loss: 0.3267\n",
            "Skip-gram | Epoch 3 | Batch 270500 | Loss: 0.3542\n",
            "Skip-gram | Epoch 3 | Batch 271000 | Loss: 0.3404\n",
            "Skip-gram | Epoch 3 | Batch 271500 | Loss: 0.3291\n",
            "Skip-gram | Epoch 3 | Batch 272000 | Loss: 0.3045\n",
            "Skip-gram | Epoch 3 | Batch 272500 | Loss: 0.3123\n",
            "Skip-gram | Epoch 3 | Batch 273000 | Loss: 0.3010\n",
            "Skip-gram | Epoch 3 | Batch 273500 | Loss: 0.1903\n",
            "Skip-gram | Epoch 3 | Batch 274000 | Loss: 0.3314\n",
            "Skip-gram | Epoch 3 | Batch 274500 | Loss: 0.3258\n",
            "Skip-gram | Epoch 3 | Batch 275000 | Loss: 0.3264\n",
            "Skip-gram | Epoch 3 | Batch 275500 | Loss: 0.3173\n",
            "Skip-gram | Epoch 3 | Batch 276000 | Loss: 0.2823\n",
            "Skip-gram | Epoch 3 | Batch 276500 | Loss: 0.2816\n",
            "Skip-gram | Epoch 3 | Batch 277000 | Loss: 0.3538\n",
            "Skip-gram | Epoch 3 | Batch 277500 | Loss: 0.3522\n",
            "Skip-gram | Epoch 3 | Batch 278000 | Loss: 0.3132\n",
            "Skip-gram | Epoch 3 | Batch 278500 | Loss: 0.3205\n",
            "Skip-gram | Epoch 3 | Batch 279000 | Loss: 0.3242\n",
            "Skip-gram | Epoch 3 | Batch 279500 | Loss: 0.3137\n",
            "Skip-gram | Epoch 3 | Batch 280000 | Loss: 0.3079\n",
            "Skip-gram | Epoch 3 | Batch 280500 | Loss: 0.3130\n",
            "Skip-gram | Epoch 3 | Batch 281000 | Loss: 0.3144\n",
            "Skip-gram | Epoch 3 | Batch 281500 | Loss: 0.3208\n",
            "Skip-gram | Epoch 3 | Batch 282000 | Loss: 0.3482\n",
            "Skip-gram | Epoch 3 | Batch 282500 | Loss: 0.2985\n",
            "Skip-gram | Epoch 3 | Batch 283000 | Loss: 0.3201\n",
            "Skip-gram | Epoch 3 | Batch 283500 | Loss: 0.2005\n",
            "Skip-gram | Epoch 3 | Batch 284000 | Loss: 0.2146\n",
            "Skip-gram | Epoch 3 | Batch 284500 | Loss: 0.3418\n",
            "Skip-gram | Epoch 3 | Batch 285000 | Loss: 0.3172\n",
            "Skip-gram | Epoch 3 | Batch 285500 | Loss: 0.3205\n",
            "Skip-gram | Epoch 3 | Batch 286000 | Loss: 0.3293\n",
            "Skip-gram | Epoch 3 | Batch 286500 | Loss: 0.3285\n",
            "Skip-gram | Epoch 3 | Batch 287000 | Loss: 0.3271\n",
            "Skip-gram | Epoch 3 | Batch 287500 | Loss: 0.3207\n",
            "Skip-gram | Epoch 3 | Batch 288000 | Loss: 0.3352\n",
            "Skip-gram | Epoch 3 | Batch 288500 | Loss: 0.2380\n",
            "Skip-gram | Epoch 3 | Batch 289000 | Loss: 0.3067\n",
            "Skip-gram | Epoch 3 | Batch 289500 | Loss: 0.3366\n",
            "Skip-gram | Epoch 3 | Batch 290000 | Loss: 0.3333\n",
            "Skip-gram | Epoch 3 | Batch 290500 | Loss: 0.2862\n",
            "Skip-gram | Epoch 3 | Batch 291000 | Loss: 0.3056\n",
            "Skip-gram | Epoch 3 | Batch 291500 | Loss: 0.2910\n",
            "Skip-gram | Epoch 3 | Batch 292000 | Loss: 0.3143\n",
            "Skip-gram | Epoch 3 | Batch 292500 | Loss: 0.3150\n",
            "Skip-gram | Epoch 3 | Batch 293000 | Loss: 0.3426\n",
            "Skip-gram | Epoch 3 | Batch 293500 | Loss: 0.3168\n",
            "Skip-gram | Epoch 3 | Batch 294000 | Loss: 0.3361\n",
            "Skip-gram | Epoch 3 | Batch 294500 | Loss: 0.3244\n",
            "Skip-gram | Epoch 3 | Batch 295000 | Loss: 0.3238\n",
            "Skip-gram | Epoch 3 | Batch 295500 | Loss: 0.3277\n",
            "Skip-gram | Epoch 3 | Batch 296000 | Loss: 0.3269\n",
            "Skip-gram | Epoch 3 | Batch 296500 | Loss: 0.3267\n",
            "Skip-gram | Epoch 3 | Batch 297000 | Loss: 0.3124\n",
            "Skip-gram | Epoch 3 | Batch 297500 | Loss: 0.3360\n",
            "Skip-gram | Epoch 3 | Batch 298000 | Loss: 0.2638\n",
            "Skip-gram | Epoch 3 | Batch 298500 | Loss: 0.3496\n",
            "Skip-gram | Epoch 3 | Batch 299000 | Loss: 0.3381\n",
            "Skip-gram | Epoch 3 | Batch 299500 | Loss: 0.3225\n",
            "Skip-gram | Epoch 3 | Batch 300000 | Loss: 0.3134\n",
            "Skip-gram | Epoch 3 | Batch 300500 | Loss: 0.3233\n",
            "Skip-gram | Epoch 3 | Batch 301000 | Loss: 0.3035\n",
            "Skip-gram | Epoch 3 | Batch 301500 | Loss: 0.3255\n",
            "Skip-gram | Epoch 3 | Batch 302000 | Loss: 0.3255\n",
            "Skip-gram | Epoch 3 | Batch 302500 | Loss: 0.3343\n",
            "Skip-gram | Epoch 3 | Batch 303000 | Loss: 0.3155\n",
            "Skip-gram | Epoch 3 | Batch 303500 | Loss: 0.2959\n",
            "Skip-gram | Epoch 3 | Batch 304000 | Loss: 0.3343\n",
            "Skip-gram | Epoch 3 | Batch 304500 | Loss: 0.3560\n",
            "Skip-gram | Epoch 3 | Batch 305000 | Loss: 0.3249\n",
            "Skip-gram | Epoch 3 | Batch 305500 | Loss: 0.3117\n",
            "Skip-gram | Epoch 3 | Batch 306000 | Loss: 0.3240\n",
            "Skip-gram | Epoch 3 | Batch 306500 | Loss: 0.3172\n",
            "Skip-gram | Epoch 3 | Batch 307000 | Loss: 0.3061\n",
            "Skip-gram | Epoch 3 | Batch 307500 | Loss: 0.3319\n",
            "Skip-gram | Epoch 3 | Batch 308000 | Loss: 0.2918\n",
            "Skip-gram | Epoch 3 | Batch 308500 | Loss: 0.3540\n",
            "Skip-gram | Epoch 3 | Batch 309000 | Loss: 0.3301\n",
            "Skip-gram | Epoch 3 | Batch 309500 | Loss: 0.3296\n",
            "Skip-gram | Epoch 3 | Batch 310000 | Loss: 0.3063\n",
            "Skip-gram | Epoch 3 | Batch 310500 | Loss: 0.3323\n",
            "Skip-gram | Epoch 3 | Batch 311000 | Loss: 0.3532\n",
            "Skip-gram | Epoch 3 | Batch 311500 | Loss: 0.3309\n",
            "Skip-gram | Epoch 3 | Batch 312000 | Loss: 0.3264\n",
            "Skip-gram | Epoch 3 | Batch 312500 | Loss: 0.3245\n",
            "Skip-gram | Epoch 3 | Batch 313000 | Loss: 0.3413\n",
            "Skip-gram | Epoch 3 | Batch 313500 | Loss: 0.3290\n",
            "Skip-gram | Epoch 3 | Batch 314000 | Loss: 0.3367\n",
            "Skip-gram | Epoch 3 | Batch 314500 | Loss: 0.3039\n",
            "Skip-gram | Epoch 3 | Batch 315000 | Loss: 0.3120\n",
            "Skip-gram | Epoch 3 | Batch 315500 | Loss: 0.3270\n",
            "Skip-gram | Epoch 3 | Batch 316000 | Loss: 0.3258\n",
            "Skip-gram | Epoch 3 | Batch 316500 | Loss: 0.3517\n",
            "Skip-gram | Epoch 3 | Batch 317000 | Loss: 0.2811\n",
            "Skip-gram | Epoch 3 | Batch 317500 | Loss: 0.2768\n",
            "Skip-gram | Epoch 3 | Batch 318000 | Loss: 0.3241\n",
            "Skip-gram | Epoch 3 | Batch 318500 | Loss: 0.3021\n",
            "Skip-gram | Epoch 3 | Batch 319000 | Loss: 0.3160\n",
            "Skip-gram | Epoch 3 | Batch 319500 | Loss: 0.3177\n",
            "Skip-gram | Epoch 3 | Batch 320000 | Loss: 0.3240\n",
            "Skip-gram | Epoch 3 | Batch 320500 | Loss: 0.3578\n",
            "Skip-gram | Epoch 3 | Batch 321000 | Loss: 0.3245\n",
            "Skip-gram | Epoch 3 | Batch 321500 | Loss: 0.3278\n",
            "Skip-gram | Epoch 3 | Batch 322000 | Loss: 0.3265\n",
            "Skip-gram | Epoch 3 | Batch 322500 | Loss: 0.3392\n",
            "Skip-gram | Epoch 3 | Batch 323000 | Loss: 0.2791\n",
            "Skip-gram | Epoch 3 | Batch 323500 | Loss: 0.3359\n",
            "Skip-gram Epoch 3. Средний Loss: 0.3144\n",
            "Skip-gram | Epoch 4 | Batch 500 | Loss: 0.3441\n",
            "Skip-gram | Epoch 4 | Batch 1000 | Loss: 0.3036\n",
            "Skip-gram | Epoch 4 | Batch 1500 | Loss: 0.3100\n",
            "Skip-gram | Epoch 4 | Batch 2000 | Loss: 0.3105\n",
            "Skip-gram | Epoch 4 | Batch 2500 | Loss: 0.3007\n",
            "Skip-gram | Epoch 4 | Batch 3000 | Loss: 0.3033\n",
            "Skip-gram | Epoch 4 | Batch 3500 | Loss: 0.3387\n",
            "Skip-gram | Epoch 4 | Batch 4000 | Loss: 0.3150\n",
            "Skip-gram | Epoch 4 | Batch 4500 | Loss: 0.3261\n",
            "Skip-gram | Epoch 4 | Batch 5000 | Loss: 0.3265\n",
            "Skip-gram | Epoch 4 | Batch 5500 | Loss: 0.3555\n",
            "Skip-gram | Epoch 4 | Batch 6000 | Loss: 0.3132\n",
            "Skip-gram | Epoch 4 | Batch 6500 | Loss: 0.3165\n",
            "Skip-gram | Epoch 4 | Batch 7000 | Loss: 0.3081\n",
            "Skip-gram | Epoch 4 | Batch 7500 | Loss: 0.3037\n",
            "Skip-gram | Epoch 4 | Batch 8000 | Loss: 0.2977\n",
            "Skip-gram | Epoch 4 | Batch 8500 | Loss: 0.3223\n",
            "Skip-gram | Epoch 4 | Batch 9000 | Loss: 0.3196\n",
            "Skip-gram | Epoch 4 | Batch 9500 | Loss: 0.3257\n",
            "Skip-gram | Epoch 4 | Batch 10000 | Loss: 0.3129\n",
            "Skip-gram | Epoch 4 | Batch 10500 | Loss: 0.3333\n",
            "Skip-gram | Epoch 4 | Batch 11000 | Loss: 0.3236\n",
            "Skip-gram | Epoch 4 | Batch 11500 | Loss: 0.3346\n",
            "Skip-gram | Epoch 4 | Batch 12000 | Loss: 0.3433\n",
            "Skip-gram | Epoch 4 | Batch 12500 | Loss: 0.3117\n",
            "Skip-gram | Epoch 4 | Batch 13000 | Loss: 0.3321\n",
            "Skip-gram | Epoch 4 | Batch 13500 | Loss: 0.3247\n",
            "Skip-gram | Epoch 4 | Batch 14000 | Loss: 0.3362\n",
            "Skip-gram | Epoch 4 | Batch 14500 | Loss: 0.3131\n",
            "Skip-gram | Epoch 4 | Batch 15000 | Loss: 0.3126\n",
            "Skip-gram | Epoch 4 | Batch 15500 | Loss: 0.2847\n",
            "Skip-gram | Epoch 4 | Batch 16000 | Loss: 0.3122\n",
            "Skip-gram | Epoch 4 | Batch 16500 | Loss: 0.3500\n",
            "Skip-gram | Epoch 4 | Batch 17000 | Loss: 0.3052\n",
            "Skip-gram | Epoch 4 | Batch 17500 | Loss: 0.2787\n",
            "Skip-gram | Epoch 4 | Batch 18000 | Loss: 0.3211\n",
            "Skip-gram | Epoch 4 | Batch 18500 | Loss: 0.3514\n",
            "Skip-gram | Epoch 4 | Batch 19000 | Loss: 0.3279\n",
            "Skip-gram | Epoch 4 | Batch 19500 | Loss: 0.3134\n",
            "Skip-gram | Epoch 4 | Batch 20000 | Loss: 0.2994\n",
            "Skip-gram | Epoch 4 | Batch 20500 | Loss: 0.3008\n",
            "Skip-gram | Epoch 4 | Batch 21000 | Loss: 0.3326\n",
            "Skip-gram | Epoch 4 | Batch 21500 | Loss: 0.3200\n",
            "Skip-gram | Epoch 4 | Batch 22000 | Loss: 0.3428\n",
            "Skip-gram | Epoch 4 | Batch 22500 | Loss: 0.3228\n",
            "Skip-gram | Epoch 4 | Batch 23000 | Loss: 0.3288\n",
            "Skip-gram | Epoch 4 | Batch 23500 | Loss: 0.3287\n",
            "Skip-gram | Epoch 4 | Batch 24000 | Loss: 0.3239\n",
            "Skip-gram | Epoch 4 | Batch 24500 | Loss: 0.3106\n",
            "Skip-gram | Epoch 4 | Batch 25000 | Loss: 0.3157\n",
            "Skip-gram | Epoch 4 | Batch 25500 | Loss: 0.3264\n",
            "Skip-gram | Epoch 4 | Batch 26000 | Loss: 0.3312\n",
            "Skip-gram | Epoch 4 | Batch 26500 | Loss: 0.2900\n",
            "Skip-gram | Epoch 4 | Batch 27000 | Loss: 0.2913\n",
            "Skip-gram | Epoch 4 | Batch 27500 | Loss: 0.3295\n",
            "Skip-gram | Epoch 4 | Batch 28000 | Loss: 0.3264\n",
            "Skip-gram | Epoch 4 | Batch 28500 | Loss: 0.3339\n",
            "Skip-gram | Epoch 4 | Batch 29000 | Loss: 0.3370\n",
            "Skip-gram | Epoch 4 | Batch 29500 | Loss: 0.3455\n",
            "Skip-gram | Epoch 4 | Batch 30000 | Loss: 0.2995\n",
            "Skip-gram | Epoch 4 | Batch 30500 | Loss: 0.3428\n",
            "Skip-gram | Epoch 4 | Batch 31000 | Loss: 0.3123\n",
            "Skip-gram | Epoch 4 | Batch 31500 | Loss: 0.3654\n",
            "Skip-gram | Epoch 4 | Batch 32000 | Loss: 0.3009\n",
            "Skip-gram | Epoch 4 | Batch 32500 | Loss: 0.2807\n",
            "Skip-gram | Epoch 4 | Batch 33000 | Loss: 0.3472\n",
            "Skip-gram | Epoch 4 | Batch 33500 | Loss: 0.3168\n",
            "Skip-gram | Epoch 4 | Batch 34000 | Loss: 0.3143\n",
            "Skip-gram | Epoch 4 | Batch 34500 | Loss: 0.2790\n",
            "Skip-gram | Epoch 4 | Batch 35000 | Loss: 0.1829\n",
            "Skip-gram | Epoch 4 | Batch 35500 | Loss: 0.1493\n",
            "Skip-gram | Epoch 4 | Batch 36000 | Loss: 0.1660\n",
            "Skip-gram | Epoch 4 | Batch 36500 | Loss: 0.3323\n",
            "Skip-gram | Epoch 4 | Batch 37000 | Loss: 0.3179\n",
            "Skip-gram | Epoch 4 | Batch 37500 | Loss: 0.3554\n",
            "Skip-gram | Epoch 4 | Batch 38000 | Loss: 0.3094\n",
            "Skip-gram | Epoch 4 | Batch 38500 | Loss: 0.3274\n",
            "Skip-gram | Epoch 4 | Batch 39000 | Loss: 0.3404\n",
            "Skip-gram | Epoch 4 | Batch 39500 | Loss: 0.3333\n",
            "Skip-gram | Epoch 4 | Batch 40000 | Loss: 0.2920\n",
            "Skip-gram | Epoch 4 | Batch 40500 | Loss: 0.3229\n",
            "Skip-gram | Epoch 4 | Batch 41000 | Loss: 0.3238\n",
            "Skip-gram | Epoch 4 | Batch 41500 | Loss: 0.3052\n",
            "Skip-gram | Epoch 4 | Batch 42000 | Loss: 0.3524\n",
            "Skip-gram | Epoch 4 | Batch 42500 | Loss: 0.3002\n",
            "Skip-gram | Epoch 4 | Batch 43000 | Loss: 0.3224\n",
            "Skip-gram | Epoch 4 | Batch 43500 | Loss: 0.3460\n",
            "Skip-gram | Epoch 4 | Batch 44000 | Loss: 0.3462\n",
            "Skip-gram | Epoch 4 | Batch 44500 | Loss: 0.3343\n",
            "Skip-gram | Epoch 4 | Batch 45000 | Loss: 0.3512\n",
            "Skip-gram | Epoch 4 | Batch 45500 | Loss: 0.3726\n",
            "Skip-gram | Epoch 4 | Batch 46000 | Loss: 0.3018\n",
            "Skip-gram | Epoch 4 | Batch 46500 | Loss: 0.3354\n",
            "Skip-gram | Epoch 4 | Batch 47000 | Loss: 0.3131\n",
            "Skip-gram | Epoch 4 | Batch 47500 | Loss: 0.3272\n",
            "Skip-gram | Epoch 4 | Batch 48000 | Loss: 0.3272\n",
            "Skip-gram | Epoch 4 | Batch 48500 | Loss: 0.3352\n",
            "Skip-gram | Epoch 4 | Batch 49000 | Loss: 0.3069\n",
            "Skip-gram | Epoch 4 | Batch 49500 | Loss: 0.3096\n",
            "Skip-gram | Epoch 4 | Batch 50000 | Loss: 0.3301\n",
            "Skip-gram | Epoch 4 | Batch 50500 | Loss: 0.3372\n",
            "Skip-gram | Epoch 4 | Batch 51000 | Loss: 0.3165\n",
            "Skip-gram | Epoch 4 | Batch 51500 | Loss: 0.3247\n",
            "Skip-gram | Epoch 4 | Batch 52000 | Loss: 0.3295\n",
            "Skip-gram | Epoch 4 | Batch 52500 | Loss: 0.3171\n",
            "Skip-gram | Epoch 4 | Batch 53000 | Loss: 0.3367\n",
            "Skip-gram | Epoch 4 | Batch 53500 | Loss: 0.3278\n",
            "Skip-gram | Epoch 4 | Batch 54000 | Loss: 0.3351\n",
            "Skip-gram | Epoch 4 | Batch 54500 | Loss: 0.3325\n",
            "Skip-gram | Epoch 4 | Batch 55000 | Loss: 0.3274\n",
            "Skip-gram | Epoch 4 | Batch 55500 | Loss: 0.3428\n",
            "Skip-gram | Epoch 4 | Batch 56000 | Loss: 0.2205\n",
            "Skip-gram | Epoch 4 | Batch 56500 | Loss: 0.2623\n",
            "Skip-gram | Epoch 4 | Batch 57000 | Loss: 0.3347\n",
            "Skip-gram | Epoch 4 | Batch 57500 | Loss: 0.3361\n",
            "Skip-gram | Epoch 4 | Batch 58000 | Loss: 0.3216\n",
            "Skip-gram | Epoch 4 | Batch 58500 | Loss: 0.3478\n",
            "Skip-gram | Epoch 4 | Batch 59000 | Loss: 0.3281\n",
            "Skip-gram | Epoch 4 | Batch 59500 | Loss: 0.3171\n",
            "Skip-gram | Epoch 4 | Batch 60000 | Loss: 0.2945\n",
            "Skip-gram | Epoch 4 | Batch 60500 | Loss: 0.3191\n",
            "Skip-gram | Epoch 4 | Batch 61000 | Loss: 0.3063\n",
            "Skip-gram | Epoch 4 | Batch 61500 | Loss: 0.3122\n",
            "Skip-gram | Epoch 4 | Batch 62000 | Loss: 0.3366\n",
            "Skip-gram | Epoch 4 | Batch 62500 | Loss: 0.3467\n",
            "Skip-gram | Epoch 4 | Batch 63000 | Loss: 0.3360\n",
            "Skip-gram | Epoch 4 | Batch 63500 | Loss: 0.3198\n",
            "Skip-gram | Epoch 4 | Batch 64000 | Loss: 0.3423\n",
            "Skip-gram | Epoch 4 | Batch 64500 | Loss: 0.3351\n",
            "Skip-gram | Epoch 4 | Batch 65000 | Loss: 0.3475\n",
            "Skip-gram | Epoch 4 | Batch 65500 | Loss: 0.3324\n",
            "Skip-gram | Epoch 4 | Batch 66000 | Loss: 0.3293\n",
            "Skip-gram | Epoch 4 | Batch 66500 | Loss: 0.3185\n",
            "Skip-gram | Epoch 4 | Batch 67000 | Loss: 0.3246\n",
            "Skip-gram | Epoch 4 | Batch 67500 | Loss: 0.3266\n",
            "Skip-gram | Epoch 4 | Batch 68000 | Loss: 0.3021\n",
            "Skip-gram | Epoch 4 | Batch 68500 | Loss: 0.3340\n",
            "Skip-gram | Epoch 4 | Batch 69000 | Loss: 0.3571\n",
            "Skip-gram | Epoch 4 | Batch 69500 | Loss: 0.3340\n",
            "Skip-gram | Epoch 4 | Batch 70000 | Loss: 0.3174\n",
            "Skip-gram | Epoch 4 | Batch 70500 | Loss: 0.3085\n",
            "Skip-gram | Epoch 4 | Batch 71000 | Loss: 0.3339\n",
            "Skip-gram | Epoch 4 | Batch 71500 | Loss: 0.3303\n",
            "Skip-gram | Epoch 4 | Batch 72000 | Loss: 0.3322\n",
            "Skip-gram | Epoch 4 | Batch 72500 | Loss: 0.3155\n",
            "Skip-gram | Epoch 4 | Batch 73000 | Loss: 0.3552\n",
            "Skip-gram | Epoch 4 | Batch 73500 | Loss: 0.3173\n",
            "Skip-gram | Epoch 4 | Batch 74000 | Loss: 0.2941\n",
            "Skip-gram | Epoch 4 | Batch 74500 | Loss: 0.2451\n",
            "Skip-gram | Epoch 4 | Batch 75000 | Loss: 0.2665\n",
            "Skip-gram | Epoch 4 | Batch 75500 | Loss: 0.2728\n",
            "Skip-gram | Epoch 4 | Batch 76000 | Loss: 0.3306\n",
            "Skip-gram | Epoch 4 | Batch 76500 | Loss: 0.3230\n",
            "Skip-gram | Epoch 4 | Batch 77000 | Loss: 0.2940\n",
            "Skip-gram | Epoch 4 | Batch 77500 | Loss: 0.3357\n",
            "Skip-gram | Epoch 4 | Batch 78000 | Loss: 0.3485\n",
            "Skip-gram | Epoch 4 | Batch 78500 | Loss: 0.3151\n",
            "Skip-gram | Epoch 4 | Batch 79000 | Loss: 0.3114\n",
            "Skip-gram | Epoch 4 | Batch 79500 | Loss: 0.3405\n",
            "Skip-gram | Epoch 4 | Batch 80000 | Loss: 0.3040\n",
            "Skip-gram | Epoch 4 | Batch 80500 | Loss: 0.3194\n",
            "Skip-gram | Epoch 4 | Batch 81000 | Loss: 0.3064\n",
            "Skip-gram | Epoch 4 | Batch 81500 | Loss: 0.2928\n",
            "Skip-gram | Epoch 4 | Batch 82000 | Loss: 0.2963\n",
            "Skip-gram | Epoch 4 | Batch 82500 | Loss: 0.2837\n",
            "Skip-gram | Epoch 4 | Batch 83000 | Loss: 0.2923\n",
            "Skip-gram | Epoch 4 | Batch 83500 | Loss: 0.3457\n",
            "Skip-gram | Epoch 4 | Batch 84000 | Loss: 0.3120\n",
            "Skip-gram | Epoch 4 | Batch 84500 | Loss: 0.3171\n",
            "Skip-gram | Epoch 4 | Batch 85000 | Loss: 0.2694\n",
            "Skip-gram | Epoch 4 | Batch 85500 | Loss: 0.2846\n",
            "Skip-gram | Epoch 4 | Batch 86000 | Loss: 0.3617\n",
            "Skip-gram | Epoch 4 | Batch 86500 | Loss: 0.3447\n",
            "Skip-gram | Epoch 4 | Batch 87000 | Loss: 0.3199\n",
            "Skip-gram | Epoch 4 | Batch 87500 | Loss: 0.3020\n",
            "Skip-gram | Epoch 4 | Batch 88000 | Loss: 0.3006\n",
            "Skip-gram | Epoch 4 | Batch 88500 | Loss: 0.3231\n",
            "Skip-gram | Epoch 4 | Batch 89000 | Loss: 0.3087\n",
            "Skip-gram | Epoch 4 | Batch 89500 | Loss: 0.3088\n",
            "Skip-gram | Epoch 4 | Batch 90000 | Loss: 0.3230\n",
            "Skip-gram | Epoch 4 | Batch 90500 | Loss: 0.3262\n",
            "Skip-gram | Epoch 4 | Batch 91000 | Loss: 0.3576\n",
            "Skip-gram | Epoch 4 | Batch 91500 | Loss: 0.3114\n",
            "Skip-gram | Epoch 4 | Batch 92000 | Loss: 0.3197\n",
            "Skip-gram | Epoch 4 | Batch 92500 | Loss: 0.3319\n",
            "Skip-gram | Epoch 4 | Batch 93000 | Loss: 0.3360\n",
            "Skip-gram | Epoch 4 | Batch 93500 | Loss: 0.3005\n",
            "Skip-gram | Epoch 4 | Batch 94000 | Loss: 0.3406\n",
            "Skip-gram | Epoch 4 | Batch 94500 | Loss: 0.2656\n",
            "Skip-gram | Epoch 4 | Batch 95000 | Loss: 0.3213\n",
            "Skip-gram | Epoch 4 | Batch 95500 | Loss: 0.2767\n",
            "Skip-gram | Epoch 4 | Batch 96000 | Loss: 0.3631\n",
            "Skip-gram | Epoch 4 | Batch 96500 | Loss: 0.3257\n",
            "Skip-gram | Epoch 4 | Batch 97000 | Loss: 0.3281\n",
            "Skip-gram | Epoch 4 | Batch 97500 | Loss: 0.3508\n",
            "Skip-gram | Epoch 4 | Batch 98000 | Loss: 0.3131\n",
            "Skip-gram | Epoch 4 | Batch 98500 | Loss: 0.3641\n",
            "Skip-gram | Epoch 4 | Batch 99000 | Loss: 0.3356\n",
            "Skip-gram | Epoch 4 | Batch 99500 | Loss: 0.3353\n",
            "Skip-gram | Epoch 4 | Batch 100000 | Loss: 0.3173\n",
            "Skip-gram | Epoch 4 | Batch 100500 | Loss: 0.3287\n",
            "Skip-gram | Epoch 4 | Batch 101000 | Loss: 0.3184\n",
            "Skip-gram | Epoch 4 | Batch 101500 | Loss: 0.3296\n",
            "Skip-gram | Epoch 4 | Batch 102000 | Loss: 0.3549\n",
            "Skip-gram | Epoch 4 | Batch 102500 | Loss: 0.3635\n",
            "Skip-gram | Epoch 4 | Batch 103000 | Loss: 0.3293\n",
            "Skip-gram | Epoch 4 | Batch 103500 | Loss: 0.3092\n",
            "Skip-gram | Epoch 4 | Batch 104000 | Loss: 0.3190\n",
            "Skip-gram | Epoch 4 | Batch 104500 | Loss: 0.3303\n",
            "Skip-gram | Epoch 4 | Batch 105000 | Loss: 0.3188\n",
            "Skip-gram | Epoch 4 | Batch 105500 | Loss: 0.3196\n",
            "Skip-gram | Epoch 4 | Batch 106000 | Loss: 0.3192\n",
            "Skip-gram | Epoch 4 | Batch 106500 | Loss: 0.3698\n",
            "Skip-gram | Epoch 4 | Batch 107000 | Loss: 0.3262\n",
            "Skip-gram | Epoch 4 | Batch 107500 | Loss: 0.3122\n",
            "Skip-gram | Epoch 4 | Batch 108000 | Loss: 0.3349\n",
            "Skip-gram | Epoch 4 | Batch 108500 | Loss: 0.3164\n",
            "Skip-gram | Epoch 4 | Batch 109000 | Loss: 0.3223\n",
            "Skip-gram | Epoch 4 | Batch 109500 | Loss: 0.3351\n",
            "Skip-gram | Epoch 4 | Batch 110000 | Loss: 0.3323\n",
            "Skip-gram | Epoch 4 | Batch 110500 | Loss: 0.3351\n",
            "Skip-gram | Epoch 4 | Batch 111000 | Loss: 0.3680\n",
            "Skip-gram | Epoch 4 | Batch 111500 | Loss: 0.3732\n",
            "Skip-gram | Epoch 4 | Batch 112000 | Loss: 0.3370\n",
            "Skip-gram | Epoch 4 | Batch 112500 | Loss: 0.3556\n",
            "Skip-gram | Epoch 4 | Batch 113000 | Loss: 0.3127\n",
            "Skip-gram | Epoch 4 | Batch 113500 | Loss: 0.3298\n",
            "Skip-gram | Epoch 4 | Batch 114000 | Loss: 0.3228\n",
            "Skip-gram | Epoch 4 | Batch 114500 | Loss: 0.3313\n",
            "Skip-gram | Epoch 4 | Batch 115000 | Loss: 0.3251\n",
            "Skip-gram | Epoch 4 | Batch 115500 | Loss: 0.3156\n",
            "Skip-gram | Epoch 4 | Batch 116000 | Loss: 0.3417\n",
            "Skip-gram | Epoch 4 | Batch 116500 | Loss: 0.3189\n",
            "Skip-gram | Epoch 4 | Batch 117000 | Loss: 0.2995\n",
            "Skip-gram | Epoch 4 | Batch 117500 | Loss: 0.3005\n",
            "Skip-gram | Epoch 4 | Batch 118000 | Loss: 0.3387\n",
            "Skip-gram | Epoch 4 | Batch 118500 | Loss: 0.3028\n",
            "Skip-gram | Epoch 4 | Batch 119000 | Loss: 0.3078\n",
            "Skip-gram | Epoch 4 | Batch 119500 | Loss: 0.3238\n",
            "Skip-gram | Epoch 4 | Batch 120000 | Loss: 0.3415\n",
            "Skip-gram | Epoch 4 | Batch 120500 | Loss: 0.3391\n",
            "Skip-gram | Epoch 4 | Batch 121000 | Loss: 0.3340\n",
            "Skip-gram | Epoch 4 | Batch 121500 | Loss: 0.3342\n",
            "Skip-gram | Epoch 4 | Batch 122000 | Loss: 0.3149\n",
            "Skip-gram | Epoch 4 | Batch 122500 | Loss: 0.2677\n",
            "Skip-gram | Epoch 4 | Batch 123000 | Loss: 0.1263\n",
            "Skip-gram | Epoch 4 | Batch 123500 | Loss: 0.1518\n",
            "Skip-gram | Epoch 4 | Batch 124000 | Loss: 0.1686\n",
            "Skip-gram | Epoch 4 | Batch 124500 | Loss: 0.1633\n",
            "Skip-gram | Epoch 4 | Batch 125000 | Loss: 0.1224\n",
            "Skip-gram | Epoch 4 | Batch 125500 | Loss: 0.1044\n",
            "Skip-gram | Epoch 4 | Batch 126000 | Loss: 0.2235\n",
            "Skip-gram | Epoch 4 | Batch 126500 | Loss: 0.2280\n",
            "Skip-gram | Epoch 4 | Batch 127000 | Loss: 0.3548\n",
            "Skip-gram | Epoch 4 | Batch 127500 | Loss: 0.3419\n",
            "Skip-gram | Epoch 4 | Batch 128000 | Loss: 0.3278\n",
            "Skip-gram | Epoch 4 | Batch 128500 | Loss: 0.3745\n",
            "Skip-gram | Epoch 4 | Batch 129000 | Loss: 0.3532\n",
            "Skip-gram | Epoch 4 | Batch 129500 | Loss: 0.3301\n",
            "Skip-gram | Epoch 4 | Batch 130000 | Loss: 0.3081\n",
            "Skip-gram | Epoch 4 | Batch 130500 | Loss: 0.3123\n",
            "Skip-gram | Epoch 4 | Batch 131000 | Loss: 0.3398\n",
            "Skip-gram | Epoch 4 | Batch 131500 | Loss: 0.3193\n",
            "Skip-gram | Epoch 4 | Batch 132000 | Loss: 0.3240\n",
            "Skip-gram | Epoch 4 | Batch 132500 | Loss: 0.3202\n",
            "Skip-gram | Epoch 4 | Batch 133000 | Loss: 0.2734\n",
            "Skip-gram | Epoch 4 | Batch 133500 | Loss: 0.2966\n",
            "Skip-gram | Epoch 4 | Batch 134000 | Loss: 0.2919\n",
            "Skip-gram | Epoch 4 | Batch 134500 | Loss: 0.2846\n",
            "Skip-gram | Epoch 4 | Batch 135000 | Loss: 0.3140\n",
            "Skip-gram | Epoch 4 | Batch 135500 | Loss: 0.3189\n",
            "Skip-gram | Epoch 4 | Batch 136000 | Loss: 0.3153\n",
            "Skip-gram | Epoch 4 | Batch 136500 | Loss: 0.3413\n",
            "Skip-gram | Epoch 4 | Batch 137000 | Loss: 0.3345\n",
            "Skip-gram | Epoch 4 | Batch 137500 | Loss: 0.3406\n",
            "Skip-gram | Epoch 4 | Batch 138000 | Loss: 0.3451\n",
            "Skip-gram | Epoch 4 | Batch 138500 | Loss: 0.3086\n",
            "Skip-gram | Epoch 4 | Batch 139000 | Loss: 0.3198\n",
            "Skip-gram | Epoch 4 | Batch 139500 | Loss: 0.3355\n",
            "Skip-gram | Epoch 4 | Batch 140000 | Loss: 0.3050\n",
            "Skip-gram | Epoch 4 | Batch 140500 | Loss: 0.3400\n",
            "Skip-gram | Epoch 4 | Batch 141000 | Loss: 0.3238\n",
            "Skip-gram | Epoch 4 | Batch 141500 | Loss: 0.3192\n",
            "Skip-gram | Epoch 4 | Batch 142000 | Loss: 0.3243\n",
            "Skip-gram | Epoch 4 | Batch 142500 | Loss: 0.3099\n",
            "Skip-gram | Epoch 4 | Batch 143000 | Loss: 0.3171\n",
            "Skip-gram | Epoch 4 | Batch 143500 | Loss: 0.3470\n",
            "Skip-gram | Epoch 4 | Batch 144000 | Loss: 0.3420\n",
            "Skip-gram | Epoch 4 | Batch 144500 | Loss: 0.3109\n",
            "Skip-gram | Epoch 4 | Batch 145000 | Loss: 0.3313\n",
            "Skip-gram | Epoch 4 | Batch 145500 | Loss: 0.3151\n",
            "Skip-gram | Epoch 4 | Batch 146000 | Loss: 0.3344\n",
            "Skip-gram | Epoch 4 | Batch 146500 | Loss: 0.3310\n",
            "Skip-gram | Epoch 4 | Batch 147000 | Loss: 0.3377\n",
            "Skip-gram | Epoch 4 | Batch 147500 | Loss: 0.3263\n",
            "Skip-gram | Epoch 4 | Batch 148000 | Loss: 0.3063\n",
            "Skip-gram | Epoch 4 | Batch 148500 | Loss: 0.3286\n",
            "Skip-gram | Epoch 4 | Batch 149000 | Loss: 0.3660\n",
            "Skip-gram | Epoch 4 | Batch 149500 | Loss: 0.3161\n",
            "Skip-gram | Epoch 4 | Batch 150000 | Loss: 0.3086\n",
            "Skip-gram | Epoch 4 | Batch 150500 | Loss: 0.3272\n",
            "Skip-gram | Epoch 4 | Batch 151000 | Loss: 0.3067\n",
            "Skip-gram | Epoch 4 | Batch 151500 | Loss: 0.3740\n",
            "Skip-gram | Epoch 4 | Batch 152000 | Loss: 0.3557\n",
            "Skip-gram | Epoch 4 | Batch 152500 | Loss: 0.3239\n",
            "Skip-gram | Epoch 4 | Batch 153000 | Loss: 0.3338\n",
            "Skip-gram | Epoch 4 | Batch 153500 | Loss: 0.3094\n",
            "Skip-gram | Epoch 4 | Batch 154000 | Loss: 0.3442\n",
            "Skip-gram | Epoch 4 | Batch 154500 | Loss: 0.3319\n",
            "Skip-gram | Epoch 4 | Batch 155000 | Loss: 0.3114\n",
            "Skip-gram | Epoch 4 | Batch 155500 | Loss: 0.3130\n",
            "Skip-gram | Epoch 4 | Batch 156000 | Loss: 0.3136\n",
            "Skip-gram | Epoch 4 | Batch 156500 | Loss: 0.3360\n",
            "Skip-gram | Epoch 4 | Batch 157000 | Loss: 0.3449\n",
            "Skip-gram | Epoch 4 | Batch 157500 | Loss: 0.3254\n",
            "Skip-gram | Epoch 4 | Batch 158000 | Loss: 0.3025\n",
            "Skip-gram | Epoch 4 | Batch 158500 | Loss: 0.3421\n",
            "Skip-gram | Epoch 4 | Batch 159000 | Loss: 0.3138\n",
            "Skip-gram | Epoch 4 | Batch 159500 | Loss: 0.3196\n",
            "Skip-gram | Epoch 4 | Batch 160000 | Loss: 0.3474\n",
            "Skip-gram | Epoch 4 | Batch 160500 | Loss: 0.3269\n",
            "Skip-gram | Epoch 4 | Batch 161000 | Loss: 0.3079\n",
            "Skip-gram | Epoch 4 | Batch 161500 | Loss: 0.3292\n",
            "Skip-gram | Epoch 4 | Batch 162000 | Loss: 0.3242\n",
            "Skip-gram | Epoch 4 | Batch 162500 | Loss: 0.3352\n",
            "Skip-gram | Epoch 4 | Batch 163000 | Loss: 0.3262\n",
            "Skip-gram | Epoch 4 | Batch 163500 | Loss: 0.3206\n",
            "Skip-gram | Epoch 4 | Batch 164000 | Loss: 0.3259\n",
            "Skip-gram | Epoch 4 | Batch 164500 | Loss: 0.3285\n",
            "Skip-gram | Epoch 4 | Batch 165000 | Loss: 0.3326\n",
            "Skip-gram | Epoch 4 | Batch 165500 | Loss: 0.3246\n",
            "Skip-gram | Epoch 4 | Batch 166000 | Loss: 0.3221\n",
            "Skip-gram | Epoch 4 | Batch 166500 | Loss: 0.3309\n",
            "Skip-gram | Epoch 4 | Batch 167000 | Loss: 0.3161\n",
            "Skip-gram | Epoch 4 | Batch 167500 | Loss: 0.3063\n",
            "Skip-gram | Epoch 4 | Batch 168000 | Loss: 0.3334\n",
            "Skip-gram | Epoch 4 | Batch 168500 | Loss: 0.3187\n",
            "Skip-gram | Epoch 4 | Batch 169000 | Loss: 0.3217\n",
            "Skip-gram | Epoch 4 | Batch 169500 | Loss: 0.3196\n",
            "Skip-gram | Epoch 4 | Batch 170000 | Loss: 0.3195\n",
            "Skip-gram | Epoch 4 | Batch 170500 | Loss: 0.3044\n",
            "Skip-gram | Epoch 4 | Batch 171000 | Loss: 0.3341\n",
            "Skip-gram | Epoch 4 | Batch 171500 | Loss: 0.3218\n",
            "Skip-gram | Epoch 4 | Batch 172000 | Loss: 0.3454\n",
            "Skip-gram | Epoch 4 | Batch 172500 | Loss: 0.3441\n",
            "Skip-gram | Epoch 4 | Batch 173000 | Loss: 0.3279\n",
            "Skip-gram | Epoch 4 | Batch 173500 | Loss: 0.3300\n",
            "Skip-gram | Epoch 4 | Batch 174000 | Loss: 0.3143\n",
            "Skip-gram | Epoch 4 | Batch 174500 | Loss: 0.3243\n",
            "Skip-gram | Epoch 4 | Batch 175000 | Loss: 0.3182\n",
            "Skip-gram | Epoch 4 | Batch 175500 | Loss: 0.3018\n",
            "Skip-gram | Epoch 4 | Batch 176000 | Loss: 0.3036\n",
            "Skip-gram | Epoch 4 | Batch 176500 | Loss: 0.3516\n",
            "Skip-gram | Epoch 4 | Batch 177000 | Loss: 0.3252\n",
            "Skip-gram | Epoch 4 | Batch 177500 | Loss: 0.3133\n",
            "Skip-gram | Epoch 4 | Batch 178000 | Loss: 0.3137\n",
            "Skip-gram | Epoch 4 | Batch 178500 | Loss: 0.3387\n",
            "Skip-gram | Epoch 4 | Batch 179000 | Loss: 0.3444\n",
            "Skip-gram | Epoch 4 | Batch 179500 | Loss: 0.3268\n",
            "Skip-gram | Epoch 4 | Batch 180000 | Loss: 0.3140\n",
            "Skip-gram | Epoch 4 | Batch 180500 | Loss: 0.3033\n",
            "Skip-gram | Epoch 4 | Batch 181000 | Loss: 0.3312\n",
            "Skip-gram | Epoch 4 | Batch 181500 | Loss: 0.3131\n",
            "Skip-gram | Epoch 4 | Batch 182000 | Loss: 0.3077\n",
            "Skip-gram | Epoch 4 | Batch 182500 | Loss: 0.3053\n",
            "Skip-gram | Epoch 4 | Batch 183000 | Loss: 0.3030\n",
            "Skip-gram | Epoch 4 | Batch 183500 | Loss: 0.3569\n",
            "Skip-gram | Epoch 4 | Batch 184000 | Loss: 0.3300\n",
            "Skip-gram | Epoch 4 | Batch 184500 | Loss: 0.3097\n",
            "Skip-gram | Epoch 4 | Batch 185000 | Loss: 0.3072\n",
            "Skip-gram | Epoch 4 | Batch 185500 | Loss: 0.3392\n",
            "Skip-gram | Epoch 4 | Batch 186000 | Loss: 0.3325\n",
            "Skip-gram | Epoch 4 | Batch 186500 | Loss: 0.3072\n",
            "Skip-gram | Epoch 4 | Batch 187000 | Loss: 0.3285\n",
            "Skip-gram | Epoch 4 | Batch 187500 | Loss: 0.3435\n",
            "Skip-gram | Epoch 4 | Batch 188000 | Loss: 0.3260\n",
            "Skip-gram | Epoch 4 | Batch 188500 | Loss: 0.3391\n",
            "Skip-gram | Epoch 4 | Batch 189000 | Loss: 0.3100\n",
            "Skip-gram | Epoch 4 | Batch 189500 | Loss: 0.3454\n",
            "Skip-gram | Epoch 4 | Batch 190000 | Loss: 0.3355\n",
            "Skip-gram | Epoch 4 | Batch 190500 | Loss: 0.3389\n",
            "Skip-gram | Epoch 4 | Batch 191000 | Loss: 0.3155\n",
            "Skip-gram | Epoch 4 | Batch 191500 | Loss: 0.3095\n",
            "Skip-gram | Epoch 4 | Batch 192000 | Loss: 0.3432\n",
            "Skip-gram | Epoch 4 | Batch 192500 | Loss: 0.3109\n",
            "Skip-gram | Epoch 4 | Batch 193000 | Loss: 0.3355\n",
            "Skip-gram | Epoch 4 | Batch 193500 | Loss: 0.3151\n",
            "Skip-gram | Epoch 4 | Batch 194000 | Loss: 0.3134\n",
            "Skip-gram | Epoch 4 | Batch 194500 | Loss: 0.3066\n",
            "Skip-gram | Epoch 4 | Batch 195000 | Loss: 0.3396\n",
            "Skip-gram | Epoch 4 | Batch 195500 | Loss: 0.3290\n",
            "Skip-gram | Epoch 4 | Batch 196000 | Loss: 0.3230\n",
            "Skip-gram | Epoch 4 | Batch 196500 | Loss: 0.3260\n",
            "Skip-gram | Epoch 4 | Batch 197000 | Loss: 0.3123\n",
            "Skip-gram | Epoch 4 | Batch 197500 | Loss: 0.3139\n",
            "Skip-gram | Epoch 4 | Batch 198000 | Loss: 0.3199\n",
            "Skip-gram | Epoch 4 | Batch 198500 | Loss: 0.2998\n",
            "Skip-gram | Epoch 4 | Batch 199000 | Loss: 0.3006\n",
            "Skip-gram | Epoch 4 | Batch 199500 | Loss: 0.2952\n",
            "Skip-gram | Epoch 4 | Batch 200000 | Loss: 0.3321\n",
            "Skip-gram | Epoch 4 | Batch 200500 | Loss: 0.2847\n",
            "Skip-gram | Epoch 4 | Batch 201000 | Loss: 0.3475\n",
            "Skip-gram | Epoch 4 | Batch 201500 | Loss: 0.3227\n",
            "Skip-gram | Epoch 4 | Batch 202000 | Loss: 0.3318\n",
            "Skip-gram | Epoch 4 | Batch 202500 | Loss: 0.3433\n",
            "Skip-gram | Epoch 4 | Batch 203000 | Loss: 0.3287\n",
            "Skip-gram | Epoch 4 | Batch 203500 | Loss: 0.3356\n",
            "Skip-gram | Epoch 4 | Batch 204000 | Loss: 0.3406\n",
            "Skip-gram | Epoch 4 | Batch 204500 | Loss: 0.3396\n",
            "Skip-gram | Epoch 4 | Batch 205000 | Loss: 0.3065\n",
            "Skip-gram | Epoch 4 | Batch 205500 | Loss: 0.3277\n",
            "Skip-gram | Epoch 4 | Batch 206000 | Loss: 0.3441\n",
            "Skip-gram | Epoch 4 | Batch 206500 | Loss: 0.3526\n",
            "Skip-gram | Epoch 4 | Batch 207000 | Loss: 0.3243\n",
            "Skip-gram | Epoch 4 | Batch 207500 | Loss: 0.3181\n",
            "Skip-gram | Epoch 4 | Batch 208000 | Loss: 0.3415\n",
            "Skip-gram | Epoch 4 | Batch 208500 | Loss: 0.3379\n",
            "Skip-gram | Epoch 4 | Batch 209000 | Loss: 0.3276\n",
            "Skip-gram | Epoch 4 | Batch 209500 | Loss: 0.3278\n",
            "Skip-gram | Epoch 4 | Batch 210000 | Loss: 0.3282\n",
            "Skip-gram | Epoch 4 | Batch 210500 | Loss: 0.3162\n",
            "Skip-gram | Epoch 4 | Batch 211000 | Loss: 0.2741\n",
            "Skip-gram | Epoch 4 | Batch 211500 | Loss: 0.2491\n",
            "Skip-gram | Epoch 4 | Batch 212000 | Loss: 0.2180\n",
            "Skip-gram | Epoch 4 | Batch 212500 | Loss: 0.2578\n",
            "Skip-gram | Epoch 4 | Batch 213000 | Loss: 0.2952\n",
            "Skip-gram | Epoch 4 | Batch 213500 | Loss: 0.3638\n",
            "Skip-gram | Epoch 4 | Batch 214000 | Loss: 0.3443\n",
            "Skip-gram | Epoch 4 | Batch 214500 | Loss: 0.3271\n",
            "Skip-gram | Epoch 4 | Batch 215000 | Loss: 0.3123\n",
            "Skip-gram | Epoch 4 | Batch 215500 | Loss: 0.3001\n",
            "Skip-gram | Epoch 4 | Batch 216000 | Loss: 0.3417\n",
            "Skip-gram | Epoch 4 | Batch 216500 | Loss: 0.3129\n",
            "Skip-gram | Epoch 4 | Batch 217000 | Loss: 0.3289\n",
            "Skip-gram | Epoch 4 | Batch 217500 | Loss: 0.3025\n",
            "Skip-gram | Epoch 4 | Batch 218000 | Loss: 0.3590\n",
            "Skip-gram | Epoch 4 | Batch 218500 | Loss: 0.3135\n",
            "Skip-gram | Epoch 4 | Batch 219000 | Loss: 0.3247\n",
            "Skip-gram | Epoch 4 | Batch 219500 | Loss: 0.3305\n",
            "Skip-gram | Epoch 4 | Batch 220000 | Loss: 0.3180\n",
            "Skip-gram | Epoch 4 | Batch 220500 | Loss: 0.3383\n",
            "Skip-gram | Epoch 4 | Batch 221000 | Loss: 0.3218\n",
            "Skip-gram | Epoch 4 | Batch 221500 | Loss: 0.3219\n",
            "Skip-gram | Epoch 4 | Batch 222000 | Loss: 0.3310\n",
            "Skip-gram | Epoch 4 | Batch 222500 | Loss: 0.2994\n",
            "Skip-gram | Epoch 4 | Batch 223000 | Loss: 0.3309\n",
            "Skip-gram | Epoch 4 | Batch 223500 | Loss: 0.3247\n",
            "Skip-gram | Epoch 4 | Batch 224000 | Loss: 0.3360\n",
            "Skip-gram | Epoch 4 | Batch 224500 | Loss: 0.3145\n",
            "Skip-gram | Epoch 4 | Batch 225000 | Loss: 0.3248\n",
            "Skip-gram | Epoch 4 | Batch 225500 | Loss: 0.3144\n",
            "Skip-gram | Epoch 4 | Batch 226000 | Loss: 0.3088\n",
            "Skip-gram | Epoch 4 | Batch 226500 | Loss: 0.3154\n",
            "Skip-gram | Epoch 4 | Batch 227000 | Loss: 0.3431\n",
            "Skip-gram | Epoch 4 | Batch 227500 | Loss: 0.3250\n",
            "Skip-gram | Epoch 4 | Batch 228000 | Loss: 0.3329\n",
            "Skip-gram | Epoch 4 | Batch 228500 | Loss: 0.3222\n",
            "Skip-gram | Epoch 4 | Batch 229000 | Loss: 0.3024\n",
            "Skip-gram | Epoch 4 | Batch 229500 | Loss: 0.3218\n",
            "Skip-gram | Epoch 4 | Batch 230000 | Loss: 0.3158\n",
            "Skip-gram | Epoch 4 | Batch 230500 | Loss: 0.3336\n",
            "Skip-gram | Epoch 4 | Batch 231000 | Loss: 0.3283\n",
            "Skip-gram | Epoch 4 | Batch 231500 | Loss: 0.3158\n",
            "Skip-gram | Epoch 4 | Batch 232000 | Loss: 0.2783\n",
            "Skip-gram | Epoch 4 | Batch 232500 | Loss: 0.2626\n",
            "Skip-gram | Epoch 4 | Batch 233000 | Loss: 0.3603\n",
            "Skip-gram | Epoch 4 | Batch 233500 | Loss: 0.3144\n",
            "Skip-gram | Epoch 4 | Batch 234000 | Loss: 0.3269\n",
            "Skip-gram | Epoch 4 | Batch 234500 | Loss: 0.3174\n",
            "Skip-gram | Epoch 4 | Batch 235000 | Loss: 0.3302\n",
            "Skip-gram | Epoch 4 | Batch 235500 | Loss: 0.3366\n",
            "Skip-gram | Epoch 4 | Batch 236000 | Loss: 0.3087\n",
            "Skip-gram | Epoch 4 | Batch 236500 | Loss: 0.3463\n",
            "Skip-gram | Epoch 4 | Batch 237000 | Loss: 0.3260\n",
            "Skip-gram | Epoch 4 | Batch 237500 | Loss: 0.3205\n",
            "Skip-gram | Epoch 4 | Batch 238000 | Loss: 0.3250\n",
            "Skip-gram | Epoch 4 | Batch 238500 | Loss: 0.2984\n",
            "Skip-gram | Epoch 4 | Batch 239000 | Loss: 0.3132\n",
            "Skip-gram | Epoch 4 | Batch 239500 | Loss: 0.3159\n",
            "Skip-gram | Epoch 4 | Batch 240000 | Loss: 0.3344\n",
            "Skip-gram | Epoch 4 | Batch 240500 | Loss: 0.3294\n",
            "Skip-gram | Epoch 4 | Batch 241000 | Loss: 0.3421\n",
            "Skip-gram | Epoch 4 | Batch 241500 | Loss: 0.3401\n",
            "Skip-gram | Epoch 4 | Batch 242000 | Loss: 0.2838\n",
            "Skip-gram | Epoch 4 | Batch 242500 | Loss: 0.3088\n",
            "Skip-gram | Epoch 4 | Batch 243000 | Loss: 0.3105\n",
            "Skip-gram | Epoch 4 | Batch 243500 | Loss: 0.3228\n",
            "Skip-gram | Epoch 4 | Batch 244000 | Loss: 0.3441\n",
            "Skip-gram | Epoch 4 | Batch 244500 | Loss: 0.3391\n",
            "Skip-gram | Epoch 4 | Batch 245000 | Loss: 0.3710\n",
            "Skip-gram | Epoch 4 | Batch 245500 | Loss: 0.3367\n",
            "Skip-gram | Epoch 4 | Batch 246000 | Loss: 0.3279\n",
            "Skip-gram | Epoch 4 | Batch 246500 | Loss: 0.3309\n",
            "Skip-gram | Epoch 4 | Batch 247000 | Loss: 0.3276\n",
            "Skip-gram | Epoch 4 | Batch 247500 | Loss: 0.3188\n",
            "Skip-gram | Epoch 4 | Batch 248000 | Loss: 0.3032\n",
            "Skip-gram | Epoch 4 | Batch 248500 | Loss: 0.3279\n",
            "Skip-gram | Epoch 4 | Batch 249000 | Loss: 0.3443\n",
            "Skip-gram | Epoch 4 | Batch 249500 | Loss: 0.3252\n",
            "Skip-gram | Epoch 4 | Batch 250000 | Loss: 0.3210\n",
            "Skip-gram | Epoch 4 | Batch 250500 | Loss: 0.3392\n",
            "Skip-gram | Epoch 4 | Batch 251000 | Loss: 0.3367\n",
            "Skip-gram | Epoch 4 | Batch 251500 | Loss: 0.3082\n",
            "Skip-gram | Epoch 4 | Batch 252000 | Loss: 0.3402\n",
            "Skip-gram | Epoch 4 | Batch 252500 | Loss: 0.3196\n",
            "Skip-gram | Epoch 4 | Batch 253000 | Loss: 0.2846\n",
            "Skip-gram | Epoch 4 | Batch 253500 | Loss: 0.3066\n",
            "Skip-gram | Epoch 4 | Batch 254000 | Loss: 0.3141\n",
            "Skip-gram | Epoch 4 | Batch 254500 | Loss: 0.3515\n",
            "Skip-gram | Epoch 4 | Batch 255000 | Loss: 0.2932\n",
            "Skip-gram | Epoch 4 | Batch 255500 | Loss: 0.3243\n",
            "Skip-gram | Epoch 4 | Batch 256000 | Loss: 0.3312\n",
            "Skip-gram | Epoch 4 | Batch 256500 | Loss: 0.3175\n",
            "Skip-gram | Epoch 4 | Batch 257000 | Loss: 0.3171\n",
            "Skip-gram | Epoch 4 | Batch 257500 | Loss: 0.3238\n",
            "Skip-gram | Epoch 4 | Batch 258000 | Loss: 0.3095\n",
            "Skip-gram | Epoch 4 | Batch 258500 | Loss: 0.2599\n",
            "Skip-gram | Epoch 4 | Batch 259000 | Loss: 0.2784\n",
            "Skip-gram | Epoch 4 | Batch 259500 | Loss: 0.3396\n",
            "Skip-gram | Epoch 4 | Batch 260000 | Loss: 0.3502\n",
            "Skip-gram | Epoch 4 | Batch 260500 | Loss: 0.3314\n",
            "Skip-gram | Epoch 4 | Batch 261000 | Loss: 0.3458\n",
            "Skip-gram | Epoch 4 | Batch 261500 | Loss: 0.3081\n",
            "Skip-gram | Epoch 4 | Batch 262000 | Loss: 0.3394\n",
            "Skip-gram | Epoch 4 | Batch 262500 | Loss: 0.3322\n",
            "Skip-gram | Epoch 4 | Batch 263000 | Loss: 0.3303\n",
            "Skip-gram | Epoch 4 | Batch 263500 | Loss: 0.3476\n",
            "Skip-gram | Epoch 4 | Batch 264000 | Loss: 0.3317\n",
            "Skip-gram | Epoch 4 | Batch 264500 | Loss: 0.3289\n",
            "Skip-gram | Epoch 4 | Batch 265000 | Loss: 0.3233\n",
            "Skip-gram | Epoch 4 | Batch 265500 | Loss: 0.3406\n",
            "Skip-gram | Epoch 4 | Batch 266000 | Loss: 0.3236\n",
            "Skip-gram | Epoch 4 | Batch 266500 | Loss: 0.3187\n",
            "Skip-gram | Epoch 4 | Batch 267000 | Loss: 0.3090\n",
            "Skip-gram | Epoch 4 | Batch 267500 | Loss: 0.3230\n",
            "Skip-gram | Epoch 4 | Batch 268000 | Loss: 0.3453\n",
            "Skip-gram | Epoch 4 | Batch 268500 | Loss: 0.3208\n",
            "Skip-gram | Epoch 4 | Batch 269000 | Loss: 0.3360\n",
            "Skip-gram | Epoch 4 | Batch 269500 | Loss: 0.2933\n",
            "Skip-gram | Epoch 4 | Batch 270000 | Loss: 0.3217\n",
            "Skip-gram | Epoch 4 | Batch 270500 | Loss: 0.3594\n",
            "Skip-gram | Epoch 4 | Batch 271000 | Loss: 0.3383\n",
            "Skip-gram | Epoch 4 | Batch 271500 | Loss: 0.3284\n",
            "Skip-gram | Epoch 4 | Batch 272000 | Loss: 0.3087\n",
            "Skip-gram | Epoch 4 | Batch 272500 | Loss: 0.3143\n",
            "Skip-gram | Epoch 4 | Batch 273000 | Loss: 0.3111\n",
            "Skip-gram | Epoch 4 | Batch 273500 | Loss: 0.1944\n",
            "Skip-gram | Epoch 4 | Batch 274000 | Loss: 0.3336\n",
            "Skip-gram | Epoch 4 | Batch 274500 | Loss: 0.3332\n",
            "Skip-gram | Epoch 4 | Batch 275000 | Loss: 0.3250\n",
            "Skip-gram | Epoch 4 | Batch 275500 | Loss: 0.3191\n",
            "Skip-gram | Epoch 4 | Batch 276000 | Loss: 0.2917\n",
            "Skip-gram | Epoch 4 | Batch 276500 | Loss: 0.2832\n",
            "Skip-gram | Epoch 4 | Batch 277000 | Loss: 0.3598\n",
            "Skip-gram | Epoch 4 | Batch 277500 | Loss: 0.3571\n",
            "Skip-gram | Epoch 4 | Batch 278000 | Loss: 0.3205\n",
            "Skip-gram | Epoch 4 | Batch 278500 | Loss: 0.3283\n",
            "Skip-gram | Epoch 4 | Batch 279000 | Loss: 0.3349\n",
            "Skip-gram | Epoch 4 | Batch 279500 | Loss: 0.3225\n",
            "Skip-gram | Epoch 4 | Batch 280000 | Loss: 0.3105\n",
            "Skip-gram | Epoch 4 | Batch 280500 | Loss: 0.3101\n",
            "Skip-gram | Epoch 4 | Batch 281000 | Loss: 0.3247\n",
            "Skip-gram | Epoch 4 | Batch 281500 | Loss: 0.3257\n",
            "Skip-gram | Epoch 4 | Batch 282000 | Loss: 0.3532\n",
            "Skip-gram | Epoch 4 | Batch 282500 | Loss: 0.3028\n",
            "Skip-gram | Epoch 4 | Batch 283000 | Loss: 0.3231\n",
            "Skip-gram | Epoch 4 | Batch 283500 | Loss: 0.2014\n",
            "Skip-gram | Epoch 4 | Batch 284000 | Loss: 0.2184\n",
            "Skip-gram | Epoch 4 | Batch 284500 | Loss: 0.3442\n",
            "Skip-gram | Epoch 4 | Batch 285000 | Loss: 0.3180\n",
            "Skip-gram | Epoch 4 | Batch 285500 | Loss: 0.3257\n",
            "Skip-gram | Epoch 4 | Batch 286000 | Loss: 0.3291\n",
            "Skip-gram | Epoch 4 | Batch 286500 | Loss: 0.3328\n",
            "Skip-gram | Epoch 4 | Batch 287000 | Loss: 0.3348\n",
            "Skip-gram | Epoch 4 | Batch 287500 | Loss: 0.3297\n",
            "Skip-gram | Epoch 4 | Batch 288000 | Loss: 0.3376\n",
            "Skip-gram | Epoch 4 | Batch 288500 | Loss: 0.2400\n",
            "Skip-gram | Epoch 4 | Batch 289000 | Loss: 0.3099\n",
            "Skip-gram | Epoch 4 | Batch 289500 | Loss: 0.3422\n",
            "Skip-gram | Epoch 4 | Batch 290000 | Loss: 0.3419\n",
            "Skip-gram | Epoch 4 | Batch 290500 | Loss: 0.2930\n",
            "Skip-gram | Epoch 4 | Batch 291000 | Loss: 0.3078\n",
            "Skip-gram | Epoch 4 | Batch 291500 | Loss: 0.2873\n",
            "Skip-gram | Epoch 4 | Batch 292000 | Loss: 0.3228\n",
            "Skip-gram | Epoch 4 | Batch 292500 | Loss: 0.3174\n",
            "Skip-gram | Epoch 4 | Batch 293000 | Loss: 0.3484\n",
            "Skip-gram | Epoch 4 | Batch 293500 | Loss: 0.3247\n",
            "Skip-gram | Epoch 4 | Batch 294000 | Loss: 0.3417\n",
            "Skip-gram | Epoch 4 | Batch 294500 | Loss: 0.3278\n",
            "Skip-gram | Epoch 4 | Batch 295000 | Loss: 0.3209\n",
            "Skip-gram | Epoch 4 | Batch 295500 | Loss: 0.3411\n",
            "Skip-gram | Epoch 4 | Batch 296000 | Loss: 0.3307\n",
            "Skip-gram | Epoch 4 | Batch 296500 | Loss: 0.3319\n",
            "Skip-gram | Epoch 4 | Batch 297000 | Loss: 0.3160\n",
            "Skip-gram | Epoch 4 | Batch 297500 | Loss: 0.3435\n",
            "Skip-gram | Epoch 4 | Batch 298000 | Loss: 0.2646\n",
            "Skip-gram | Epoch 4 | Batch 298500 | Loss: 0.3558\n",
            "Skip-gram | Epoch 4 | Batch 299000 | Loss: 0.3420\n",
            "Skip-gram | Epoch 4 | Batch 299500 | Loss: 0.3255\n",
            "Skip-gram | Epoch 4 | Batch 300000 | Loss: 0.3198\n",
            "Skip-gram | Epoch 4 | Batch 300500 | Loss: 0.3285\n",
            "Skip-gram | Epoch 4 | Batch 301000 | Loss: 0.3104\n",
            "Skip-gram | Epoch 4 | Batch 301500 | Loss: 0.3300\n",
            "Skip-gram | Epoch 4 | Batch 302000 | Loss: 0.3244\n",
            "Skip-gram | Epoch 4 | Batch 302500 | Loss: 0.3426\n",
            "Skip-gram | Epoch 4 | Batch 303000 | Loss: 0.3210\n",
            "Skip-gram | Epoch 4 | Batch 303500 | Loss: 0.2989\n",
            "Skip-gram | Epoch 4 | Batch 304000 | Loss: 0.3374\n",
            "Skip-gram | Epoch 4 | Batch 304500 | Loss: 0.3601\n",
            "Skip-gram | Epoch 4 | Batch 305000 | Loss: 0.3436\n",
            "Skip-gram | Epoch 4 | Batch 305500 | Loss: 0.3103\n",
            "Skip-gram | Epoch 4 | Batch 306000 | Loss: 0.3258\n",
            "Skip-gram | Epoch 4 | Batch 306500 | Loss: 0.3208\n",
            "Skip-gram | Epoch 4 | Batch 307000 | Loss: 0.3016\n",
            "Skip-gram | Epoch 4 | Batch 307500 | Loss: 0.3383\n",
            "Skip-gram | Epoch 4 | Batch 308000 | Loss: 0.2975\n",
            "Skip-gram | Epoch 4 | Batch 308500 | Loss: 0.3542\n",
            "Skip-gram | Epoch 4 | Batch 309000 | Loss: 0.3262\n",
            "Skip-gram | Epoch 4 | Batch 309500 | Loss: 0.3355\n",
            "Skip-gram | Epoch 4 | Batch 310000 | Loss: 0.3100\n",
            "Skip-gram | Epoch 4 | Batch 310500 | Loss: 0.3425\n",
            "Skip-gram | Epoch 4 | Batch 311000 | Loss: 0.3532\n",
            "Skip-gram | Epoch 4 | Batch 311500 | Loss: 0.3393\n",
            "Skip-gram | Epoch 4 | Batch 312000 | Loss: 0.3352\n",
            "Skip-gram | Epoch 4 | Batch 312500 | Loss: 0.3233\n",
            "Skip-gram | Epoch 4 | Batch 313000 | Loss: 0.3467\n",
            "Skip-gram | Epoch 4 | Batch 313500 | Loss: 0.3375\n",
            "Skip-gram | Epoch 4 | Batch 314000 | Loss: 0.3445\n",
            "Skip-gram | Epoch 4 | Batch 314500 | Loss: 0.3095\n",
            "Skip-gram | Epoch 4 | Batch 315000 | Loss: 0.3167\n",
            "Skip-gram | Epoch 4 | Batch 315500 | Loss: 0.3320\n",
            "Skip-gram | Epoch 4 | Batch 316000 | Loss: 0.3292\n",
            "Skip-gram | Epoch 4 | Batch 316500 | Loss: 0.3584\n",
            "Skip-gram | Epoch 4 | Batch 317000 | Loss: 0.2851\n",
            "Skip-gram | Epoch 4 | Batch 317500 | Loss: 0.2777\n",
            "Skip-gram | Epoch 4 | Batch 318000 | Loss: 0.3332\n",
            "Skip-gram | Epoch 4 | Batch 318500 | Loss: 0.3068\n",
            "Skip-gram | Epoch 4 | Batch 319000 | Loss: 0.3194\n",
            "Skip-gram | Epoch 4 | Batch 319500 | Loss: 0.3269\n",
            "Skip-gram | Epoch 4 | Batch 320000 | Loss: 0.3327\n",
            "Skip-gram | Epoch 4 | Batch 320500 | Loss: 0.3606\n",
            "Skip-gram | Epoch 4 | Batch 321000 | Loss: 0.3317\n",
            "Skip-gram | Epoch 4 | Batch 321500 | Loss: 0.3341\n",
            "Skip-gram | Epoch 4 | Batch 322000 | Loss: 0.3303\n",
            "Skip-gram | Epoch 4 | Batch 322500 | Loss: 0.3400\n",
            "Skip-gram | Epoch 4 | Batch 323000 | Loss: 0.2861\n",
            "Skip-gram | Epoch 4 | Batch 323500 | Loss: 0.3426\n",
            "Skip-gram Epoch 4. Средний Loss: 0.3200\n",
            "Skip-gram | Epoch 5 | Batch 500 | Loss: 0.3471\n",
            "Skip-gram | Epoch 5 | Batch 1000 | Loss: 0.3113\n",
            "Skip-gram | Epoch 5 | Batch 1500 | Loss: 0.3127\n",
            "Skip-gram | Epoch 5 | Batch 2000 | Loss: 0.3213\n",
            "Skip-gram | Epoch 5 | Batch 2500 | Loss: 0.2972\n",
            "Skip-gram | Epoch 5 | Batch 3000 | Loss: 0.3052\n",
            "Skip-gram | Epoch 5 | Batch 3500 | Loss: 0.3407\n",
            "Skip-gram | Epoch 5 | Batch 4000 | Loss: 0.3240\n",
            "Skip-gram | Epoch 5 | Batch 4500 | Loss: 0.3250\n",
            "Skip-gram | Epoch 5 | Batch 5000 | Loss: 0.3338\n",
            "Skip-gram | Epoch 5 | Batch 5500 | Loss: 0.3655\n",
            "Skip-gram | Epoch 5 | Batch 6000 | Loss: 0.3103\n",
            "Skip-gram | Epoch 5 | Batch 6500 | Loss: 0.3175\n",
            "Skip-gram | Epoch 5 | Batch 7000 | Loss: 0.3169\n",
            "Skip-gram | Epoch 5 | Batch 7500 | Loss: 0.3210\n",
            "Skip-gram | Epoch 5 | Batch 8000 | Loss: 0.2945\n",
            "Skip-gram | Epoch 5 | Batch 8500 | Loss: 0.3247\n",
            "Skip-gram | Epoch 5 | Batch 9000 | Loss: 0.3135\n",
            "Skip-gram | Epoch 5 | Batch 9500 | Loss: 0.3353\n",
            "Skip-gram | Epoch 5 | Batch 10000 | Loss: 0.3146\n",
            "Skip-gram | Epoch 5 | Batch 10500 | Loss: 0.3326\n",
            "Skip-gram | Epoch 5 | Batch 11000 | Loss: 0.3300\n",
            "Skip-gram | Epoch 5 | Batch 11500 | Loss: 0.3423\n",
            "Skip-gram | Epoch 5 | Batch 12000 | Loss: 0.3480\n",
            "Skip-gram | Epoch 5 | Batch 12500 | Loss: 0.3132\n",
            "Skip-gram | Epoch 5 | Batch 13000 | Loss: 0.3348\n",
            "Skip-gram | Epoch 5 | Batch 13500 | Loss: 0.3263\n",
            "Skip-gram | Epoch 5 | Batch 14000 | Loss: 0.3412\n",
            "Skip-gram | Epoch 5 | Batch 14500 | Loss: 0.3185\n",
            "Skip-gram | Epoch 5 | Batch 15000 | Loss: 0.3262\n",
            "Skip-gram | Epoch 5 | Batch 15500 | Loss: 0.2930\n",
            "Skip-gram | Epoch 5 | Batch 16000 | Loss: 0.3108\n",
            "Skip-gram | Epoch 5 | Batch 16500 | Loss: 0.3554\n",
            "Skip-gram | Epoch 5 | Batch 17000 | Loss: 0.3161\n",
            "Skip-gram | Epoch 5 | Batch 17500 | Loss: 0.2773\n",
            "Skip-gram | Epoch 5 | Batch 18000 | Loss: 0.3280\n",
            "Skip-gram | Epoch 5 | Batch 18500 | Loss: 0.3598\n",
            "Skip-gram | Epoch 5 | Batch 19000 | Loss: 0.3301\n",
            "Skip-gram | Epoch 5 | Batch 19500 | Loss: 0.3231\n",
            "Skip-gram | Epoch 5 | Batch 20000 | Loss: 0.3136\n",
            "Skip-gram | Epoch 5 | Batch 20500 | Loss: 0.3076\n",
            "Skip-gram | Epoch 5 | Batch 21000 | Loss: 0.3318\n",
            "Skip-gram | Epoch 5 | Batch 21500 | Loss: 0.3177\n",
            "Skip-gram | Epoch 5 | Batch 22000 | Loss: 0.3549\n",
            "Skip-gram | Epoch 5 | Batch 22500 | Loss: 0.3228\n",
            "Skip-gram | Epoch 5 | Batch 23000 | Loss: 0.3304\n",
            "Skip-gram | Epoch 5 | Batch 23500 | Loss: 0.3335\n",
            "Skip-gram | Epoch 5 | Batch 24000 | Loss: 0.3298\n",
            "Skip-gram | Epoch 5 | Batch 24500 | Loss: 0.3106\n",
            "Skip-gram | Epoch 5 | Batch 25000 | Loss: 0.3160\n",
            "Skip-gram | Epoch 5 | Batch 25500 | Loss: 0.3304\n",
            "Skip-gram | Epoch 5 | Batch 26000 | Loss: 0.3393\n",
            "Skip-gram | Epoch 5 | Batch 26500 | Loss: 0.2945\n",
            "Skip-gram | Epoch 5 | Batch 27000 | Loss: 0.2976\n",
            "Skip-gram | Epoch 5 | Batch 27500 | Loss: 0.3280\n",
            "Skip-gram | Epoch 5 | Batch 28000 | Loss: 0.3268\n",
            "Skip-gram | Epoch 5 | Batch 28500 | Loss: 0.3394\n",
            "Skip-gram | Epoch 5 | Batch 29000 | Loss: 0.3363\n",
            "Skip-gram | Epoch 5 | Batch 29500 | Loss: 0.3448\n",
            "Skip-gram | Epoch 5 | Batch 30000 | Loss: 0.3047\n",
            "Skip-gram | Epoch 5 | Batch 30500 | Loss: 0.3501\n",
            "Skip-gram | Epoch 5 | Batch 31000 | Loss: 0.3242\n",
            "Skip-gram | Epoch 5 | Batch 31500 | Loss: 0.3671\n",
            "Skip-gram | Epoch 5 | Batch 32000 | Loss: 0.3043\n",
            "Skip-gram | Epoch 5 | Batch 32500 | Loss: 0.2889\n",
            "Skip-gram | Epoch 5 | Batch 33000 | Loss: 0.3525\n",
            "Skip-gram | Epoch 5 | Batch 33500 | Loss: 0.3125\n",
            "Skip-gram | Epoch 5 | Batch 34000 | Loss: 0.3183\n",
            "Skip-gram | Epoch 5 | Batch 34500 | Loss: 0.2794\n",
            "Skip-gram | Epoch 5 | Batch 35000 | Loss: 0.1841\n",
            "Skip-gram | Epoch 5 | Batch 35500 | Loss: 0.1495\n",
            "Skip-gram | Epoch 5 | Batch 36000 | Loss: 0.1662\n",
            "Skip-gram | Epoch 5 | Batch 36500 | Loss: 0.3419\n",
            "Skip-gram | Epoch 5 | Batch 37000 | Loss: 0.3222\n",
            "Skip-gram | Epoch 5 | Batch 37500 | Loss: 0.3699\n",
            "Skip-gram | Epoch 5 | Batch 38000 | Loss: 0.3125\n",
            "Skip-gram | Epoch 5 | Batch 38500 | Loss: 0.3311\n",
            "Skip-gram | Epoch 5 | Batch 39000 | Loss: 0.3407\n",
            "Skip-gram | Epoch 5 | Batch 39500 | Loss: 0.3407\n",
            "Skip-gram | Epoch 5 | Batch 40000 | Loss: 0.2985\n",
            "Skip-gram | Epoch 5 | Batch 40500 | Loss: 0.3258\n",
            "Skip-gram | Epoch 5 | Batch 41000 | Loss: 0.3236\n",
            "Skip-gram | Epoch 5 | Batch 41500 | Loss: 0.3176\n",
            "Skip-gram | Epoch 5 | Batch 42000 | Loss: 0.3652\n",
            "Skip-gram | Epoch 5 | Batch 42500 | Loss: 0.3049\n",
            "Skip-gram | Epoch 5 | Batch 43000 | Loss: 0.3316\n",
            "Skip-gram | Epoch 5 | Batch 43500 | Loss: 0.3545\n",
            "Skip-gram | Epoch 5 | Batch 44000 | Loss: 0.3512\n",
            "Skip-gram | Epoch 5 | Batch 44500 | Loss: 0.3380\n",
            "Skip-gram | Epoch 5 | Batch 45000 | Loss: 0.3576\n",
            "Skip-gram | Epoch 5 | Batch 45500 | Loss: 0.3842\n",
            "Skip-gram | Epoch 5 | Batch 46000 | Loss: 0.3066\n",
            "Skip-gram | Epoch 5 | Batch 46500 | Loss: 0.3444\n",
            "Skip-gram | Epoch 5 | Batch 47000 | Loss: 0.3101\n",
            "Skip-gram | Epoch 5 | Batch 47500 | Loss: 0.3348\n",
            "Skip-gram | Epoch 5 | Batch 48000 | Loss: 0.3304\n",
            "Skip-gram | Epoch 5 | Batch 48500 | Loss: 0.3471\n",
            "Skip-gram | Epoch 5 | Batch 49000 | Loss: 0.3031\n",
            "Skip-gram | Epoch 5 | Batch 49500 | Loss: 0.3122\n",
            "Skip-gram | Epoch 5 | Batch 50000 | Loss: 0.3352\n",
            "Skip-gram | Epoch 5 | Batch 50500 | Loss: 0.3403\n",
            "Skip-gram | Epoch 5 | Batch 51000 | Loss: 0.3214\n",
            "Skip-gram | Epoch 5 | Batch 51500 | Loss: 0.3319\n",
            "Skip-gram | Epoch 5 | Batch 52000 | Loss: 0.3309\n",
            "Skip-gram | Epoch 5 | Batch 52500 | Loss: 0.3187\n",
            "Skip-gram | Epoch 5 | Batch 53000 | Loss: 0.3413\n",
            "Skip-gram | Epoch 5 | Batch 53500 | Loss: 0.3300\n",
            "Skip-gram | Epoch 5 | Batch 54000 | Loss: 0.3299\n",
            "Skip-gram | Epoch 5 | Batch 54500 | Loss: 0.3327\n",
            "Skip-gram | Epoch 5 | Batch 55000 | Loss: 0.3380\n",
            "Skip-gram | Epoch 5 | Batch 55500 | Loss: 0.3493\n",
            "Skip-gram | Epoch 5 | Batch 56000 | Loss: 0.2219\n",
            "Skip-gram | Epoch 5 | Batch 56500 | Loss: 0.2626\n",
            "Skip-gram | Epoch 5 | Batch 57000 | Loss: 0.3397\n",
            "Skip-gram | Epoch 5 | Batch 57500 | Loss: 0.3460\n",
            "Skip-gram | Epoch 5 | Batch 58000 | Loss: 0.3217\n",
            "Skip-gram | Epoch 5 | Batch 58500 | Loss: 0.3489\n",
            "Skip-gram | Epoch 5 | Batch 59000 | Loss: 0.3346\n",
            "Skip-gram | Epoch 5 | Batch 59500 | Loss: 0.3325\n",
            "Skip-gram | Epoch 5 | Batch 60000 | Loss: 0.3042\n",
            "Skip-gram | Epoch 5 | Batch 60500 | Loss: 0.3209\n",
            "Skip-gram | Epoch 5 | Batch 61000 | Loss: 0.3068\n",
            "Skip-gram | Epoch 5 | Batch 61500 | Loss: 0.3133\n",
            "Skip-gram | Epoch 5 | Batch 62000 | Loss: 0.3485\n",
            "Skip-gram | Epoch 5 | Batch 62500 | Loss: 0.3540\n",
            "Skip-gram | Epoch 5 | Batch 63000 | Loss: 0.3395\n",
            "Skip-gram | Epoch 5 | Batch 63500 | Loss: 0.3233\n",
            "Skip-gram | Epoch 5 | Batch 64000 | Loss: 0.3463\n",
            "Skip-gram | Epoch 5 | Batch 64500 | Loss: 0.3404\n",
            "Skip-gram | Epoch 5 | Batch 65000 | Loss: 0.3552\n",
            "Skip-gram | Epoch 5 | Batch 65500 | Loss: 0.3327\n",
            "Skip-gram | Epoch 5 | Batch 66000 | Loss: 0.3336\n",
            "Skip-gram | Epoch 5 | Batch 66500 | Loss: 0.3236\n",
            "Skip-gram | Epoch 5 | Batch 67000 | Loss: 0.3306\n",
            "Skip-gram | Epoch 5 | Batch 67500 | Loss: 0.3327\n",
            "Skip-gram | Epoch 5 | Batch 68000 | Loss: 0.3111\n",
            "Skip-gram | Epoch 5 | Batch 68500 | Loss: 0.3350\n",
            "Skip-gram | Epoch 5 | Batch 69000 | Loss: 0.3630\n",
            "Skip-gram | Epoch 5 | Batch 69500 | Loss: 0.3394\n",
            "Skip-gram | Epoch 5 | Batch 70000 | Loss: 0.3242\n",
            "Skip-gram | Epoch 5 | Batch 70500 | Loss: 0.3202\n",
            "Skip-gram | Epoch 5 | Batch 71000 | Loss: 0.3398\n",
            "Skip-gram | Epoch 5 | Batch 71500 | Loss: 0.3397\n",
            "Skip-gram | Epoch 5 | Batch 72000 | Loss: 0.3397\n",
            "Skip-gram | Epoch 5 | Batch 72500 | Loss: 0.3220\n",
            "Skip-gram | Epoch 5 | Batch 73000 | Loss: 0.3643\n",
            "Skip-gram | Epoch 5 | Batch 73500 | Loss: 0.3220\n",
            "Skip-gram | Epoch 5 | Batch 74000 | Loss: 0.2949\n",
            "Skip-gram | Epoch 5 | Batch 74500 | Loss: 0.2523\n",
            "Skip-gram | Epoch 5 | Batch 75000 | Loss: 0.2678\n",
            "Skip-gram | Epoch 5 | Batch 75500 | Loss: 0.2739\n",
            "Skip-gram | Epoch 5 | Batch 76000 | Loss: 0.3394\n",
            "Skip-gram | Epoch 5 | Batch 76500 | Loss: 0.3320\n",
            "Skip-gram | Epoch 5 | Batch 77000 | Loss: 0.3005\n",
            "Skip-gram | Epoch 5 | Batch 77500 | Loss: 0.3401\n",
            "Skip-gram | Epoch 5 | Batch 78000 | Loss: 0.3544\n",
            "Skip-gram | Epoch 5 | Batch 78500 | Loss: 0.3175\n",
            "Skip-gram | Epoch 5 | Batch 79000 | Loss: 0.3121\n",
            "Skip-gram | Epoch 5 | Batch 79500 | Loss: 0.3437\n",
            "Skip-gram | Epoch 5 | Batch 80000 | Loss: 0.3079\n",
            "Skip-gram | Epoch 5 | Batch 80500 | Loss: 0.3222\n",
            "Skip-gram | Epoch 5 | Batch 81000 | Loss: 0.3092\n",
            "Skip-gram | Epoch 5 | Batch 81500 | Loss: 0.2967\n",
            "Skip-gram | Epoch 5 | Batch 82000 | Loss: 0.3031\n",
            "Skip-gram | Epoch 5 | Batch 82500 | Loss: 0.2845\n",
            "Skip-gram | Epoch 5 | Batch 83000 | Loss: 0.2986\n",
            "Skip-gram | Epoch 5 | Batch 83500 | Loss: 0.3478\n",
            "Skip-gram | Epoch 5 | Batch 84000 | Loss: 0.3185\n",
            "Skip-gram | Epoch 5 | Batch 84500 | Loss: 0.3219\n",
            "Skip-gram | Epoch 5 | Batch 85000 | Loss: 0.2728\n",
            "Skip-gram | Epoch 5 | Batch 85500 | Loss: 0.2837\n",
            "Skip-gram | Epoch 5 | Batch 86000 | Loss: 0.3679\n",
            "Skip-gram | Epoch 5 | Batch 86500 | Loss: 0.3481\n",
            "Skip-gram | Epoch 5 | Batch 87000 | Loss: 0.3303\n",
            "Skip-gram | Epoch 5 | Batch 87500 | Loss: 0.3130\n",
            "Skip-gram | Epoch 5 | Batch 88000 | Loss: 0.3094\n",
            "Skip-gram | Epoch 5 | Batch 88500 | Loss: 0.3246\n",
            "Skip-gram | Epoch 5 | Batch 89000 | Loss: 0.3135\n",
            "Skip-gram | Epoch 5 | Batch 89500 | Loss: 0.3092\n",
            "Skip-gram | Epoch 5 | Batch 90000 | Loss: 0.3276\n",
            "Skip-gram | Epoch 5 | Batch 90500 | Loss: 0.3288\n",
            "Skip-gram | Epoch 5 | Batch 91000 | Loss: 0.3582\n",
            "Skip-gram | Epoch 5 | Batch 91500 | Loss: 0.3154\n",
            "Skip-gram | Epoch 5 | Batch 92000 | Loss: 0.3233\n",
            "Skip-gram | Epoch 5 | Batch 92500 | Loss: 0.3334\n",
            "Skip-gram | Epoch 5 | Batch 93000 | Loss: 0.3485\n",
            "Skip-gram | Epoch 5 | Batch 93500 | Loss: 0.3112\n",
            "Skip-gram | Epoch 5 | Batch 94000 | Loss: 0.3450\n",
            "Skip-gram | Epoch 5 | Batch 94500 | Loss: 0.2718\n",
            "Skip-gram | Epoch 5 | Batch 95000 | Loss: 0.3251\n",
            "Skip-gram | Epoch 5 | Batch 95500 | Loss: 0.2823\n",
            "Skip-gram | Epoch 5 | Batch 96000 | Loss: 0.3708\n",
            "Skip-gram | Epoch 5 | Batch 96500 | Loss: 0.3345\n",
            "Skip-gram | Epoch 5 | Batch 97000 | Loss: 0.3293\n",
            "Skip-gram | Epoch 5 | Batch 97500 | Loss: 0.3592\n",
            "Skip-gram | Epoch 5 | Batch 98000 | Loss: 0.3104\n",
            "Skip-gram | Epoch 5 | Batch 98500 | Loss: 0.3658\n",
            "Skip-gram | Epoch 5 | Batch 99000 | Loss: 0.3439\n",
            "Skip-gram | Epoch 5 | Batch 99500 | Loss: 0.3445\n",
            "Skip-gram | Epoch 5 | Batch 100000 | Loss: 0.3265\n",
            "Skip-gram | Epoch 5 | Batch 100500 | Loss: 0.3356\n",
            "Skip-gram | Epoch 5 | Batch 101000 | Loss: 0.3205\n",
            "Skip-gram | Epoch 5 | Batch 101500 | Loss: 0.3366\n",
            "Skip-gram | Epoch 5 | Batch 102000 | Loss: 0.3612\n",
            "Skip-gram | Epoch 5 | Batch 102500 | Loss: 0.3698\n",
            "Skip-gram | Epoch 5 | Batch 103000 | Loss: 0.3374\n",
            "Skip-gram | Epoch 5 | Batch 103500 | Loss: 0.3097\n",
            "Skip-gram | Epoch 5 | Batch 104000 | Loss: 0.3279\n",
            "Skip-gram | Epoch 5 | Batch 104500 | Loss: 0.3286\n",
            "Skip-gram | Epoch 5 | Batch 105000 | Loss: 0.3301\n",
            "Skip-gram | Epoch 5 | Batch 105500 | Loss: 0.3260\n",
            "Skip-gram | Epoch 5 | Batch 106000 | Loss: 0.3238\n",
            "Skip-gram | Epoch 5 | Batch 106500 | Loss: 0.3764\n",
            "Skip-gram | Epoch 5 | Batch 107000 | Loss: 0.3365\n",
            "Skip-gram | Epoch 5 | Batch 107500 | Loss: 0.3134\n",
            "Skip-gram | Epoch 5 | Batch 108000 | Loss: 0.3373\n",
            "Skip-gram | Epoch 5 | Batch 108500 | Loss: 0.3264\n",
            "Skip-gram | Epoch 5 | Batch 109000 | Loss: 0.3298\n",
            "Skip-gram | Epoch 5 | Batch 109500 | Loss: 0.3400\n",
            "Skip-gram | Epoch 5 | Batch 110000 | Loss: 0.3369\n",
            "Skip-gram | Epoch 5 | Batch 110500 | Loss: 0.3407\n",
            "Skip-gram | Epoch 5 | Batch 111000 | Loss: 0.3682\n",
            "Skip-gram | Epoch 5 | Batch 111500 | Loss: 0.3738\n",
            "Skip-gram | Epoch 5 | Batch 112000 | Loss: 0.3517\n",
            "Skip-gram | Epoch 5 | Batch 112500 | Loss: 0.3581\n",
            "Skip-gram | Epoch 5 | Batch 113000 | Loss: 0.3177\n",
            "Skip-gram | Epoch 5 | Batch 113500 | Loss: 0.3376\n",
            "Skip-gram | Epoch 5 | Batch 114000 | Loss: 0.3309\n",
            "Skip-gram | Epoch 5 | Batch 114500 | Loss: 0.3354\n",
            "Skip-gram | Epoch 5 | Batch 115000 | Loss: 0.3248\n",
            "Skip-gram | Epoch 5 | Batch 115500 | Loss: 0.3241\n",
            "Skip-gram | Epoch 5 | Batch 116000 | Loss: 0.3428\n",
            "Skip-gram | Epoch 5 | Batch 116500 | Loss: 0.3186\n",
            "Skip-gram | Epoch 5 | Batch 117000 | Loss: 0.3073\n",
            "Skip-gram | Epoch 5 | Batch 117500 | Loss: 0.3087\n",
            "Skip-gram | Epoch 5 | Batch 118000 | Loss: 0.3470\n",
            "Skip-gram | Epoch 5 | Batch 118500 | Loss: 0.3172\n",
            "Skip-gram | Epoch 5 | Batch 119000 | Loss: 0.3210\n",
            "Skip-gram | Epoch 5 | Batch 119500 | Loss: 0.3282\n",
            "Skip-gram | Epoch 5 | Batch 120000 | Loss: 0.3496\n",
            "Skip-gram | Epoch 5 | Batch 120500 | Loss: 0.3438\n",
            "Skip-gram | Epoch 5 | Batch 121000 | Loss: 0.3414\n",
            "Skip-gram | Epoch 5 | Batch 121500 | Loss: 0.3378\n",
            "Skip-gram | Epoch 5 | Batch 122000 | Loss: 0.3225\n",
            "Skip-gram | Epoch 5 | Batch 122500 | Loss: 0.2698\n",
            "Skip-gram | Epoch 5 | Batch 123000 | Loss: 0.1255\n",
            "Skip-gram | Epoch 5 | Batch 123500 | Loss: 0.1559\n",
            "Skip-gram | Epoch 5 | Batch 124000 | Loss: 0.1732\n",
            "Skip-gram | Epoch 5 | Batch 124500 | Loss: 0.1611\n",
            "Skip-gram | Epoch 5 | Batch 125000 | Loss: 0.1222\n",
            "Skip-gram | Epoch 5 | Batch 125500 | Loss: 0.1070\n",
            "Skip-gram | Epoch 5 | Batch 126000 | Loss: 0.2319\n",
            "Skip-gram | Epoch 5 | Batch 126500 | Loss: 0.2372\n",
            "Skip-gram | Epoch 5 | Batch 127000 | Loss: 0.3625\n",
            "Skip-gram | Epoch 5 | Batch 127500 | Loss: 0.3532\n",
            "Skip-gram | Epoch 5 | Batch 128000 | Loss: 0.3385\n",
            "Skip-gram | Epoch 5 | Batch 128500 | Loss: 0.3808\n",
            "Skip-gram | Epoch 5 | Batch 129000 | Loss: 0.3621\n",
            "Skip-gram | Epoch 5 | Batch 129500 | Loss: 0.3305\n",
            "Skip-gram | Epoch 5 | Batch 130000 | Loss: 0.3181\n",
            "Skip-gram | Epoch 5 | Batch 130500 | Loss: 0.3133\n",
            "Skip-gram | Epoch 5 | Batch 131000 | Loss: 0.3495\n",
            "Skip-gram | Epoch 5 | Batch 131500 | Loss: 0.3196\n",
            "Skip-gram | Epoch 5 | Batch 132000 | Loss: 0.3296\n",
            "Skip-gram | Epoch 5 | Batch 132500 | Loss: 0.3181\n",
            "Skip-gram | Epoch 5 | Batch 133000 | Loss: 0.2806\n",
            "Skip-gram | Epoch 5 | Batch 133500 | Loss: 0.3014\n",
            "Skip-gram | Epoch 5 | Batch 134000 | Loss: 0.3081\n",
            "Skip-gram | Epoch 5 | Batch 134500 | Loss: 0.2908\n",
            "Skip-gram | Epoch 5 | Batch 135000 | Loss: 0.3162\n",
            "Skip-gram | Epoch 5 | Batch 135500 | Loss: 0.3235\n",
            "Skip-gram | Epoch 5 | Batch 136000 | Loss: 0.3232\n",
            "Skip-gram | Epoch 5 | Batch 136500 | Loss: 0.3477\n",
            "Skip-gram | Epoch 5 | Batch 137000 | Loss: 0.3353\n",
            "Skip-gram | Epoch 5 | Batch 137500 | Loss: 0.3487\n",
            "Skip-gram | Epoch 5 | Batch 138000 | Loss: 0.3435\n",
            "Skip-gram | Epoch 5 | Batch 138500 | Loss: 0.3170\n",
            "Skip-gram | Epoch 5 | Batch 139000 | Loss: 0.3223\n",
            "Skip-gram | Epoch 5 | Batch 139500 | Loss: 0.3351\n",
            "Skip-gram | Epoch 5 | Batch 140000 | Loss: 0.3047\n",
            "Skip-gram | Epoch 5 | Batch 140500 | Loss: 0.3471\n",
            "Skip-gram | Epoch 5 | Batch 141000 | Loss: 0.3302\n",
            "Skip-gram | Epoch 5 | Batch 141500 | Loss: 0.3246\n",
            "Skip-gram | Epoch 5 | Batch 142000 | Loss: 0.3254\n",
            "Skip-gram | Epoch 5 | Batch 142500 | Loss: 0.3202\n",
            "Skip-gram | Epoch 5 | Batch 143000 | Loss: 0.3189\n",
            "Skip-gram | Epoch 5 | Batch 143500 | Loss: 0.3455\n",
            "Skip-gram | Epoch 5 | Batch 144000 | Loss: 0.3572\n",
            "Skip-gram | Epoch 5 | Batch 144500 | Loss: 0.3220\n",
            "Skip-gram | Epoch 5 | Batch 145000 | Loss: 0.3424\n",
            "Skip-gram | Epoch 5 | Batch 145500 | Loss: 0.3253\n",
            "Skip-gram | Epoch 5 | Batch 146000 | Loss: 0.3349\n",
            "Skip-gram | Epoch 5 | Batch 146500 | Loss: 0.3398\n",
            "Skip-gram | Epoch 5 | Batch 147000 | Loss: 0.3474\n",
            "Skip-gram | Epoch 5 | Batch 147500 | Loss: 0.3343\n",
            "Skip-gram | Epoch 5 | Batch 148000 | Loss: 0.3203\n",
            "Skip-gram | Epoch 5 | Batch 148500 | Loss: 0.3415\n",
            "Skip-gram | Epoch 5 | Batch 149000 | Loss: 0.3674\n",
            "Skip-gram | Epoch 5 | Batch 149500 | Loss: 0.3253\n",
            "Skip-gram | Epoch 5 | Batch 150000 | Loss: 0.3182\n",
            "Skip-gram | Epoch 5 | Batch 150500 | Loss: 0.3320\n",
            "Skip-gram | Epoch 5 | Batch 151000 | Loss: 0.3081\n",
            "Skip-gram | Epoch 5 | Batch 151500 | Loss: 0.3748\n",
            "Skip-gram | Epoch 5 | Batch 152000 | Loss: 0.3584\n",
            "Skip-gram | Epoch 5 | Batch 152500 | Loss: 0.3302\n",
            "Skip-gram | Epoch 5 | Batch 153000 | Loss: 0.3388\n",
            "Skip-gram | Epoch 5 | Batch 153500 | Loss: 0.3197\n",
            "Skip-gram | Epoch 5 | Batch 154000 | Loss: 0.3492\n",
            "Skip-gram | Epoch 5 | Batch 154500 | Loss: 0.3358\n",
            "Skip-gram | Epoch 5 | Batch 155000 | Loss: 0.3203\n",
            "Skip-gram | Epoch 5 | Batch 155500 | Loss: 0.3242\n",
            "Skip-gram | Epoch 5 | Batch 156000 | Loss: 0.3178\n",
            "Skip-gram | Epoch 5 | Batch 156500 | Loss: 0.3451\n",
            "Skip-gram | Epoch 5 | Batch 157000 | Loss: 0.3475\n",
            "Skip-gram | Epoch 5 | Batch 157500 | Loss: 0.3346\n",
            "Skip-gram | Epoch 5 | Batch 158000 | Loss: 0.3047\n",
            "Skip-gram | Epoch 5 | Batch 158500 | Loss: 0.3489\n",
            "Skip-gram | Epoch 5 | Batch 159000 | Loss: 0.3226\n",
            "Skip-gram | Epoch 5 | Batch 159500 | Loss: 0.3247\n",
            "Skip-gram | Epoch 5 | Batch 160000 | Loss: 0.3466\n",
            "Skip-gram | Epoch 5 | Batch 160500 | Loss: 0.3376\n",
            "Skip-gram | Epoch 5 | Batch 161000 | Loss: 0.3160\n",
            "Skip-gram | Epoch 5 | Batch 161500 | Loss: 0.3361\n",
            "Skip-gram | Epoch 5 | Batch 162000 | Loss: 0.3333\n",
            "Skip-gram | Epoch 5 | Batch 162500 | Loss: 0.3352\n",
            "Skip-gram | Epoch 5 | Batch 163000 | Loss: 0.3287\n",
            "Skip-gram | Epoch 5 | Batch 163500 | Loss: 0.3301\n",
            "Skip-gram | Epoch 5 | Batch 164000 | Loss: 0.3283\n",
            "Skip-gram | Epoch 5 | Batch 164500 | Loss: 0.3344\n",
            "Skip-gram | Epoch 5 | Batch 165000 | Loss: 0.3355\n",
            "Skip-gram | Epoch 5 | Batch 165500 | Loss: 0.3370\n",
            "Skip-gram | Epoch 5 | Batch 166000 | Loss: 0.3268\n",
            "Skip-gram | Epoch 5 | Batch 166500 | Loss: 0.3373\n",
            "Skip-gram | Epoch 5 | Batch 167000 | Loss: 0.3160\n",
            "Skip-gram | Epoch 5 | Batch 167500 | Loss: 0.3164\n",
            "Skip-gram | Epoch 5 | Batch 168000 | Loss: 0.3392\n",
            "Skip-gram | Epoch 5 | Batch 168500 | Loss: 0.3187\n",
            "Skip-gram | Epoch 5 | Batch 169000 | Loss: 0.3308\n",
            "Skip-gram | Epoch 5 | Batch 169500 | Loss: 0.3276\n",
            "Skip-gram | Epoch 5 | Batch 170000 | Loss: 0.3199\n",
            "Skip-gram | Epoch 5 | Batch 170500 | Loss: 0.3112\n",
            "Skip-gram | Epoch 5 | Batch 171000 | Loss: 0.3367\n",
            "Skip-gram | Epoch 5 | Batch 171500 | Loss: 0.3249\n",
            "Skip-gram | Epoch 5 | Batch 172000 | Loss: 0.3376\n",
            "Skip-gram | Epoch 5 | Batch 172500 | Loss: 0.3466\n",
            "Skip-gram | Epoch 5 | Batch 173000 | Loss: 0.3347\n",
            "Skip-gram | Epoch 5 | Batch 173500 | Loss: 0.3339\n",
            "Skip-gram | Epoch 5 | Batch 174000 | Loss: 0.3157\n",
            "Skip-gram | Epoch 5 | Batch 174500 | Loss: 0.3329\n",
            "Skip-gram | Epoch 5 | Batch 175000 | Loss: 0.3242\n",
            "Skip-gram | Epoch 5 | Batch 175500 | Loss: 0.3101\n",
            "Skip-gram | Epoch 5 | Batch 176000 | Loss: 0.3095\n",
            "Skip-gram | Epoch 5 | Batch 176500 | Loss: 0.3584\n",
            "Skip-gram | Epoch 5 | Batch 177000 | Loss: 0.3287\n",
            "Skip-gram | Epoch 5 | Batch 177500 | Loss: 0.3193\n",
            "Skip-gram | Epoch 5 | Batch 178000 | Loss: 0.3085\n",
            "Skip-gram | Epoch 5 | Batch 178500 | Loss: 0.3415\n",
            "Skip-gram | Epoch 5 | Batch 179000 | Loss: 0.3475\n",
            "Skip-gram | Epoch 5 | Batch 179500 | Loss: 0.3336\n",
            "Skip-gram | Epoch 5 | Batch 180000 | Loss: 0.3178\n",
            "Skip-gram | Epoch 5 | Batch 180500 | Loss: 0.3049\n",
            "Skip-gram | Epoch 5 | Batch 181000 | Loss: 0.3424\n",
            "Skip-gram | Epoch 5 | Batch 181500 | Loss: 0.3138\n",
            "Skip-gram | Epoch 5 | Batch 182000 | Loss: 0.3145\n",
            "Skip-gram | Epoch 5 | Batch 182500 | Loss: 0.3137\n",
            "Skip-gram | Epoch 5 | Batch 183000 | Loss: 0.3099\n",
            "Skip-gram | Epoch 5 | Batch 183500 | Loss: 0.3578\n",
            "Skip-gram | Epoch 5 | Batch 184000 | Loss: 0.3332\n",
            "Skip-gram | Epoch 5 | Batch 184500 | Loss: 0.3082\n",
            "Skip-gram | Epoch 5 | Batch 185000 | Loss: 0.3128\n",
            "Skip-gram | Epoch 5 | Batch 185500 | Loss: 0.3515\n",
            "Skip-gram | Epoch 5 | Batch 186000 | Loss: 0.3391\n",
            "Skip-gram | Epoch 5 | Batch 186500 | Loss: 0.3096\n",
            "Skip-gram | Epoch 5 | Batch 187000 | Loss: 0.3305\n",
            "Skip-gram | Epoch 5 | Batch 187500 | Loss: 0.3439\n",
            "Skip-gram | Epoch 5 | Batch 188000 | Loss: 0.3349\n",
            "Skip-gram | Epoch 5 | Batch 188500 | Loss: 0.3434\n",
            "Skip-gram | Epoch 5 | Batch 189000 | Loss: 0.3183\n",
            "Skip-gram | Epoch 5 | Batch 189500 | Loss: 0.3596\n",
            "Skip-gram | Epoch 5 | Batch 190000 | Loss: 0.3324\n",
            "Skip-gram | Epoch 5 | Batch 190500 | Loss: 0.3460\n",
            "Skip-gram | Epoch 5 | Batch 191000 | Loss: 0.3271\n",
            "Skip-gram | Epoch 5 | Batch 191500 | Loss: 0.3168\n",
            "Skip-gram | Epoch 5 | Batch 192000 | Loss: 0.3443\n",
            "Skip-gram | Epoch 5 | Batch 192500 | Loss: 0.3167\n",
            "Skip-gram | Epoch 5 | Batch 193000 | Loss: 0.3419\n",
            "Skip-gram | Epoch 5 | Batch 193500 | Loss: 0.3250\n",
            "Skip-gram | Epoch 5 | Batch 194000 | Loss: 0.3243\n",
            "Skip-gram | Epoch 5 | Batch 194500 | Loss: 0.3104\n",
            "Skip-gram | Epoch 5 | Batch 195000 | Loss: 0.3379\n",
            "Skip-gram | Epoch 5 | Batch 195500 | Loss: 0.3235\n",
            "Skip-gram | Epoch 5 | Batch 196000 | Loss: 0.3299\n",
            "Skip-gram | Epoch 5 | Batch 196500 | Loss: 0.3411\n",
            "Skip-gram | Epoch 5 | Batch 197000 | Loss: 0.3123\n",
            "Skip-gram | Epoch 5 | Batch 197500 | Loss: 0.3154\n",
            "Skip-gram | Epoch 5 | Batch 198000 | Loss: 0.3230\n",
            "Skip-gram | Epoch 5 | Batch 198500 | Loss: 0.3063\n",
            "Skip-gram | Epoch 5 | Batch 199000 | Loss: 0.3096\n",
            "Skip-gram | Epoch 5 | Batch 199500 | Loss: 0.2855\n",
            "Skip-gram | Epoch 5 | Batch 200000 | Loss: 0.3420\n",
            "Skip-gram | Epoch 5 | Batch 200500 | Loss: 0.2892\n",
            "Skip-gram | Epoch 5 | Batch 201000 | Loss: 0.3514\n",
            "Skip-gram | Epoch 5 | Batch 201500 | Loss: 0.3323\n",
            "Skip-gram | Epoch 5 | Batch 202000 | Loss: 0.3362\n",
            "Skip-gram | Epoch 5 | Batch 202500 | Loss: 0.3411\n",
            "Skip-gram | Epoch 5 | Batch 203000 | Loss: 0.3348\n",
            "Skip-gram | Epoch 5 | Batch 203500 | Loss: 0.3377\n",
            "Skip-gram | Epoch 5 | Batch 204000 | Loss: 0.3521\n",
            "Skip-gram | Epoch 5 | Batch 204500 | Loss: 0.3484\n",
            "Skip-gram | Epoch 5 | Batch 205000 | Loss: 0.3120\n",
            "Skip-gram | Epoch 5 | Batch 205500 | Loss: 0.3303\n",
            "Skip-gram | Epoch 5 | Batch 206000 | Loss: 0.3469\n",
            "Skip-gram | Epoch 5 | Batch 206500 | Loss: 0.3548\n",
            "Skip-gram | Epoch 5 | Batch 207000 | Loss: 0.3277\n",
            "Skip-gram | Epoch 5 | Batch 207500 | Loss: 0.3235\n",
            "Skip-gram | Epoch 5 | Batch 208000 | Loss: 0.3482\n",
            "Skip-gram | Epoch 5 | Batch 208500 | Loss: 0.3434\n",
            "Skip-gram | Epoch 5 | Batch 209000 | Loss: 0.3298\n",
            "Skip-gram | Epoch 5 | Batch 209500 | Loss: 0.3326\n",
            "Skip-gram | Epoch 5 | Batch 210000 | Loss: 0.3311\n",
            "Skip-gram | Epoch 5 | Batch 210500 | Loss: 0.3252\n",
            "Skip-gram | Epoch 5 | Batch 211000 | Loss: 0.2801\n",
            "Skip-gram | Epoch 5 | Batch 211500 | Loss: 0.2503\n",
            "Skip-gram | Epoch 5 | Batch 212000 | Loss: 0.2210\n",
            "Skip-gram | Epoch 5 | Batch 212500 | Loss: 0.2635\n",
            "Skip-gram | Epoch 5 | Batch 213000 | Loss: 0.3066\n",
            "Skip-gram | Epoch 5 | Batch 213500 | Loss: 0.3754\n",
            "Skip-gram | Epoch 5 | Batch 214000 | Loss: 0.3445\n",
            "Skip-gram | Epoch 5 | Batch 214500 | Loss: 0.3362\n",
            "Skip-gram | Epoch 5 | Batch 215000 | Loss: 0.3183\n",
            "Skip-gram | Epoch 5 | Batch 215500 | Loss: 0.3064\n",
            "Skip-gram | Epoch 5 | Batch 216000 | Loss: 0.3572\n",
            "Skip-gram | Epoch 5 | Batch 216500 | Loss: 0.3227\n",
            "Skip-gram | Epoch 5 | Batch 217000 | Loss: 0.3331\n",
            "Skip-gram | Epoch 5 | Batch 217500 | Loss: 0.3073\n",
            "Skip-gram | Epoch 5 | Batch 218000 | Loss: 0.3642\n",
            "Skip-gram | Epoch 5 | Batch 218500 | Loss: 0.3263\n",
            "Skip-gram | Epoch 5 | Batch 219000 | Loss: 0.3278\n",
            "Skip-gram | Epoch 5 | Batch 219500 | Loss: 0.3453\n",
            "Skip-gram | Epoch 5 | Batch 220000 | Loss: 0.3277\n",
            "Skip-gram | Epoch 5 | Batch 220500 | Loss: 0.3461\n",
            "Skip-gram | Epoch 5 | Batch 221000 | Loss: 0.3299\n",
            "Skip-gram | Epoch 5 | Batch 221500 | Loss: 0.3210\n",
            "Skip-gram | Epoch 5 | Batch 222000 | Loss: 0.3415\n",
            "Skip-gram | Epoch 5 | Batch 222500 | Loss: 0.3060\n",
            "Skip-gram | Epoch 5 | Batch 223000 | Loss: 0.3395\n",
            "Skip-gram | Epoch 5 | Batch 223500 | Loss: 0.3257\n",
            "Skip-gram | Epoch 5 | Batch 224000 | Loss: 0.3362\n",
            "Skip-gram | Epoch 5 | Batch 224500 | Loss: 0.3224\n",
            "Skip-gram | Epoch 5 | Batch 225000 | Loss: 0.3325\n",
            "Skip-gram | Epoch 5 | Batch 225500 | Loss: 0.3227\n",
            "Skip-gram | Epoch 5 | Batch 226000 | Loss: 0.3171\n",
            "Skip-gram | Epoch 5 | Batch 226500 | Loss: 0.3167\n",
            "Skip-gram | Epoch 5 | Batch 227000 | Loss: 0.3537\n",
            "Skip-gram | Epoch 5 | Batch 227500 | Loss: 0.3321\n",
            "Skip-gram | Epoch 5 | Batch 228000 | Loss: 0.3433\n",
            "Skip-gram | Epoch 5 | Batch 228500 | Loss: 0.3318\n",
            "Skip-gram | Epoch 5 | Batch 229000 | Loss: 0.3076\n",
            "Skip-gram | Epoch 5 | Batch 229500 | Loss: 0.3333\n",
            "Skip-gram | Epoch 5 | Batch 230000 | Loss: 0.3201\n",
            "Skip-gram | Epoch 5 | Batch 230500 | Loss: 0.3379\n",
            "Skip-gram | Epoch 5 | Batch 231000 | Loss: 0.3406\n",
            "Skip-gram | Epoch 5 | Batch 231500 | Loss: 0.3166\n",
            "Skip-gram | Epoch 5 | Batch 232000 | Loss: 0.2855\n",
            "Skip-gram | Epoch 5 | Batch 232500 | Loss: 0.2635\n",
            "Skip-gram | Epoch 5 | Batch 233000 | Loss: 0.3628\n",
            "Skip-gram | Epoch 5 | Batch 233500 | Loss: 0.3170\n",
            "Skip-gram | Epoch 5 | Batch 234000 | Loss: 0.3336\n",
            "Skip-gram | Epoch 5 | Batch 234500 | Loss: 0.3262\n",
            "Skip-gram | Epoch 5 | Batch 235000 | Loss: 0.3343\n",
            "Skip-gram | Epoch 5 | Batch 235500 | Loss: 0.3395\n",
            "Skip-gram | Epoch 5 | Batch 236000 | Loss: 0.3152\n",
            "Skip-gram | Epoch 5 | Batch 236500 | Loss: 0.3531\n",
            "Skip-gram | Epoch 5 | Batch 237000 | Loss: 0.3362\n",
            "Skip-gram | Epoch 5 | Batch 237500 | Loss: 0.3261\n",
            "Skip-gram | Epoch 5 | Batch 238000 | Loss: 0.3352\n",
            "Skip-gram | Epoch 5 | Batch 238500 | Loss: 0.3038\n",
            "Skip-gram | Epoch 5 | Batch 239000 | Loss: 0.3179\n",
            "Skip-gram | Epoch 5 | Batch 239500 | Loss: 0.3270\n",
            "Skip-gram | Epoch 5 | Batch 240000 | Loss: 0.3458\n",
            "Skip-gram | Epoch 5 | Batch 240500 | Loss: 0.3333\n",
            "Skip-gram | Epoch 5 | Batch 241000 | Loss: 0.3496\n",
            "Skip-gram | Epoch 5 | Batch 241500 | Loss: 0.3406\n",
            "Skip-gram | Epoch 5 | Batch 242000 | Loss: 0.2870\n",
            "Skip-gram | Epoch 5 | Batch 242500 | Loss: 0.3190\n",
            "Skip-gram | Epoch 5 | Batch 243000 | Loss: 0.3117\n",
            "Skip-gram | Epoch 5 | Batch 243500 | Loss: 0.3284\n",
            "Skip-gram | Epoch 5 | Batch 244000 | Loss: 0.3429\n",
            "Skip-gram | Epoch 5 | Batch 244500 | Loss: 0.3518\n",
            "Skip-gram | Epoch 5 | Batch 245000 | Loss: 0.3707\n",
            "Skip-gram | Epoch 5 | Batch 245500 | Loss: 0.3350\n",
            "Skip-gram | Epoch 5 | Batch 246000 | Loss: 0.3368\n",
            "Skip-gram | Epoch 5 | Batch 246500 | Loss: 0.3323\n",
            "Skip-gram | Epoch 5 | Batch 247000 | Loss: 0.3337\n",
            "Skip-gram | Epoch 5 | Batch 247500 | Loss: 0.3308\n",
            "Skip-gram | Epoch 5 | Batch 248000 | Loss: 0.3087\n",
            "Skip-gram | Epoch 5 | Batch 248500 | Loss: 0.3305\n",
            "Skip-gram | Epoch 5 | Batch 249000 | Loss: 0.3477\n",
            "Skip-gram | Epoch 5 | Batch 249500 | Loss: 0.3330\n",
            "Skip-gram | Epoch 5 | Batch 250000 | Loss: 0.3249\n",
            "Skip-gram | Epoch 5 | Batch 250500 | Loss: 0.3496\n",
            "Skip-gram | Epoch 5 | Batch 251000 | Loss: 0.3411\n",
            "Skip-gram | Epoch 5 | Batch 251500 | Loss: 0.3205\n",
            "Skip-gram | Epoch 5 | Batch 252000 | Loss: 0.3417\n",
            "Skip-gram | Epoch 5 | Batch 252500 | Loss: 0.3202\n",
            "Skip-gram | Epoch 5 | Batch 253000 | Loss: 0.2861\n",
            "Skip-gram | Epoch 5 | Batch 253500 | Loss: 0.3154\n",
            "Skip-gram | Epoch 5 | Batch 254000 | Loss: 0.3204\n",
            "Skip-gram | Epoch 5 | Batch 254500 | Loss: 0.3619\n",
            "Skip-gram | Epoch 5 | Batch 255000 | Loss: 0.3009\n",
            "Skip-gram | Epoch 5 | Batch 255500 | Loss: 0.3327\n",
            "Skip-gram | Epoch 5 | Batch 256000 | Loss: 0.3367\n",
            "Skip-gram | Epoch 5 | Batch 256500 | Loss: 0.3215\n",
            "Skip-gram | Epoch 5 | Batch 257000 | Loss: 0.3235\n",
            "Skip-gram | Epoch 5 | Batch 257500 | Loss: 0.3253\n",
            "Skip-gram | Epoch 5 | Batch 258000 | Loss: 0.3157\n",
            "Skip-gram | Epoch 5 | Batch 258500 | Loss: 0.2677\n",
            "Skip-gram | Epoch 5 | Batch 259000 | Loss: 0.2924\n",
            "Skip-gram | Epoch 5 | Batch 259500 | Loss: 0.3480\n",
            "Skip-gram | Epoch 5 | Batch 260000 | Loss: 0.3531\n",
            "Skip-gram | Epoch 5 | Batch 260500 | Loss: 0.3401\n",
            "Skip-gram | Epoch 5 | Batch 261000 | Loss: 0.3489\n",
            "Skip-gram | Epoch 5 | Batch 261500 | Loss: 0.3157\n",
            "Skip-gram | Epoch 5 | Batch 262000 | Loss: 0.3459\n",
            "Skip-gram | Epoch 5 | Batch 262500 | Loss: 0.3412\n",
            "Skip-gram | Epoch 5 | Batch 263000 | Loss: 0.3361\n",
            "Skip-gram | Epoch 5 | Batch 263500 | Loss: 0.3570\n",
            "Skip-gram | Epoch 5 | Batch 264000 | Loss: 0.3431\n",
            "Skip-gram | Epoch 5 | Batch 264500 | Loss: 0.3377\n",
            "Skip-gram | Epoch 5 | Batch 265000 | Loss: 0.3253\n",
            "Skip-gram | Epoch 5 | Batch 265500 | Loss: 0.3569\n",
            "Skip-gram | Epoch 5 | Batch 266000 | Loss: 0.3317\n",
            "Skip-gram | Epoch 5 | Batch 266500 | Loss: 0.3224\n",
            "Skip-gram | Epoch 5 | Batch 267000 | Loss: 0.3109\n",
            "Skip-gram | Epoch 5 | Batch 267500 | Loss: 0.3319\n",
            "Skip-gram | Epoch 5 | Batch 268000 | Loss: 0.3474\n",
            "Skip-gram | Epoch 5 | Batch 268500 | Loss: 0.3254\n",
            "Skip-gram | Epoch 5 | Batch 269000 | Loss: 0.3429\n",
            "Skip-gram | Epoch 5 | Batch 269500 | Loss: 0.2934\n",
            "Skip-gram | Epoch 5 | Batch 270000 | Loss: 0.3343\n",
            "Skip-gram | Epoch 5 | Batch 270500 | Loss: 0.3652\n",
            "Skip-gram | Epoch 5 | Batch 271000 | Loss: 0.3476\n",
            "Skip-gram | Epoch 5 | Batch 271500 | Loss: 0.3427\n",
            "Skip-gram | Epoch 5 | Batch 272000 | Loss: 0.3144\n",
            "Skip-gram | Epoch 5 | Batch 272500 | Loss: 0.3238\n",
            "Skip-gram | Epoch 5 | Batch 273000 | Loss: 0.3121\n",
            "Skip-gram | Epoch 5 | Batch 273500 | Loss: 0.2008\n",
            "Skip-gram | Epoch 5 | Batch 274000 | Loss: 0.3366\n",
            "Skip-gram | Epoch 5 | Batch 274500 | Loss: 0.3434\n",
            "Skip-gram | Epoch 5 | Batch 275000 | Loss: 0.3344\n",
            "Skip-gram | Epoch 5 | Batch 275500 | Loss: 0.3288\n",
            "Skip-gram | Epoch 5 | Batch 276000 | Loss: 0.2937\n",
            "Skip-gram | Epoch 5 | Batch 276500 | Loss: 0.2912\n",
            "Skip-gram | Epoch 5 | Batch 277000 | Loss: 0.3680\n",
            "Skip-gram | Epoch 5 | Batch 277500 | Loss: 0.3606\n",
            "Skip-gram | Epoch 5 | Batch 278000 | Loss: 0.3162\n",
            "Skip-gram | Epoch 5 | Batch 278500 | Loss: 0.3404\n",
            "Skip-gram | Epoch 5 | Batch 279000 | Loss: 0.3384\n",
            "Skip-gram | Epoch 5 | Batch 279500 | Loss: 0.3254\n",
            "Skip-gram | Epoch 5 | Batch 280000 | Loss: 0.3205\n",
            "Skip-gram | Epoch 5 | Batch 280500 | Loss: 0.3099\n",
            "Skip-gram | Epoch 5 | Batch 281000 | Loss: 0.3250\n",
            "Skip-gram | Epoch 5 | Batch 281500 | Loss: 0.3316\n",
            "Skip-gram | Epoch 5 | Batch 282000 | Loss: 0.3630\n",
            "Skip-gram | Epoch 5 | Batch 282500 | Loss: 0.3055\n",
            "Skip-gram | Epoch 5 | Batch 283000 | Loss: 0.3325\n",
            "Skip-gram | Epoch 5 | Batch 283500 | Loss: 0.2039\n",
            "Skip-gram | Epoch 5 | Batch 284000 | Loss: 0.2262\n",
            "Skip-gram | Epoch 5 | Batch 284500 | Loss: 0.3580\n",
            "Skip-gram | Epoch 5 | Batch 285000 | Loss: 0.3266\n",
            "Skip-gram | Epoch 5 | Batch 285500 | Loss: 0.3264\n",
            "Skip-gram | Epoch 5 | Batch 286000 | Loss: 0.3332\n",
            "Skip-gram | Epoch 5 | Batch 286500 | Loss: 0.3444\n",
            "Skip-gram | Epoch 5 | Batch 287000 | Loss: 0.3414\n",
            "Skip-gram | Epoch 5 | Batch 287500 | Loss: 0.3328\n",
            "Skip-gram | Epoch 5 | Batch 288000 | Loss: 0.3474\n",
            "Skip-gram | Epoch 5 | Batch 288500 | Loss: 0.2477\n",
            "Skip-gram | Epoch 5 | Batch 289000 | Loss: 0.3114\n",
            "Skip-gram | Epoch 5 | Batch 289500 | Loss: 0.3518\n",
            "Skip-gram | Epoch 5 | Batch 290000 | Loss: 0.3525\n",
            "Skip-gram | Epoch 5 | Batch 290500 | Loss: 0.2944\n",
            "Skip-gram | Epoch 5 | Batch 291000 | Loss: 0.3122\n",
            "Skip-gram | Epoch 5 | Batch 291500 | Loss: 0.2972\n",
            "Skip-gram | Epoch 5 | Batch 292000 | Loss: 0.3315\n",
            "Skip-gram | Epoch 5 | Batch 292500 | Loss: 0.3230\n",
            "Skip-gram | Epoch 5 | Batch 293000 | Loss: 0.3495\n",
            "Skip-gram | Epoch 5 | Batch 293500 | Loss: 0.3350\n",
            "Skip-gram | Epoch 5 | Batch 294000 | Loss: 0.3442\n",
            "Skip-gram | Epoch 5 | Batch 294500 | Loss: 0.3367\n",
            "Skip-gram | Epoch 5 | Batch 295000 | Loss: 0.3318\n",
            "Skip-gram | Epoch 5 | Batch 295500 | Loss: 0.3440\n",
            "Skip-gram | Epoch 5 | Batch 296000 | Loss: 0.3297\n",
            "Skip-gram | Epoch 5 | Batch 296500 | Loss: 0.3340\n",
            "Skip-gram | Epoch 5 | Batch 297000 | Loss: 0.3196\n",
            "Skip-gram | Epoch 5 | Batch 297500 | Loss: 0.3480\n",
            "Skip-gram | Epoch 5 | Batch 298000 | Loss: 0.2685\n",
            "Skip-gram | Epoch 5 | Batch 298500 | Loss: 0.3682\n",
            "Skip-gram | Epoch 5 | Batch 299000 | Loss: 0.3552\n",
            "Skip-gram | Epoch 5 | Batch 299500 | Loss: 0.3320\n",
            "Skip-gram | Epoch 5 | Batch 300000 | Loss: 0.3222\n",
            "Skip-gram | Epoch 5 | Batch 300500 | Loss: 0.3370\n",
            "Skip-gram | Epoch 5 | Batch 301000 | Loss: 0.3133\n",
            "Skip-gram | Epoch 5 | Batch 301500 | Loss: 0.3331\n",
            "Skip-gram | Epoch 5 | Batch 302000 | Loss: 0.3361\n",
            "Skip-gram | Epoch 5 | Batch 302500 | Loss: 0.3445\n",
            "Skip-gram | Epoch 5 | Batch 303000 | Loss: 0.3237\n",
            "Skip-gram | Epoch 5 | Batch 303500 | Loss: 0.3086\n",
            "Skip-gram | Epoch 5 | Batch 304000 | Loss: 0.3393\n",
            "Skip-gram | Epoch 5 | Batch 304500 | Loss: 0.3711\n",
            "Skip-gram | Epoch 5 | Batch 305000 | Loss: 0.3450\n",
            "Skip-gram | Epoch 5 | Batch 305500 | Loss: 0.3166\n",
            "Skip-gram | Epoch 5 | Batch 306000 | Loss: 0.3403\n",
            "Skip-gram | Epoch 5 | Batch 306500 | Loss: 0.3247\n",
            "Skip-gram | Epoch 5 | Batch 307000 | Loss: 0.3079\n",
            "Skip-gram | Epoch 5 | Batch 307500 | Loss: 0.3426\n",
            "Skip-gram | Epoch 5 | Batch 308000 | Loss: 0.3045\n",
            "Skip-gram | Epoch 5 | Batch 308500 | Loss: 0.3629\n",
            "Skip-gram | Epoch 5 | Batch 309000 | Loss: 0.3341\n",
            "Skip-gram | Epoch 5 | Batch 309500 | Loss: 0.3445\n",
            "Skip-gram | Epoch 5 | Batch 310000 | Loss: 0.3149\n",
            "Skip-gram | Epoch 5 | Batch 310500 | Loss: 0.3479\n",
            "Skip-gram | Epoch 5 | Batch 311000 | Loss: 0.3557\n",
            "Skip-gram | Epoch 5 | Batch 311500 | Loss: 0.3459\n",
            "Skip-gram | Epoch 5 | Batch 312000 | Loss: 0.3408\n",
            "Skip-gram | Epoch 5 | Batch 312500 | Loss: 0.3326\n",
            "Skip-gram | Epoch 5 | Batch 313000 | Loss: 0.3454\n",
            "Skip-gram | Epoch 5 | Batch 313500 | Loss: 0.3438\n",
            "Skip-gram | Epoch 5 | Batch 314000 | Loss: 0.3482\n",
            "Skip-gram | Epoch 5 | Batch 314500 | Loss: 0.3189\n",
            "Skip-gram | Epoch 5 | Batch 315000 | Loss: 0.3189\n",
            "Skip-gram | Epoch 5 | Batch 315500 | Loss: 0.3365\n",
            "Skip-gram | Epoch 5 | Batch 316000 | Loss: 0.3328\n",
            "Skip-gram | Epoch 5 | Batch 316500 | Loss: 0.3602\n",
            "Skip-gram | Epoch 5 | Batch 317000 | Loss: 0.2895\n",
            "Skip-gram | Epoch 5 | Batch 317500 | Loss: 0.2905\n",
            "Skip-gram | Epoch 5 | Batch 318000 | Loss: 0.3340\n",
            "Skip-gram | Epoch 5 | Batch 318500 | Loss: 0.3091\n",
            "Skip-gram | Epoch 5 | Batch 319000 | Loss: 0.3268\n",
            "Skip-gram | Epoch 5 | Batch 319500 | Loss: 0.3224\n",
            "Skip-gram | Epoch 5 | Batch 320000 | Loss: 0.3409\n",
            "Skip-gram | Epoch 5 | Batch 320500 | Loss: 0.3691\n",
            "Skip-gram | Epoch 5 | Batch 321000 | Loss: 0.3404\n",
            "Skip-gram | Epoch 5 | Batch 321500 | Loss: 0.3377\n",
            "Skip-gram | Epoch 5 | Batch 322000 | Loss: 0.3335\n",
            "Skip-gram | Epoch 5 | Batch 322500 | Loss: 0.3536\n",
            "Skip-gram | Epoch 5 | Batch 323000 | Loss: 0.2913\n",
            "Skip-gram | Epoch 5 | Batch 323500 | Loss: 0.3563\n",
            "Skip-gram Epoch 5. Средний Loss: 0.3254\n"
          ]
        }
      ],
      "source": [
        "EMB_DIM = 256\n",
        "WINDOW_SIZE = 3\n",
        "VOCAB_SIZE = len(word2id)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "sg_model = SkipGramModelNeg(VOCAB_SIZE, EMB_DIM).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(sg_model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in range(5):\n",
        "    losses = []\n",
        "\n",
        "    for x, y, labels in gen_batches_sg_neg(indexed_wiki, window_size=WINDOW_SIZE, batch_size=512):\n",
        "        x, y, labels = x.to(DEVICE), y.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = sg_model(x, y)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if len(losses) % 500 == 0:\n",
        "            print(f\"Skip-gram | Epoch {epoch+1} | Batch {len(losses)} | Loss: {np.mean(losses[-500:]):.4f}\")\n",
        "\n",
        "    print(f\"Skip-gram Epoch {epoch+1}. Средний Loss: {np.mean(losses):.4f}\")\n",
        "\n",
        "sg_weights = sg_model.embeddings.weight.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DPsppasnwv3Z",
      "metadata": {
        "id": "DPsppasnwv3Z"
      },
      "outputs": [],
      "source": [
        "torch.save(sg_model.state_dict(), '/content/drive/My Drive/sg_model_weights.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R8LHBlbywyOI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8LHBlbywyOI",
        "outputId": "bb5b7657-65a6-470f-e4d5-6b39feafb7f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Слово: новостройка | Соседи: Слова нет в словаре\n",
            "Слово: река | Соседи: ['берег', 'озеро', 'приток', 'долина', 'км']\n",
            "Слово: посёлок | Соседи: ['село', 'поселение', 'район', 'сельский', 'деревня']\n",
            "Слово: фильм | Соседи: ['режиссёр', 'сериал', 'картина', 'роман', 'эпизод']\n",
            "Слово: область | Соседи: ['район', 'ныне', 'сельский', 'ростовский', 'губерния']\n"
          ]
        }
      ],
      "source": [
        "test_words = ['новостройка', 'река', 'посёлок', 'фильм', 'область']\n",
        "\n",
        "for word in test_words:\n",
        "    closest = get_closest_words(word, sg_weights, word2id, id2word)\n",
        "    print(f\"Слово: {word} | Соседи: {closest}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SqbKUeXenjkC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqbKUeXenjkC",
        "outputId": "ce8fdcff-b216-4168-96a3-a41e73fa5e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Слово: новостройка | Соседи: Слова нет в словаре\n",
            "Слово: река | Соседи: ['берег', 'озеро', 'ручей', 'долина', 'км']\n",
            "Слово: посёлок | Соседи: ['село', 'район', 'хутор', 'деревня', 'улица']\n",
            "Слово: фильм | Соседи: ['сериал', 'мультфильм', 'эпизод', 'режиссёр', 'роман']\n",
            "Слово: область | Соседи: ['губерния', 'уезд', 'район', 'областной', 'край']\n"
          ]
        }
      ],
      "source": [
        "test_words = ['новостройка', 'река', 'посёлок', 'фильм', 'область']\n",
        "\n",
        "for word in test_words:\n",
        "    closest = get_closest_words(word, cbow_weights, word2id, id2word)\n",
        "    print(f\"Слово: {word} | Соседи: {closest}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4-m278_XEEmJ",
      "metadata": {
        "id": "4-m278_XEEmJ"
      },
      "source": [
        "Удивительно, но сильно дольше обучавшийся skip-gram в итоге справился не сильно лучше cbow модели. Больше всего промахов у модели skip-gram с подбором синонимов к слову \"область\", тут и \"ныне\" и \"сельский\" довольно далеки."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3b61b7c",
      "metadata": {
        "id": "c3b61b7c"
      },
      "source": [
        "# Задание 2 (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66eff080",
      "metadata": {
        "id": "66eff080"
      },
      "source": [
        "Обучите 1 word2vec и 1 fastext модель в gensim. В каждой из модели нужно задать все параметры, которые мы разбирали на семинаре. Заданные значения должны отличаться от дефолтных и от тех, что мы использовали на семинаре."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "986c2018",
      "metadata": {
        "id": "986c2018"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec, FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e5035bdc",
      "metadata": {
        "id": "e5035bdc"
      },
      "outputs": [],
      "source": [
        "model_w2v = Word2Vec(sentences=lemmas,\n",
        "                     vector_size=256,\n",
        "                     window=5,\n",
        "                     min_count=5,\n",
        "                     sg=1,\n",
        "                     epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_w2v.wv.most_similar('каша'))"
      ],
      "metadata": {
        "id": "_3QnTU5i5uhT",
        "outputId": "ed6a5280-5d9c-4a52-e27b-e712677d1807",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_3QnTU5i5uhT",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('взбить', 0.870908796787262), ('ячменный', 0.8703863024711609), ('жареный', 0.8668773174285889), ('обжарить', 0.8611367344856262), ('бульон', 0.8577075600624084), ('сметана', 0.857672929763794), ('приправа', 0.8549471497535706), ('набитый', 0.8544008731842041), ('пирожное', 0.8543984293937683), ('репчатый', 0.8529167771339417)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "tjTPY3tiWiBq",
      "metadata": {
        "id": "tjTPY3tiWiBq"
      },
      "outputs": [],
      "source": [
        "model_ft = FastText(sentences=lemmas,\n",
        "                    vector_size=256,\n",
        "                    window=5,\n",
        "                    min_count=5,\n",
        "                    sg=1,\n",
        "                    epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ps0ojlJcWh-f",
      "metadata": {
        "id": "ps0ojlJcWh-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "152a4611-014f-42c0-9a7c-110d0eafbc51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('кашар', 0.888312816619873), ('аша', 0.7705908417701721), ('паприкаша', 0.7528278827667236), ('кашкра', 0.7398574352264404), ('чаша', 0.7350392937660217), ('паша', 0.7347075939178467), ('кабаниха', 0.7147276997566223), ('каштан', 0.7145907878875732), ('камыш', 0.7047783732414246), ('даша', 0.7017403841018677)]\n"
          ]
        }
      ],
      "source": [
        "print(model_ft.wv.most_similar('каша'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4bb928c",
      "metadata": {
        "id": "e4bb928c"
      },
      "source": [
        "# Задание 3 (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3019b0d1",
      "metadata": {
        "id": "3019b0d1"
      },
      "source": [
        "Используя датасет для классификации (labeled.csv), обучите классификатор на базе эмбеддингов. Оцените качество на отложенной выборке.   \n",
        "В качестве эмбеддинг модели вы можете использовать одну из моделей обученных в предыдущем задании или использовать одну из предобученных моделей с rusvectores (удостоверьтесь что правильно воспроизводите предобработку в этом случае!)  \n",
        "Для того, чтобы построить эмбединг целого текста, усредните вектора отдельных слов в один общий вектор.\n",
        "В качестве алгоритма классификации используйте LogisicticRegression (можете попробовать SGDClassifier, чтобы было побыстрее)  \n",
        "F1 мера должна быть выше 20%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ed908832",
      "metadata": {
        "id": "ed908832"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('sample_data/labeled.csv')"
      ],
      "metadata": {
        "id": "lEj91IMxDGbI"
      },
      "id": "lEj91IMxDGbI",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['norm_text'] = data['comment'].apply(preprocess_with_lemma)"
      ],
      "metadata": {
        "id": "pa7XaYTEGulC"
      },
      "id": "pa7XaYTEGulC",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text, model, vector_size):\n",
        "    vectors = [model_w2v.wv[word] for word in text if word in model_w2v.wv]\n",
        "\n",
        "    if not vectors:\n",
        "        return np.zeros(vector_size)\n",
        "\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "DIM = 256\n",
        "X = np.array([get_embedding(text, model_w2v, DIM) for text in data['norm_text']])\n",
        "y = data['toxic'].values"
      ],
      "metadata": {
        "id": "KfVlAw1OG68a"
      },
      "id": "KfVlAw1OG68a",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(C=1.0, max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "preds = clf.predict(X_test)\n",
        "print(classification_report(y_test, preds))"
      ],
      "metadata": {
        "id": "hT5SC7_OHiS4",
        "outputId": "049774ab-bbe0-4ace-a917-abb2ce8442ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hT5SC7_OHiS4",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.92      0.87      1944\n",
            "         1.0       0.78      0.63      0.70       939\n",
            "\n",
            "    accuracy                           0.82      2883\n",
            "   macro avg       0.81      0.77      0.79      2883\n",
            "weighted avg       0.82      0.82      0.82      2883\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60c18c5a",
      "metadata": {
        "id": "60c18c5a"
      },
      "source": [
        "# Задание 4 (2 доп балла)\n",
        "\n",
        "В тетрадку с фастекстом добавьте код для обучения с negative sampling (задача сводится к бинарной классификации) и обучите модель. Проверьте полученную модель на нескольких словах. Похожие слова должны быть похожими по смыслу и по форме."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "metadata": {
        "id": "nWKrkSgzJulg",
        "outputId": "e068ccc8-e13f-4405-83cf-ab1c77cc30f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nWKrkSgzJulg",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Torch version: 2.9.0+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "def tokenize(text):\n",
        "    tokens = re.sub('#+', ' ', text.lower()).split()\n",
        "    tokens = [token.strip(punctuation) for token in tokens]\n",
        "    tokens = [token for token in tokens if token]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "v6cFDxwLNZf9"
      },
      "id": "v6cFDxwLNZf9",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngrammer(raw_string, n=2):\n",
        "    ngrams = []\n",
        "    raw_string = ''.join(['<', raw_string, '>'])\n",
        "    for i in range(0,len(raw_string)-n+1):\n",
        "        ngram = ''.join(raw_string[i:i+n])\n",
        "        if ngram == '<' or ngram == '>':\n",
        "            continue\n",
        "        ngrams.append(ngram)\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "gftRXEO6NaTh"
      },
      "id": "gftRXEO6NaTh",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_tokens(tokens, min_ngram_size, max_ngram_size):\n",
        "    tokens_with_subwords = []\n",
        "    for token in tokens:\n",
        "        subtokens = []\n",
        "        for i in range(min_ngram_size, max_ngram_size+1):\n",
        "            if len(token) > i:\n",
        "                subtokens.extend(ngrammer(token, i))\n",
        "        tokens_with_subwords.append(subtokens)\n",
        "    return tokens_with_subwords"
      ],
      "metadata": {
        "id": "FTyl4VHCNb83"
      },
      "id": "FTyl4VHCNb83",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SubwordTokenizer:\n",
        "    def __init__(self, ngram_range=(1,1), min_count=5):\n",
        "        self.min_ngram_size, self.max_ngram_size = ngram_range\n",
        "        self.min_count = min_count\n",
        "        self.subword_vocab = None\n",
        "        self.fullword_vocab = None\n",
        "        self.vocab = None\n",
        "        self.id2word = None\n",
        "        self.word2id = None\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        # чтобы построить словарь нужно пройти по всему корпусу и собрать частоты всех уникальных слов и нграммов\n",
        "        unfiltered_subword_vocab = Counter()\n",
        "        unfiltered_fullword_vocab = Counter()\n",
        "        for text in texts:\n",
        "            tokens = tokenize(text)\n",
        "            unfiltered_fullword_vocab.update(tokens)\n",
        "            subwords_per_token = split_tokens(tokens, self.min_ngram_size, self.max_ngram_size)\n",
        "            for subwords in subwords_per_token:\n",
        "                # в одном слове могут быть одинаковые нграммы поэтому возьмем только уникальные\n",
        "                unfiltered_subword_vocab.update(set(subwords))\n",
        "\n",
        "        self.fullword_vocab = set()\n",
        "        self.subword_vocab = set()\n",
        "\n",
        "        # теперь отфильтруем по частоте\n",
        "        for word, count in unfiltered_fullword_vocab.items():\n",
        "            if count >= self.min_count:\n",
        "                self.fullword_vocab.add(word)\n",
        "        # для нграммов сделаем порог побольше чтобы не создавать слишком много нграммов\n",
        "        # и учитывать только действительно частотные\n",
        "        for word, count in unfiltered_subword_vocab.items():\n",
        "            if count >= (self.min_count * 100):\n",
        "                self.subword_vocab.add(word)\n",
        "\n",
        "        # общий словарь\n",
        "        self.vocab = self.fullword_vocab | self.subword_vocab\n",
        "        self.id2word = {i:word for i,word in enumerate(self.vocab)}\n",
        "        self.word2id = {word:i for i,word in self.id2word.items()}\n",
        "\n",
        "    def subword_tokenize(self, text):\n",
        "        if self.vocab is None:\n",
        "            raise AttributeError('Vocabulary is not built!')\n",
        "        # разбиваем на токены\n",
        "        tokens = tokenize(text)\n",
        "        # каждый токен разбиваем на символьные нграммы\n",
        "        tokens_with_subwords = split_tokens(tokens, self.min_ngram_size, self.max_ngram_size)\n",
        "        # оставляет только токены и нграммы которые есть в словаре\n",
        "        only_vocab_tokens_with_subwords = []\n",
        "        for full_token, sub_tokens in zip(tokens, tokens_with_subwords):\n",
        "            filtered = []\n",
        "            if full_token in self.vocab:\n",
        "                # само слово и нграммы хранятся в одном списке\n",
        "                # но слово будет всегда первым в списке\n",
        "                filtered.append(full_token)\n",
        "            filtered.extend([subtoken for subtoken in set(sub_tokens) if subtoken in self.vocab])\n",
        "            only_vocab_tokens_with_subwords.append(filtered)\n",
        "\n",
        "        return only_vocab_tokens_with_subwords\n",
        "\n",
        "    def encode(self, subword_tokenized_text):\n",
        "        # маппим токены и нграммы в их индексы в словаре\n",
        "        encoded_text = []\n",
        "        for token in subword_tokenized_text:\n",
        "            if not token:\n",
        "                continue\n",
        "            encoded_text.append([self.word2id[token[0]]] + [self.word2id[t] for t in set(token[1:]) if t in self.word2id and t != token[0]])\n",
        "        return encoded_text\n",
        "\n",
        "    def __call__(self, text):\n",
        "        return self.encode(self.subword_tokenize(text))"
      ],
      "metadata": {
        "id": "-VjqPaOoNgA6"
      },
      "id": "-VjqPaOoNgA6",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SubwordTokenizer(ngram_range=(2,4), min_count=10)\n",
        "tokenizer.build_vocab(wiki)"
      ],
      "metadata": {
        "id": "pXA49bM2Nln4"
      },
      "id": "pXA49bM2Nln4",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FastTextNegSampling(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=100):\n",
        "        super().__init__()\n",
        "        self.target_embeddings = nn.EmbeddingBag(vocab_size, emb_dim, mode='mean')\n",
        "        self.context_embeddings = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "    def forward(self, target_subwords, context_words, offsets):\n",
        "        target_vecs = self.target_embeddings(target_subwords, offsets)\n",
        "        context_vecs = self.context_embeddings(context_words)\n",
        "\n",
        "        return torch.sum(target_vecs * context_vecs, dim=1)"
      ],
      "metadata": {
        "id": "jnsf9IBnNnqr"
      },
      "id": "jnsf9IBnNnqr",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_batches_neg(sentences, tokenizer, window=5, batch_size=512, n_neg=5):\n",
        "    while True:\n",
        "        target_subwords, offsets, contexts, labels = [], [0], [], []\n",
        "\n",
        "        for sent in sentences:\n",
        "            encoded = tokenizer(sent)\n",
        "            if len(encoded) < 2: continue\n",
        "\n",
        "            for i in range(len(encoded)):\n",
        "                target = encoded[i]\n",
        "                start = max(0, i - window)\n",
        "                end = min(len(encoded), i + window + 1)\n",
        "                pos_indices = [encoded[j][0] for j in range(start, end) if i != j]\n",
        "\n",
        "                for pos_idx in pos_indices:\n",
        "                    target_subwords.extend(target)\n",
        "                    offsets.append(offsets[-1] + len(target))\n",
        "                    contexts.append(pos_idx)\n",
        "                    labels.append(1)\n",
        "\n",
        "                    for _ in range(n_neg):\n",
        "                        target_subwords.extend(target)\n",
        "                        offsets.append(offsets[-1] + len(target))\n",
        "                        contexts.append(np.random.randint(0, len(tokenizer.vocab)))\n",
        "                        labels.append(0)\n",
        "\n",
        "                if len(contexts) >= batch_size:\n",
        "                    yield (torch.LongTensor(target_subwords),\n",
        "                           torch.LongTensor(offsets[:-1]),\n",
        "                           torch.LongTensor(contexts),\n",
        "                           torch.FloatTensor(labels))\n",
        "                    target_subwords, offsets, contexts, labels = [], [0], [], []"
      ],
      "metadata": {
        "id": "olsN7lBOOQCJ"
      },
      "id": "olsN7lBOOQCJ",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastTextNegSampling(len(tokenizer.vocab)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "train_gen = gen_batches_neg(wiki, tokenizer, batch_size=1024)\n",
        "\n",
        "for step in range(2001):\n",
        "    subwords, off, ctx, lbl = next(train_gen)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    preds = model(subwords.to(device), ctx.to(device), off.to(device))\n",
        "    loss = criterion(preds, lbl.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 500 == 0:\n",
        "        print(f\"Step {step}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "-5Ic_-4ROVje",
        "outputId": "5ae381e4-1fa1-4827-b985-b759fb511c9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-5Ic_-4ROVje",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Loss: 1.5416\n",
            "Step 500, Loss: 1.6266\n",
            "Step 1000, Loss: 1.3166\n",
            "Step 1500, Loss: 1.6304\n",
            "Step 2000, Loss: 1.1620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector(word, model, tokenizer):\n",
        "    model.eval()\n",
        "    tokens = tokenizer(word)[0]\n",
        "    subwords = torch.LongTensor(tokens).to(device)\n",
        "    offset = torch.LongTensor([0]).to(device)\n",
        "    with torch.no_grad():\n",
        "        vec = model.target_embeddings(subwords, offset)\n",
        "    return vec.cpu().numpy()"
      ],
      "metadata": {
        "id": "zUV2LRCXOdy9"
      },
      "id": "zUV2LRCXOdy9",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_vector(\"нос\", model, tokenizer)"
      ],
      "metadata": {
        "id": "mhGk1yj2OvIa",
        "outputId": "685f91c4-cec7-4139-9454-0924bdc270bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mhGk1yj2OvIa",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.13846257, -0.14302301,  0.15178375,  0.6185609 ,  0.32083163,\n",
              "         0.1683947 , -0.47352323,  0.16885343, -0.09866892,  0.3684754 ,\n",
              "         0.33515102,  0.16406013, -0.37696043, -0.06562246,  0.4532543 ,\n",
              "         0.21261689,  0.20693934, -0.00247377, -0.50505364,  0.43035278,\n",
              "         0.75401926,  0.15918767,  1.0646186 , -0.50896984,  0.28520927,\n",
              "        -0.24431667,  0.38205248, -0.05343943,  0.00879141,  0.13118945,\n",
              "        -0.3463318 ,  0.63581896,  0.2885397 ,  0.9826933 ,  0.08578312,\n",
              "        -0.32318664,  0.33146152,  0.46398038,  0.34402713, -0.12774496,\n",
              "        -0.26661348,  1.174057  , -0.17074664,  0.07546492,  0.16881272,\n",
              "         0.33689183, -0.6178116 ,  0.15365107,  0.33820468, -0.6467811 ,\n",
              "        -0.55077016,  0.28717497,  0.40538654,  0.6012375 , -0.71409583,\n",
              "        -0.08945696,  0.5198597 , -0.8568907 ,  0.4533511 , -0.60046464,\n",
              "         0.36029738, -0.06455822,  0.34550548,  0.23540969,  0.27582708,\n",
              "         0.47075024,  0.29856104,  0.14271839,  0.06400982, -0.6536585 ,\n",
              "        -0.11177037, -0.16222699, -0.02332009, -0.29658514, -0.12022243,\n",
              "         0.46556497, -0.16622436, -0.12331271,  0.45357972, -0.1170195 ,\n",
              "         0.14629272, -0.8696469 ,  0.25798273,  0.11477818, -0.22189614,\n",
              "        -0.07174013,  0.10456622,  0.39230958, -0.19427732, -0.58042204,\n",
              "        -0.10618043,  0.5230454 ,  0.15844259,  0.8499378 ,  0.55102223,\n",
              "         0.46525097,  0.24038045, -0.32284778, -0.14411142,  0.17074327]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def most_similar_neg(word, model, tokenizer, top_n=10):\n",
        "    word_vec = get_vector(word, model, tokenizer)\n",
        "\n",
        "    all_embeddings = model.target_embeddings.weight.detach().cpu().numpy()\n",
        "\n",
        "    distances = cosine_distances(word_vec.reshape(1, -1), all_embeddings)[0]\n",
        "\n",
        "    indices = distances.argsort()[:top_n]\n",
        "\n",
        "    return [(tokenizer.id2word[i], distances[i]) for i in indices]\n",
        "\n",
        "print(most_similar_neg(\"нос\", model, tokenizer))"
      ],
      "metadata": {
        "id": "mmfuOfJ4VmQC",
        "outputId": "012e8584-c297-4bcb-f97d-7c7478b970f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mmfuOfJ4VmQC",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('нос', np.float32(0.47815406)), ('ос', np.float32(0.49888468)), ('с>', np.float32(0.5321866)), ('сас', np.float32(0.60100335)), ('собирают', np.float32(0.60179764)), ('прошлом', np.float32(0.6234371)), ('насколько', np.float32(0.62434417)), ('<н', np.float32(0.628259)), ('заблуждение', np.float32(0.6288296)), ('сох', np.float32(0.6330385))]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}